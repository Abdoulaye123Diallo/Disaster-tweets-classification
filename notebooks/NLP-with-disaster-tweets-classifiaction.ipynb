{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a46012-b136-43a9-ad81-76ac62a83136",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Pour la suite du cours, nous allons faire l'apprentissage par la pratique avec un cas réel de projet de Machine Learning\n",
    "\n",
    "Description du projet: Prédire le prix des biens immobiliers à Ames, Iowa.\n",
    "\n",
    "Les données sont disponibles dans: [sklearn.datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml)\n",
    "\n",
    "\n",
    "Voici un récapitulatif des objectifs :\n",
    "- Réaliser une analyse exploratoire.\n",
    "- Tester différents modèles de prédiction afin de répondre au mieux à la problématique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f714cf0-f5bc-491c-80e4-38472f4d409d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T12:28:27.081285Z",
     "iopub.status.busy": "2024-06-28T12:28:27.081285Z",
     "iopub.status.idle": "2024-06-28T12:28:27.098789Z",
     "shell.execute_reply": "2024-06-28T12:28:27.097789Z",
     "shell.execute_reply.started": "2024-06-28T12:28:27.081285Z"
    }
   },
   "source": [
    "__Sources utiles__\n",
    "\n",
    "- [Introduction à MLOps](https://ashutoshtripathi.com/2021/08/18/mlops-a-complete-guide-to-machine-learning-operations-mlops-vs-devops/)\n",
    "\n",
    "- [MLFLOW - Site de référence](https://mlflow.org/docs/latest/index.html)\n",
    "- [MLFLOW - Tutorial](https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html)\n",
    "- [MLFLOW - Tracking](https://mlflow.org/docs/latest/tracking.html)\n",
    "- [MLFLOW - Model Registry](https://mlflow.org/docs/latest/model-registry.html#)\n",
    "- [MLFLOW - Serve a model](https://mlflow.org/docs/latest/model-registry.html#serving-an-mlflow-model-from-model-registry)\n",
    "\n",
    "- [Evidently - tutorial d'analyse de Data drift](https://github.com/evidentlyai/evidently/tree/main/examples/sample_notebooks)\n",
    "- [API Flask - Démarche de mise en oeuvre](http://web.univ-ubs.fr/lmba/lardjane/python/c4.pdf)\n",
    "- [FastAPI - Démarche de mise en oeuvre](https://towardsdatascience.com/how-to-build-and-deploy-a-machine-learning-model-with-fastapi-64c505213857)\n",
    "- [Azure - Tuto déploiement application web ](https://learn.microsoft.com/fr-fr/azure/app-service/quickstart-python?tabs=flask%2Cwindows%2Cazure-portal%2Cvscode-deploy%2Cdeploy-instructions-azportal%2Cterminal-bash%2Cdeploy-instructions-zip-azcli)\n",
    "- [Tests unitaires - Unittest ou Pytest](https://www.sitepoint.com/python-unit-testing-unittest-pytest/)\n",
    "\n",
    "- [Pythonanywhere](https://www.pythonanywhere.com/)\n",
    "- [Heroku](https://www.heroku.com/)\n",
    "-[Azure webapp - Déploiement automatisé via Github](https://learn.microsoft.com/fr-fr/azure/app-service/deploy-continuous-deployment?tabs=github)\n",
    "- Streamlit ou gradio pour la mise en place d'un dashbord\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14612581-a3ef-4522-8ddd-64adf0221e56",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038e700c-aabe-4e62-9b37-d8ffa755cf32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T10:02:04.633195Z",
     "iopub.status.busy": "2024-06-29T10:02:04.633195Z",
     "iopub.status.idle": "2024-06-29T10:02:05.612841Z",
     "shell.execute_reply": "2024-06-29T10:02:05.611845Z",
     "shell.execute_reply.started": "2024-06-29T10:02:04.633195Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pendulum\n",
    "import plotly.express as px\n",
    "import ppscore as pps\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, TransformedTargetRegressor\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, VotingRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.metrics import (r2_score,\n",
    "                             root_mean_squared_error,\n",
    "                             mean_absolute_percentage_error,\n",
    "                             mean_absolute_error,\n",
    "                             max_error,\n",
    "                            )\n",
    "from sklearn.model_selection import train_test_split, learning_curve, LearningCurveDisplay\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from ydata_profiling import ProfileReport\n",
    "from yellowbrick.regressor import PredictionError, ResidualsPlot\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from settings.params import MODEL_PARAMS, SEED\n",
    "from src.make_dataset import load_data\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "set_config(display=\"diagram\", print_changed_only=False)  # display sklearn pipeline as diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d2c00-ebe7-409c-b936-a3a36a10a6b0",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c12ca6-6ee6-4a5b-bd46-b567a6c5c34b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:21.715720Z",
     "iopub.status.busy": "2024-06-29T09:03:21.714721Z",
     "iopub.status.idle": "2024-06-29T09:03:21.867996Z",
     "shell.execute_reply": "2024-06-29T09:03:21.867826Z",
     "shell.execute_reply.started": "2024-06-29T09:03:21.715720Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-30 10:35:54.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - Target name: target\n",
      "\u001b[32m2025-07-30 10:35:54.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \n",
      "Project directory: c:\\Users\\DELL\\Desktop\\mlops-project - Tweets \n",
      "Reports dir: c:\\Users\\DELL\\Desktop\\mlops-project - Tweets\\reports\n"
     ]
    }
   ],
   "source": [
    "# Set logging format\n",
    "log_fmt = \"<green>{time:YYYY-MM-DD HH:mm:ss.SSS!UTC}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - {message}\"\n",
    "logger.configure(handlers=[{\"sink\": sys.stderr, \"format\": log_fmt}])\n",
    "\n",
    "# current data\n",
    "CURRENT_DATE = pendulum.now(tz=\"UTC\")\n",
    "\n",
    "# target name definition\n",
    "TARGET_NAME = MODEL_PARAMS[\"TARGET_NAME\"]\n",
    "logger.info(f\"Target name: {TARGET_NAME}\")\n",
    "\n",
    "\n",
    "# directories\n",
    "PROJECT_DIR = Path.cwd().parent\n",
    "REPORTS_DIR = Path(PROJECT_DIR, \"reports\")\n",
    "\n",
    "logger.info(f\"\\nProject directory: {PROJECT_DIR} \\nReports dir: {REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9b5106-eb95-4b15-8655-836f5b458283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:21.869006Z",
     "iopub.status.busy": "2024-06-29T09:03:21.869006Z",
     "iopub.status.idle": "2024-06-29T09:03:22.009874Z",
     "shell.execute_reply": "2024-06-29T09:03:22.008866Z",
     "shell.execute_reply.started": "2024-06-29T09:03:21.869006Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TARGET_NAME': 'target',\n",
       " 'MIN_COMPLETION_RATE': 0.75,\n",
       " 'MIN_PPS': 0.1,\n",
       " 'DEFAULT_FEATURE_NAMES': ['Alley',\n",
       "  'BsmtQual',\n",
       "  'ExterQual',\n",
       "  'Foundation',\n",
       "  'FullBath',\n",
       "  'GarageArea',\n",
       "  'GarageCars',\n",
       "  'GarageFinish',\n",
       "  'GarageType',\n",
       "  'GrLivArea',\n",
       "  'KitchenQualMSSubClass',\n",
       "  'Neighborhood',\n",
       "  'OverallQual',\n",
       "  'TotRmsAbvGrd',\n",
       "  'building_age',\n",
       "  'remodel_age',\n",
       "  'garage_age'],\n",
       " 'TEST_SIZE': 0.25}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17884a5-99c8-452a-b3fb-7cf45931ca1b",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7288f4e1-0a82-4ab1-ad61-a4f80aeb4289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.011873Z",
     "iopub.status.busy": "2024-06-29T09:03:22.010873Z",
     "iopub.status.idle": "2024-06-29T09:03:22.199509Z",
     "shell.execute_reply": "2024-06-29T09:03:22.198509Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.011873Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data like pandas.DataFrame\n",
    "data = load_data(dataset_name=\"tweets\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "071fe3c2-3617-43b6-aca2-045339e61256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.201512Z",
     "iopub.status.busy": "2024-06-29T09:03:22.200509Z",
     "iopub.status.idle": "2024-06-29T09:03:22.373482Z",
     "shell.execute_reply": "2024-06-29T09:03:22.372486Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.201512Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f0fb9-7a60-447c-81bd-3fa18149fa7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.374486Z",
     "iopub.status.busy": "2024-06-29T09:03:22.373482Z",
     "iopub.status.idle": "2024-06-29T09:03:22.514516Z",
     "shell.execute_reply": "2024-06-29T09:03:22.514516Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.374486Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c5b45-80f1-48d6-bad9-5d95640cb17d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.674645Z",
     "iopub.status.busy": "2024-06-29T09:03:22.674645Z",
     "iopub.status.idle": "2024-06-29T09:03:22.938731Z",
     "shell.execute_reply": "2024-06-29T09:03:22.937730Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.674645Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.describe(include=\"all\") #, datetime_is_numeric=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b46232-40eb-4568-be4f-9c9f2a8a0d4d",
   "metadata": {},
   "source": [
    "# EDA: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a178177-9ff1-4b6d-b967-b328cae9a06b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.940731Z",
     "iopub.status.busy": "2024-06-29T09:03:22.939726Z",
     "iopub.status.idle": "2024-06-29T09:03:24.434116Z",
     "shell.execute_reply": "2024-06-29T09:03:24.434116Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.940731Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# barplot for missing value rate\n",
    "msno.bar(data,\n",
    "         filter=\"top\",\n",
    "         p=MODEL_PARAMS[\"MIN_COMPLETION_RATE\"],\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1e6dc-d2a7-4750-902c-79516b698c3b",
   "metadata": {},
   "source": [
    "## Target analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed730084-8eed-4d15-962a-0f1c1fc98d49",
   "metadata": {
    "tags": []
   },
   "source": [
    "Todo:\n",
    "\n",
    "    1- Ajouter un parameter dans le dictionnaire MODEL_PARAMS, appelé TARGET_NAME, qui prend la valeur \"SalePrice\"\n",
    "\n",
    "    2- Construite l'histogramme de la variable cible en récupérant le nom depuis le dictionnaire MODEL_PARAMS\n",
    "    3- Construire un 2e graphique sur la même ligne (variable target transformer en log)\n",
    "    4- Continuer l'analyse exploratoire des variables  explicatives (quanti et quali)\n",
    "    5- Faire une première sélection des variables explicatives à utiliser pour le modèle \n",
    "    6- Ajouter dans MODEL_PARAMS, un key appelé FEATURE_NAMES, qui stocke la liste des variables explicatives sélectionnées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb20318-d0eb-4dd6-94d2-a3c06bdcff13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:24.437117Z",
     "iopub.status.busy": "2024-06-29T09:03:24.436116Z",
     "iopub.status.idle": "2024-06-29T09:03:24.576283Z",
     "shell.execute_reply": "2024-06-29T09:03:24.576283Z",
     "shell.execute_reply.started": "2024-06-29T09:03:24.437117Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_PARAMS[\"TARGET_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08927768",
   "metadata": {},
   "source": [
    "### Distribution des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1d76b-ac16-4a60-a4ce-af3b8c225e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:24.578283Z",
     "iopub.status.busy": "2024-06-29T09:03:24.577279Z",
     "iopub.status.idle": "2024-06-29T09:03:24.733931Z",
     "shell.execute_reply": "2024-06-29T09:03:24.732887Z",
     "shell.execute_reply.started": "2024-06-29T09:03:24.577279Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_counts = data[TARGET_NAME].value_counts()\n",
    "target_pct = data[TARGET_NAME].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"Distribution des classes:\")\n",
    "print(f\"  Classe 0 (Non-Disaster): {target_counts[0]} ({target_pct[0]:.1f}%)\")\n",
    "print(f\"  Classe 1 (Disaster): {target_counts[1]} ({target_pct[1]:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Graphique en barres\n",
    "target_counts.plot(kind='bar', ax=axes[0], color=['skyblue', 'salmon'])\n",
    "axes[0].set_title('Distribution des Classes')\n",
    "axes[0].set_xlabel('Target (0=Non-Disaster, 1=Disaster)')\n",
    "axes[0].set_ylabel('Nombre de tweets')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Graphique en secteurs\n",
    "axes[1].pie(target_counts.values, labels=['Non-Disaster', 'Disaster'], \n",
    "           autopct='%1.1f%%', colors=['skyblue', 'salmon'])\n",
    "axes[1].set_title('Répartition des Classes')\n",
    "\n",
    "# Graphique d'équilibre\n",
    "class_balance = min(target_counts) / max(target_counts)\n",
    "axes[2].bar(['Équilibre des Classes'], [class_balance], color='orange')\n",
    "axes[2].set_title(f'Équilibre: {class_balance:.2f}')\n",
    "axes[2].set_ylabel('Ratio (min/max)')\n",
    "axes[2].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9456b",
   "metadata": {},
   "source": [
    "## Analyse des features catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd78d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📊 4. ANALYSE DES FEATURES CATÉGORIELLES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Analyse des keywords\n",
    "print(\"🔑 ANALYSE DES KEYWORDS:\")\n",
    "keyword_counts = data['keyword'].value_counts(dropna=False).head(15)\n",
    "print(f\"Top 15 keywords les plus fréquents:\")\n",
    "print(keyword_counts)\n",
    "\n",
    "# Keywords par classe\n",
    "keyword_disaster = data[data['target'] == 1]['keyword'].value_counts(dropna=False).head(10)\n",
    "keyword_normal = data[data['target'] == 0]['keyword'].value_counts(dropna=False).head(10)\n",
    "\n",
    "print(f\"\\nTop 10 keywords - Disaster tweets:\")\n",
    "print(keyword_disaster)\n",
    "print(f\"\\nTop 10 keywords - Normal tweets:\")\n",
    "print(keyword_normal)\n",
    "\n",
    "# Analyse des locations\n",
    "print(\"\\n📍 ANALYSE DES LOCATIONS:\")\n",
    "location_counts = data['location'].value_counts(dropna=False).head(15)\n",
    "print(f\"Top 15 locations les plus fréquentes:\")\n",
    "print(location_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Keywords globaux\n",
    "keyword_counts.plot(kind='barh', ax=axes[0,0], color='lightblue')\n",
    "axes[0,0].set_title('Top 15 Keywords')\n",
    "axes[0,0].set_xlabel('Fréquence')\n",
    "\n",
    "# Locations globales\n",
    "location_counts.plot(kind='barh', ax=axes[0,1], color='lightgreen')\n",
    "axes[0,1].set_title('Top 15 Locations')\n",
    "axes[0,1].set_xlabel('Fréquence')\n",
    "\n",
    "# Keywords par classe\n",
    "keyword_disaster.head(8).plot(kind='barh', ax=axes[1,0], color='salmon')\n",
    "axes[1,0].set_title('Top Keywords - Disaster Tweets')\n",
    "axes[1,0].set_xlabel('Fréquence')\n",
    "\n",
    "keyword_normal.head(8).plot(kind='barh', ax=axes[1,1], color='skyblue')\n",
    "axes[1,1].set_title('Top Keywords - Normal Tweets')\n",
    "axes[1,1].set_xlabel('Fréquence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0080ae36",
   "metadata": {},
   "source": [
    "### Analyse de texte approfondie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99642702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📊 5. ANALYSE APPROFONDIE DU TEXTE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Longueur des textes\n",
    "data['text_length'] = data['text'].str.len()\n",
    "data['word_count'] = data['text'].str.split().str.len()\n",
    "\n",
    "print(\"📏 STATISTIQUES DE LONGUEUR:\")\n",
    "length_stats = data.groupby('target')[['text_length', 'word_count']].describe()\n",
    "print(length_stats)\n",
    "\n",
    "# Caractères spéciaux\n",
    "data['url_count'] = data['text'].str.count(r'http\\S+|www\\S+')\n",
    "data['mention_count'] = data['text'].str.count(r'@\\w+')\n",
    "data['hashtag_count'] = data['text'].str.count(r'#\\w+')\n",
    "data['exclamation_count'] = data['text'].str.count(r'!')\n",
    "data['question_count'] = data['text'].str.count(r'\\?')\n",
    "data['caps_count'] = data['text'].str.count(r'[A-Z]')\n",
    "\n",
    "print(\"\\n🔍 CARACTÈRES SPÉCIAUX par classe:\")\n",
    "special_chars = ['url_count', 'mention_count', 'hashtag_count', 'exclamation_count', 'question_count', 'caps_count']\n",
    "special_stats = data.groupby('target')[special_chars].mean()\n",
    "print(special_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7299e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des longueurs\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Distribution longueur caractères\n",
    "for i, target in enumerate([0, 1]):\n",
    "    df = data[data[TARGET_NAME] == 0]['text_length']\n",
    "    axes[0, 0].hist(df, bins=30, alpha=0.7, label=f'Target {target}')\n",
    "axes[0, 0].set_title('Distribution - Longueur en Caractères')\n",
    "axes[0, 0].set_xlabel('Nombre de caractères')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Distribution nombre de mots\n",
    "for i, target in enumerate([0, 1]):\n",
    "    df = data[data[TARGET_NAME] == target]['word_count']\n",
    "    axes[0, 1].hist(df, bins=30, alpha=0.7, label=f'Target {target}')\n",
    "axes[0, 1].set_title('Distribution - Nombre de Mots')\n",
    "axes[0, 1].set_xlabel('Nombre de mots')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Boxplot longueurs par classe\n",
    "data.boxplot(column='text_length', by='target', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Longueur par Classe')\n",
    "axes[0, 2].set_xlabel('Target')\n",
    "\n",
    "# Caractères spéciaux\n",
    "special_stats.T.plot(kind='bar', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Caractères Spéciaux par Classe')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].legend(['Non-Disaster', 'Disaster'])\n",
    "\n",
    "# Corrélation longueur vs target\n",
    "correlation_length = data[['text_length', 'word_count', 'target']].corr()\n",
    "sns.heatmap(correlation_length, annot=True, ax=axes[1, 1], cmap='coolwarm')\n",
    "axes[1, 1].set_title('Corrélation Longueur vs Target')\n",
    "\n",
    "# Distribution caps par classe\n",
    "for i, target in enumerate([0, 1]):\n",
    "    df = data[data[TARGET_NAME] == target]['caps_count']\n",
    "    axes[1, 2].hist(df, bins=20, alpha=0.7, label=f'Target {target}')\n",
    "axes[1, 2].set_title('Distribution - Lettres Majuscules')\n",
    "axes[1, 2].set_xlabel('Nombre de majuscules')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757fb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Télécharger NLTK\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648e7c2",
   "metadata": {},
   "source": [
    "### Analyse des mots les plus fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour l'analyse textuelle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "print(\"\\n📊 6. ANALYSE DES MOTS LES PLUS FRÉQUENTS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def clean_text_for_analysis(text):\n",
    "    \"\"\"Nettoyage basique pour l'analyse des mots\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Préparer les textes\n",
    "disaster_texts = data[data['target'] == 1]['text'].apply(clean_text_for_analysis)\n",
    "normal_texts = data[data['target'] == 0]['text'].apply(clean_text_for_analysis)\n",
    "\n",
    "# Mots les plus fréquents\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_top_words(texts, n=20):\n",
    "    \"\"\"Obtenir les mots les plus fréquents\"\"\"\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "        all_words.extend(words)\n",
    "    return Counter(all_words).most_common(n)\n",
    "\n",
    "disaster_words = get_top_words(disaster_texts)\n",
    "normal_words = get_top_words(normal_texts)\n",
    "\n",
    "\n",
    "# Visualisation des mots fréquents\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Disaster words\n",
    "disaster_df = pd.DataFrame(disaster_words, columns=['Word', 'Count'])\n",
    "disaster_df.plot(x='Word', y='Count', kind='barh', ax=axes[0], color='salmon')\n",
    "axes[0].set_title('Top 20 Mots - Disaster Tweets')\n",
    "axes[0].set_xlabel('Fréquence')\n",
    "\n",
    "# Normal words\n",
    "normal_df = pd.DataFrame(normal_words, columns=['Word', 'Count'])\n",
    "normal_df.plot(x='Word', y='Count', kind='barh', ax=axes[1], color='skyblue')\n",
    "axes[1].set_title('Top 20 Mots - Normal Tweets')\n",
    "axes[1].set_xlabel('Fréquence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2652f89",
   "metadata": {},
   "source": [
    "### Word Clouds (Nuages de points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de6d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n📊 7. NUAGES DE MOTS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Word cloud pour disaster tweets\n",
    "    disaster_text_combined = ' '.join(disaster_texts)\n",
    "    wordcloud_disaster = WordCloud(width=800, height=400, \n",
    "                                 background_color='white',\n",
    "                                 stopwords=stop_words,\n",
    "                                 max_words=100).generate(disaster_text_combined)\n",
    "\n",
    "    # Word cloud pour normal tweets\n",
    "    normal_text_combined = ' '.join(normal_texts)\n",
    "    wordcloud_normal = WordCloud(width=800, height=400, \n",
    "                               background_color='white',\n",
    "                               stopwords=stop_words,\n",
    "                               max_words=100).generate(normal_text_combined)\n",
    "\n",
    "    # Affichage\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "    axes[0].imshow(wordcloud_disaster, interpolation='bilinear')\n",
    "    axes[0].set_title('Word Cloud - Disaster Tweets', fontsize=16)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(wordcloud_normal, interpolation='bilinear')\n",
    "    axes[1].set_title('Word Cloud - Normal Tweets', fontsize=16)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠️  WordCloud non disponible. Installer avec: pip install wordcloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517cfc70",
   "metadata": {},
   "source": [
    "### Analyse des corrélations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📊 8. ANALYSE DE CORRÉLATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Matrice de corrélation pour les variables numériques\n",
    "numeric_features = ['text_length', 'word_count', 'url_count', 'mention_count', \n",
    "                   'hashtag_count', 'exclamation_count', 'question_count', 'caps_count', 'target']\n",
    "\n",
    "correlation_matrix = data[numeric_features].corr()\n",
    "\n",
    "print(\"🔗 Corrélations avec la variable target:\")\n",
    "target_correlations = correlation_matrix['target'].sort_values(key=abs, ascending=False)\n",
    "print(target_correlations)\n",
    "\n",
    "# Heatmap de corrélation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Matrice de Corrélation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617dc2b0",
   "metadata": {},
   "source": [
    "### Analyse Comparative Détaillée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e85b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📊 9. ANALYSE COMPARATIVE DÉTAILLÉE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Tests statistiques\n",
    "from scipy.stats import ttest_ind, chi2_contingency\n",
    "\n",
    "print(\"🧪 TESTS STATISTIQUES:\")\n",
    "\n",
    "# Test t pour longueur de texte\n",
    "disaster_lengths = data[data['target'] == 1]['text_length']\n",
    "normal_lengths = data[data['target'] == 0]['text_length']\n",
    "t_stat, p_value = ttest_ind(disaster_lengths, normal_lengths)\n",
    "\n",
    "print(f\"Test t - Longueur texte: t={t_stat:.3f}, p={p_value:.6f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"  ✅ Différence significative dans la longueur des textes\")\n",
    "else:\n",
    "    print(\"  ❌ Pas de différence significative dans la longueur des textes\")\n",
    "\n",
    "# Test du chi-carré pour keywords\n",
    "if not data['keyword'].isna().all():\n",
    "    keyword_target_crosstab = pd.crosstab(data['keyword'].fillna('Missing'), data['target'])\n",
    "    chi2, p_val, dof, expected = chi2_contingency(keyword_target_crosstab)\n",
    "    print(f\"Test Chi² - Keywords: χ²={chi2:.3f}, p={p_val:.6f}\")\n",
    "    if p_val < 0.05:\n",
    "        print(\"  ✅ Association significative entre keywords et target\")\n",
    "    else:\n",
    "        print(\"  ❌ Pas d'association significative entre keywords et target\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96b26f-08a8-491f-88f6-1758dffba7fe",
   "metadata": {},
   "source": [
    "## Prétraitement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f69ca4-89d7-4b96-9924-22b66335612b",
   "metadata": {},
   "source": [
    "#### IMPORTS COMPLETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b121fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Pour Word2Vec et FastText\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "import re\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637abaee",
   "metadata": {},
   "source": [
    "#### PREPROCESSING COMPLET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d42b5258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'keyword', 'location', 'text', 'target', 'url_count',\n",
      "       'text_length', 'mention_count', 'question_count', 'caps_count',\n",
      "       'exclamation_count', 'hashtag_count', 'word_count', 'urgency_score',\n",
      "       'social_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_features(df):\n",
    "    \"\"\"Extrait TOUTES les features importantes selon votre analyse\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    # Top features (dans l'ordre de corrélation)\n",
    "    features['url_count'] = df['text'].str.count(r'http\\S+|www\\S+')           # +0.196\n",
    "    features['text_length'] = df['text'].str.len()                            # +0.182\n",
    "    features['mention_count'] = df['text'].str.count(r'@\\w+')                 # -0.103\n",
    "    features['question_count'] = df['text'].str.count(r'\\?')                  # -0.084\n",
    "    features['caps_count'] = df['text'].str.count(r'[A-Z]')                   # +0.078\n",
    "    features['exclamation_count'] = df['text'].str.count(r'!')                # -0.075\n",
    "    features['hashtag_count'] = df['text'].str.count(r'#\\w+')                 # +0.052\n",
    "    features['word_count'] = df['text'].str.split().str.len()                 # +0.040\n",
    "    \n",
    "    # Features composées\n",
    "    features['urgency_score'] = features['url_count'] + features['caps_count']\n",
    "    features['social_score'] = features['mention_count'] + features['hashtag_count']\n",
    "    \n",
    "    return features\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Nettoyage intelligent du texte\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocessing complet - remplace votre classe\"\"\"\n",
    "    print(\"🔧 Preprocessing complet...\")\n",
    "    \n",
    "    # Features numériques\n",
    "    df_features = extract_features(df)\n",
    "    \n",
    "    # Nettoyage du texte\n",
    "    df_features['clean_text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    # Encodage des catégories\n",
    "    le_location = LabelEncoder()\n",
    "    le_keyword = LabelEncoder()\n",
    "    \n",
    "    df_features['location_encoded'] = le_location.fit_transform(df['location'].fillna('unknown'))\n",
    "    df_features['keyword_encoded'] = le_keyword.fit_transform(df['keyword'].fillna('none'))\n",
    "    \n",
    "    return df_features, le_location, le_keyword\n",
    "\n",
    "print(extract_features(data).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944ca47",
   "metadata": {},
   "source": [
    "#### TOUS LES VECTORIZERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e54e221-7315-4a73-9991-2445cc7ae18a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:25.675870Z",
     "iopub.status.busy": "2024-06-29T09:03:25.674867Z",
     "iopub.status.idle": "2024-06-29T09:03:25.848597Z",
     "shell.execute_reply": "2024-06-29T09:03:25.848597Z",
     "shell.execute_reply.started": "2024-06-29T09:03:25.675870Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tfidf_features(X_train, X_test, max_features=5000):\n",
    "    \"\"\"TF-IDF Vectorization\"\"\"\n",
    "    print(\"  📝 TF-IDF...\")\n",
    "    tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(1,2))\n",
    "    X_train_vec = tfidf.fit_transform(X_train['clean_text'])\n",
    "    X_test_vec = tfidf.transform(X_test['clean_text'])\n",
    "    return X_train_vec, X_test_vec, tfidf\n",
    "\n",
    "def get_word2vec_features(X_train, X_test, vector_size=100):\n",
    "    \"\"\"Word2Vec Vectorization\"\"\"\n",
    "    print(\"  🧠 Word2Vec...\")\n",
    "    \n",
    "    # Tokenisation\n",
    "    train_tokens = [text.split() for text in X_train['clean_text']]\n",
    "    test_tokens = [text.split() for text in X_test['clean_text']]\n",
    "    \n",
    "    # Entraînement Word2Vec\n",
    "    w2v_model = Word2Vec(sentences=train_tokens, vector_size=vector_size, window=5, min_count=2, workers=4)\n",
    "    \n",
    "    # Moyenne des vecteurs par document\n",
    "    def average_vectors(tokens, model):\n",
    "        vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "    \n",
    "    X_train_vec = np.array([average_vectors(tokens, w2v_model) for tokens in train_tokens])\n",
    "    X_test_vec = np.array([average_vectors(tokens, w2v_model) for tokens in test_tokens])\n",
    "    \n",
    "    return csr_matrix(X_train_vec), csr_matrix(X_test_vec), w2v_model\n",
    "\n",
    "def get_fasttext_features(X_train, X_test, vector_size=100):\n",
    "    \"\"\"FastText Vectorization\"\"\"\n",
    "    print(\"  ⚡ FastText...\")\n",
    "    \n",
    "    # Tokenisation\n",
    "    train_tokens = [text.split() for text in X_train['clean_text']]\n",
    "    test_tokens = [text.split() for text in X_test['clean_text']]\n",
    "    \n",
    "    # Entraînement FastText\n",
    "    ft_model = FastText(sentences=train_tokens, vector_size=vector_size, window=5, min_count=2, workers=4)\n",
    "    \n",
    "    # Moyenne des vecteurs par document\n",
    "    def average_vectors(tokens, model):\n",
    "        vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "    \n",
    "    X_train_vec = np.array([average_vectors(tokens, ft_model) for tokens in train_tokens])\n",
    "    X_test_vec = np.array([average_vectors(tokens, ft_model) for tokens in test_tokens])\n",
    "    \n",
    "    return csr_matrix(X_train_vec), csr_matrix(X_test_vec), ft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036992e",
   "metadata": {},
   "source": [
    "#### MODÈLES À TESTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "536e9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    'Logistic': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss', verbosity=0),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "vectorizers_config = {\n",
    "    'TF-IDF': get_tfidf_features,\n",
    "    'Word2Vec': get_word2vec_features,\n",
    "    'FastText': get_fasttext_features\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52765c",
   "metadata": {},
   "source": [
    "#### TEST TOUTES COMBINAISONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56050150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_combinations(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Test TOUTES les combinaisons modèle + vectorizer\"\"\"\n",
    "    \n",
    "    print(\"🚀 TEST DE TOUTES LES COMBINAISONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Features numériques (communes à tous)\n",
    "    num_features = ['url_count', 'text_length', 'mention_count', 'question_count',\n",
    "                   'caps_count', 'exclamation_count', 'hashtag_count', 'word_count',\n",
    "                   'urgency_score', 'social_score', 'location_encoded', 'keyword_encoded']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train[num_features])\n",
    "    X_test_num = scaler.transform(X_test[num_features])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Boucle sur tous les vectorizers\n",
    "    for vec_name, vec_func in vectorizers_config.items():\n",
    "        print(f\"\\n📊 VECTORIZER: {vec_name}\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        try:\n",
    "            # Obtenir les features textuelles\n",
    "            X_train_text, X_test_text, vectorizer = vec_func(X_train, X_test)\n",
    "            \n",
    "            # Combiner avec les features numériques\n",
    "            X_train_final = hstack([X_train_text, csr_matrix(X_train_num)])\n",
    "            X_test_final = hstack([X_test_text, csr_matrix(X_test_num)])\n",
    "            \n",
    "            # Tester tous les modèles avec ce vectorizer\n",
    "            for model_name, model in models_config.items():\n",
    "                print(f\"  🎯 {model_name}...\", end=\" \")\n",
    "                \n",
    "                try:\n",
    "                    # Entraînement\n",
    "                    model.fit(X_train_final, y_train)\n",
    "                    \n",
    "                    # Prédictions\n",
    "                    y_pred = model.predict(X_test_final)\n",
    "                    \n",
    "                    # Métriques\n",
    "                    f1 = f1_score(y_test, y_pred)\n",
    "                    accuracy = accuracy_score(y_test, y_pred)\n",
    "                    \n",
    "                    # Cross-validation\n",
    "                    cv_scores = cross_val_score(model, X_train_final, y_train, cv=3, scoring='f1')\n",
    "                    cv_mean = cv_scores.mean()\n",
    "                    cv_std = cv_scores.std()\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Vectorizer': vec_name,\n",
    "                        'Model': model_name,\n",
    "                        'F1_Test': f1,\n",
    "                        'Accuracy_Test': accuracy,\n",
    "                        'F1_CV_Mean': cv_mean,\n",
    "                        'F1_CV_Std': cv_std,\n",
    "                        'Combination': f\"{vec_name} + {model_name}\"\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"F1: {f1:.4f} | CV: {cv_mean:.4f}±{cv_std:.4f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Erreur: {str(e)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur avec {vec_name}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed611f",
   "metadata": {},
   "source": [
    "#### Analyse des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46160ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyse complète des résultats\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"❌ Aucun résultat à analyser\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df_sorted = df.sort_values('F1_Test', ascending=False)\n",
    "    \n",
    "    print(\"\\n🏆 TOP 10 COMBINAISONS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(df_sorted.head(10)[['Combination', 'F1_Test', 'Accuracy_Test', 'F1_CV_Mean']].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n🥇 MEILLEURE COMBINAISON:\")\n",
    "    best = df_sorted.iloc[0]\n",
    "    print(f\"   {best['Combination']}\")\n",
    "    print(f\"   F1 Test: {best['F1_Test']:.4f}\")\n",
    "    print(f\"   Accuracy: {best['Accuracy_Test']:.4f}\")\n",
    "    print(f\"   F1 CV: {best['F1_CV_Mean']:.4f} ± {best['F1_CV_Std']:.4f}\")\n",
    "    \n",
    "    # Analyse par vectorizer\n",
    "    print(f\"\\n📈 ANALYSE PAR VECTORIZER:\")\n",
    "    vec_analysis = df.groupby('Vectorizer').agg({\n",
    "        'F1_Test': ['mean', 'max', 'std'],\n",
    "        'F1_CV_Mean': 'mean'\n",
    "    }).round(4)\n",
    "    print(vec_analysis)\n",
    "    \n",
    "    # Analyse par modèle\n",
    "    print(f\"\\n🤖 ANALYSE PAR MODÈLE:\")\n",
    "    model_analysis = df.groupby('Model').agg({\n",
    "        'F1_Test': ['mean', 'max', 'std'],\n",
    "        'F1_CV_Mean': 'mean'\n",
    "    }).round(4)\n",
    "    print(model_analysis)\n",
    "    \n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f499d",
   "metadata": {},
   "source": [
    "#### Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "558ad004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ QUICK START - Test complet automatique\n",
      "==================================================\n",
      "🔧 Preprocessing complet...\n",
      "🔧 Preprocessing complet...\n",
      "🚀 TEST DE TOUTES LES COMBINAISONS\n",
      "==================================================\n",
      "\n",
      "📊 VECTORIZER: TF-IDF\n",
      "-------------------------\n",
      "  📝 TF-IDF...\n",
      "  🎯 Logistic... F1: 0.7530 | CV: 0.7277±0.0058\n",
      "  🎯 RandomForest... F1: 0.6955 | CV: 0.6758±0.0092\n",
      "  🎯 XGBoost... F1: 0.7221 | CV: 0.7003±0.0086\n",
      "  🎯 LightGBM... F1: 0.7331 | CV: 0.6987±0.0083\n",
      "\n",
      "📊 VECTORIZER: Word2Vec\n",
      "-------------------------\n",
      "  🧠 Word2Vec...\n",
      "  🎯 Logistic... F1: 0.6128 | CV: 0.5993±0.0061\n",
      "  🎯 RandomForest... F1: 0.6386 | CV: 0.6349±0.0039\n",
      "  🎯 XGBoost... F1: 0.6567 | CV: 0.6498±0.0105\n",
      "  🎯 LightGBM... F1: 0.6889 | CV: 0.6596±0.0020\n",
      "\n",
      "📊 VECTORIZER: FastText\n",
      "-------------------------\n",
      "  ⚡ FastText...\n",
      "  🎯 Logistic... F1: 0.5683 | CV: 0.5607±0.0085\n",
      "  🎯 RandomForest... F1: 0.6323 | CV: 0.6368±0.0126\n",
      "  🎯 XGBoost... F1: 0.6874 | CV: 0.6543±0.0118\n",
      "  🎯 LightGBM... F1: 0.6834 | CV: 0.6627±0.0094\n",
      "\n",
      "🏆 TOP 10 COMBINAISONS\n",
      "==================================================\n",
      "            Combination  F1_Test  Accuracy_Test  F1_CV_Mean\n",
      "      TF-IDF + Logistic 0.753036       0.799737    0.727661\n",
      "      TF-IDF + LightGBM 0.733061       0.785292    0.698744\n",
      "       TF-IDF + XGBoost 0.722130       0.780696    0.700260\n",
      "  TF-IDF + RandomForest 0.695499       0.773473    0.675841\n",
      "    Word2Vec + LightGBM 0.688871       0.748523    0.659589\n",
      "     FastText + XGBoost 0.687403       0.736047    0.654262\n",
      "    FastText + LightGBM 0.683401       0.743270    0.662724\n",
      "     Word2Vec + XGBoost 0.656669       0.714380    0.649792\n",
      "Word2Vec + RandomForest 0.638614       0.712410    0.634936\n",
      "FastText + RandomForest 0.632280       0.709783    0.636828\n",
      "\n",
      "🥇 MEILLEURE COMBINAISON:\n",
      "   TF-IDF + Logistic\n",
      "   F1 Test: 0.7530\n",
      "   Accuracy: 0.7997\n",
      "   F1 CV: 0.7277 ± 0.0058\n",
      "\n",
      "📈 ANALYSE PAR VECTORIZER:\n",
      "           F1_Test                 F1_CV_Mean\n",
      "              mean     max     std       mean\n",
      "Vectorizer                                   \n",
      "FastText    0.6428  0.6874  0.0557     0.6286\n",
      "TF-IDF      0.7259  0.7530  0.0240     0.7006\n",
      "Word2Vec    0.6492  0.6889  0.0320     0.6359\n",
      "\n",
      "🤖 ANALYSE PAR MODÈLE:\n",
      "             F1_Test                 F1_CV_Mean\n",
      "                mean     max     std       mean\n",
      "Model                                          \n",
      "LightGBM      0.7018  0.7331  0.0272     0.6737\n",
      "Logistic      0.6447  0.7530  0.0964     0.6292\n",
      "RandomForest  0.6555  0.6955  0.0348     0.6492\n",
      "XGBoost       0.6887  0.7221  0.0328     0.6681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   Vectorizer         Model   F1_Test  Accuracy_Test  F1_CV_Mean  F1_CV_Std  \\\n",
       " 0      TF-IDF      Logistic  0.753036       0.799737    0.727661   0.005801   \n",
       " 3      TF-IDF      LightGBM  0.733061       0.785292    0.698744   0.008280   \n",
       " 2      TF-IDF       XGBoost  0.722130       0.780696    0.700260   0.008590   \n",
       " 1      TF-IDF  RandomForest  0.695499       0.773473    0.675841   0.009174   \n",
       " 7    Word2Vec      LightGBM  0.688871       0.748523    0.659589   0.001971   \n",
       " 10   FastText       XGBoost  0.687403       0.736047    0.654262   0.011773   \n",
       " 11   FastText      LightGBM  0.683401       0.743270    0.662724   0.009449   \n",
       " 6    Word2Vec       XGBoost  0.656669       0.714380    0.649792   0.010472   \n",
       " 5    Word2Vec  RandomForest  0.638614       0.712410    0.634936   0.003852   \n",
       " 9    FastText  RandomForest  0.632280       0.709783    0.636828   0.012617   \n",
       " 4    Word2Vec      Logistic  0.612800       0.682206    0.599289   0.006132   \n",
       " 8    FastText      Logistic  0.568278       0.665791    0.560698   0.008471   \n",
       " \n",
       "                 Combination  \n",
       " 0         TF-IDF + Logistic  \n",
       " 3         TF-IDF + LightGBM  \n",
       " 2          TF-IDF + XGBoost  \n",
       " 1     TF-IDF + RandomForest  \n",
       " 7       Word2Vec + LightGBM  \n",
       " 10       FastText + XGBoost  \n",
       " 11      FastText + LightGBM  \n",
       " 6        Word2Vec + XGBoost  \n",
       " 5   Word2Vec + RandomForest  \n",
       " 9   FastText + RandomForest  \n",
       " 4       Word2Vec + Logistic  \n",
       " 8       FastText + Logistic  ,\n",
       " [{'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'Logistic',\n",
       "   'F1_Test': 0.7530364372469636,\n",
       "   'Accuracy_Test': 0.7997373604727511,\n",
       "   'F1_CV_Mean': 0.7276606237936297,\n",
       "   'F1_CV_Std': 0.005801002031715946,\n",
       "   'Combination': 'TF-IDF + Logistic'},\n",
       "  {'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'RandomForest',\n",
       "   'F1_Test': 0.6954986760812003,\n",
       "   'Accuracy_Test': 0.7734734077478661,\n",
       "   'F1_CV_Mean': 0.6758405939711695,\n",
       "   'F1_CV_Std': 0.009173514875022739,\n",
       "   'Combination': 'TF-IDF + RandomForest'},\n",
       "  {'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'XGBoost',\n",
       "   'F1_Test': 0.7221297836938436,\n",
       "   'Accuracy_Test': 0.7806959947472094,\n",
       "   'F1_CV_Mean': 0.7002604639506317,\n",
       "   'F1_CV_Std': 0.008590499205535725,\n",
       "   'Combination': 'TF-IDF + XGBoost'},\n",
       "  {'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'LightGBM',\n",
       "   'F1_Test': 0.7330612244897959,\n",
       "   'Accuracy_Test': 0.7852921864740644,\n",
       "   'F1_CV_Mean': 0.6987440000633877,\n",
       "   'F1_CV_Std': 0.008279563091651485,\n",
       "   'Combination': 'TF-IDF + LightGBM'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'Logistic',\n",
       "   'F1_Test': 0.6128,\n",
       "   'Accuracy_Test': 0.6822061720288903,\n",
       "   'F1_CV_Mean': 0.5992892813624447,\n",
       "   'F1_CV_Std': 0.006131717155633185,\n",
       "   'Combination': 'Word2Vec + Logistic'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'RandomForest',\n",
       "   'F1_Test': 0.6386138613861386,\n",
       "   'Accuracy_Test': 0.7124097176625082,\n",
       "   'F1_CV_Mean': 0.6349362567853308,\n",
       "   'F1_CV_Std': 0.0038520808575988113,\n",
       "   'Combination': 'Word2Vec + RandomForest'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'XGBoost',\n",
       "   'F1_Test': 0.6566692975532754,\n",
       "   'Accuracy_Test': 0.7143795141168746,\n",
       "   'F1_CV_Mean': 0.6497920050055791,\n",
       "   'F1_CV_Std': 0.010472076124771369,\n",
       "   'Combination': 'Word2Vec + XGBoost'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'LightGBM',\n",
       "   'F1_Test': 0.6888708367181153,\n",
       "   'Accuracy_Test': 0.7485226526592252,\n",
       "   'F1_CV_Mean': 0.6595892935854281,\n",
       "   'F1_CV_Std': 0.0019709028305658796,\n",
       "   'Combination': 'Word2Vec + LightGBM'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'Logistic',\n",
       "   'F1_Test': 0.5682782018659881,\n",
       "   'Accuracy_Test': 0.6657912015758372,\n",
       "   'F1_CV_Mean': 0.5606983547424431,\n",
       "   'F1_CV_Std': 0.008470814308984238,\n",
       "   'Combination': 'FastText + Logistic'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'RandomForest',\n",
       "   'F1_Test': 0.632279534109817,\n",
       "   'Accuracy_Test': 0.7097833223900197,\n",
       "   'F1_CV_Mean': 0.6368282487813725,\n",
       "   'F1_CV_Std': 0.012616721634197319,\n",
       "   'Combination': 'FastText + RandomForest'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'XGBoost',\n",
       "   'F1_Test': 0.687402799377916,\n",
       "   'Accuracy_Test': 0.7360472751149048,\n",
       "   'F1_CV_Mean': 0.6542622392926707,\n",
       "   'F1_CV_Std': 0.01177294641979108,\n",
       "   'Combination': 'FastText + XGBoost'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'LightGBM',\n",
       "   'F1_Test': 0.6834008097165992,\n",
       "   'Accuracy_Test': 0.7432698621142482,\n",
       "   'F1_CV_Mean': 0.6627238002203779,\n",
       "   'F1_CV_Std': 0.009448901779433827,\n",
       "   'Combination': 'FastText + LightGBM'}])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quick_start(df, target_col='target'):\n",
    "    \"\"\"Démarrage rapide - teste tout automatiquement\"\"\"\n",
    "    \n",
    "    print(\"⚡ QUICK START - Test complet automatique\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Split des données\n",
    "    X = df[['text', 'location', 'keyword']]\n",
    "    y = df[target_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Preprocessing\n",
    "    X_train_processed, _, _ = preprocess_data(X_train)\n",
    "    X_test_processed, _, _ = preprocess_data(X_test)\n",
    "    \n",
    "    # Test toutes les combinaisons\n",
    "    results = test_all_combinations(X_train_processed, X_test_processed, y_train, y_test)\n",
    "    \n",
    "    # Analyse\n",
    "    df_results = analyze_results(results)\n",
    "    \n",
    "    return df_results, results \n",
    "\n",
    "quick_start(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0149b-accd-4144-b485-f8b5d0050b5d",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf730cb1",
   "metadata": {},
   "source": [
    "#### Prétraitement sur les données (Split données + TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f3ccf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Preprocessing complet...\n",
      "🔧 Preprocessing complet...\n",
      "  📝 TF-IDF...\n"
     ]
    }
   ],
   "source": [
    "# Étape 1 : split des colonnes utiles\n",
    "X = data[['text', 'location', 'keyword']]\n",
    "y = data['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Étape 2 : preprocessing (déjà défini dans ton code)\n",
    "X_train_processed, _, _ = preprocess_data(X_train_raw)\n",
    "X_test_processed, _, _ = preprocess_data(X_test_raw)\n",
    "\n",
    "# Étape 3 : vectorisation TF-IDF (déjà définie aussi)\n",
    "X_train_vec, X_test_vec, tfidf = get_tfidf_features(X_train_processed, X_test_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df35727",
   "metadata": {},
   "source": [
    "#### Optimisation : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "290b332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params LR: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best F1 CV LR: 0.7389304268413587\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 2, 3],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    param_grid,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_lr.fit(X_train_vec, y_train)\n",
    "print(\"Best params LR:\", grid_lr.best_params_)\n",
    "print(\"Best F1 CV LR:\", grid_lr.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076888e8",
   "metadata": {},
   "source": [
    "#### Optimisation : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params XGB: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 800}\n",
      "Best F1 CV XGB: 0.715595017305107\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [700, 800, 600],\n",
    "    'max_depth': [7, 5, 6],\n",
    "    'learning_rate': [0.5, 0.3, 0.2]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    param_grid_xgb,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_xgb.fit(X_train_vec, y_train)\n",
    "print(\"Best params XGB:\", grid_xgb.best_params_)\n",
    "print(\"Best F1 CV XGB:\", grid_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6258c1b",
   "metadata": {},
   "source": [
    "#### Optimisation : LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5eed4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params LGBM: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}\n",
      "Best F1 CV LGBM: 0.6870690104510798\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "param_grid_lgb = {\n",
    "    'n_estimators': [600, 400, 200],\n",
    "    'max_depth': [-1, -3, 0],\n",
    "    'learning_rate': [0.05, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "grid_lgb = GridSearchCV(\n",
    "    lgb.LGBMClassifier(random_state=42),\n",
    "    param_grid_lgb,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_lgb.fit(X_train_vec, y_train)\n",
    "print(\"Best params LGBM:\", grid_lgb.best_params_)\n",
    "print(\"Best F1 CV LGBM:\", grid_lgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e7c45",
   "metadata": {},
   "source": [
    "#### Évaluation sur test set du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5743b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 test: 0.75642965204236\n",
      "Accuracy test: 0.788575180564675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.81       869\n",
      "           1       0.75      0.76      0.76       654\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.78      0.79      0.78      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "best_model = grid_lr.best_estimator_\n",
    "y_pred = best_model.predict(X_test_vec)\n",
    "\n",
    "print(\"F1 test:\", f1_score(y_test, y_pred))\n",
    "print(\"Accuracy test:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b77837-3e2c-48ab-9b8c-477319114c1d",
   "metadata": {},
   "source": [
    "## Data profiling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32f3c9bb-e15f-4c6b-9bdb-eb44186a864e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# https://github.com/ydataai/ydata-profiling: générer le rapport des données avec ProfileReport\n",
    "# generate report\n",
    "profile = ProfileReport(data, title=\"House price - Report\")\n",
    "\n",
    "# save report to html\n",
    "profile.to_file(Path(REPORTS_DIR, \"house_price_profiling.html\"))\n",
    "\n",
    "# Display the report in the notebook\n",
    "# profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1834a8-4ff6-4ac7-bf91-1b55df9727fa",
   "metadata": {},
   "source": [
    "## Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa3d3a-08c3-4a4a-9181-9f3311ab72a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:30.433303Z",
     "iopub.status.busy": "2024-06-29T09:03:30.433303Z",
     "iopub.status.idle": "2024-06-29T09:03:30.665064Z",
     "shell.execute_reply": "2024-06-29T09:03:30.663564Z",
     "shell.execute_reply.started": "2024-06-29T09:03:30.433303Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c7e3f-7d53-4574-bdf3-73ac9831642e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:30.667064Z",
     "iopub.status.busy": "2024-06-29T09:03:30.666065Z",
     "iopub.status.idle": "2024-06-29T09:03:31.924565Z",
     "shell.execute_reply": "2024-06-29T09:03:31.923565Z",
     "shell.execute_reply.started": "2024-06-29T09:03:30.667064Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictive Power Score (PPS) : https://github.com/8080labs/ppscore/\n",
    "pps_predictors = pps.predictors(df=data.drop([\"Id\", \"YrSold\", \"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\"], axis=1),\n",
    "                                y=TARGET_NAME, output=\"df\", random_seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf8c23-ecfd-4902-a613-442bf4b304ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:31.926813Z",
     "iopub.status.busy": "2024-06-29T09:03:31.925566Z",
     "iopub.status.idle": "2024-06-29T09:03:32.207505Z",
     "shell.execute_reply": "2024-06-29T09:03:32.206333Z",
     "shell.execute_reply.started": "2024-06-29T09:03:31.926316Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pps_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56574238-6d8d-4a3a-a272-938d1a462343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:32.209158Z",
     "iopub.status.busy": "2024-06-29T09:03:32.209158Z",
     "iopub.status.idle": "2024-06-29T09:03:32.442537Z",
     "shell.execute_reply": "2024-06-29T09:03:32.442537Z",
     "shell.execute_reply.started": "2024-06-29T09:03:32.209158Z"
    }
   },
   "outputs": [],
   "source": [
    "pps.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ecc0cc-a38f-44d9-b5dd-dd490961d472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:32.443537Z",
     "iopub.status.busy": "2024-06-29T09:03:32.443537Z",
     "iopub.status.idle": "2024-06-29T09:03:32.678555Z",
     "shell.execute_reply": "2024-06-29T09:03:32.677447Z",
     "shell.execute_reply.started": "2024-06-29T09:03:32.443537Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if there are invalide pps scores in the output\n",
    "pps_predictors.is_valid_score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59092c72-7e55-4c72-8e61-6579b2b48d66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:32.679553Z",
     "iopub.status.busy": "2024-06-29T09:03:32.679553Z",
     "iopub.status.idle": "2024-06-29T09:03:32.943111Z",
     "shell.execute_reply": "2024-06-29T09:03:32.942115Z",
     "shell.execute_reply.started": "2024-06-29T09:03:32.679553Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get feature names\n",
    "FEATURE_NAMES = pps_predictors.loc[pps_predictors.ppscore >= MODEL_PARAMS[\"MIN_PPS\"], \"x\"].values\n",
    "set(FEATURE_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c5598-42bc-4d5f-9928-0fae68c34430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T09:43:54.012805Z",
     "iopub.status.busy": "2024-06-28T09:43:54.012805Z",
     "iopub.status.idle": "2024-06-28T09:43:54.523844Z",
     "shell.execute_reply": "2024-06-28T09:43:54.522757Z",
     "shell.execute_reply.started": "2024-06-28T09:43:54.012805Z"
    }
   },
   "source": [
    "__Data leakage__\n",
    "\n",
    "Attention à la fuite des données.\n",
    "\n",
    "Des variables importantes alors qu'elles ne seront pas disponibles lors de la prédiction (exemple: SaleCondition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a61b0-91bd-42d0-b195-bbe0323cf57b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:32.944111Z",
     "iopub.status.busy": "2024-06-29T09:03:32.944111Z",
     "iopub.status.idle": "2024-06-29T09:03:33.802041Z",
     "shell.execute_reply": "2024-06-29T09:03:33.801041Z",
     "shell.execute_reply.started": "2024-06-29T09:03:32.944111Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.barplot(data=pps_predictors.loc[lambda dfr: dfr.ppscore > 0], y=\"x\", x=\"ppscore\", orient=\"h\")\n",
    "ax.set_title(F\"Predictive Power Score (PPS) for {TARGET_NAME}\")\n",
    "\n",
    "# add the annotation\n",
    "ax.bar_label(ax.containers[-1], fmt='%.3f', label_type='edge');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb9d01-a4ab-4480-b5f9-9c1c3bd0d3dd",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3ec99-a858-4fe7-bbbd-ad288df83f29",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c48b7-bc4d-418e-87ff-2b4fe336f1ab",
   "metadata": {},
   "source": [
    "![mlflow-tracking](https://mlflow.org/docs/latest/_images/quickstart_tracking_overview.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741737c-7f25-41f0-9c4a-ba8045d2a24a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:33.805043Z",
     "iopub.status.busy": "2024-06-29T09:03:33.804106Z",
     "iopub.status.idle": "2024-06-29T09:03:33.988965Z",
     "shell.execute_reply": "2024-06-29T09:03:33.988965Z",
     "shell.execute_reply.started": "2024-06-29T09:03:33.805043Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Union, Dict, Any\n",
    "\n",
    "\n",
    "def eval_metrics(y_actual: Union[pd.DataFrame, pd.Series, np.ndarray],\n",
    "                 y_pred: Union[pd.DataFrame, pd.Series, np.ndarray]\n",
    "                 ) -> Dict[str, float]:\n",
    "    \"\"\"Compute evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        y_actual: Ground truth (correct) target values\n",
    "        y_pred: Estimated target values.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: dictionary of evaluation metrics.\n",
    "            Expected keys are: \"rmse\", \"mae\", \"mape\", \"r2\", \"max_error\"\n",
    "\n",
    "    \"\"\"\n",
    "    # Calculate Root mean squared error, named rmse\n",
    "    rmse = root_mean_squared_error(y_actual, y_pred)\n",
    "    # Calculate mean absolute error, named mae\n",
    "    mae = mean_absolute_error(y_actual, y_pred)\n",
    "    # Mean absolute percentage error (MAPE)\n",
    "    mape = mean_absolute_percentage_error(y_actual, y_pred)\n",
    "    # Calculate R-squared: coefficient of determination, named r2\n",
    "    r2 = r2_score(y_actual, y_pred)\n",
    "    # Calculate max error: maximum value of absolute error (y_actual - y_pred), named maxerror\n",
    "    maxerror = max_error(y_actual, y_pred)\n",
    "    return {\"rmse\": rmse,\n",
    "            \"mae\": mae,\n",
    "            \"mape\": mape,\n",
    "            \"r2\": r2,\n",
    "            \"max_error\": maxerror\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999fb2c4-4d5c-4083-a31b-f850232e418b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:33.993959Z",
     "iopub.status.busy": "2024-06-29T09:03:33.993959Z",
     "iopub.status.idle": "2024-06-29T09:03:34.206410Z",
     "shell.execute_reply": "2024-06-29T09:03:34.205911Z",
     "shell.execute_reply.started": "2024-06-29T09:03:33.993959Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_pipeline(numerical_transformer: list,\n",
    "                    categorical_transformer: list,\n",
    "                    estimator: Pipeline,\n",
    "                    target_transformer: bool=False,\n",
    "                    **kwargs: dict) -> Pipeline:\n",
    "    \"\"\"Define pipeline for modeling.\n",
    "\n",
    "    Args:\n",
    "        numerical_transformer:\n",
    "        categorical_transformer:\n",
    "        target_transformer:\n",
    "        estimator:\n",
    "        kwargs:\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: sklearn pipeline\n",
    "    \"\"\"\n",
    "    numerical_transformer = make_pipeline(*numerical_transformer)\n",
    "\n",
    "    categorical_transformer = make_pipeline(*categorical_transformer)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numerical_transformer, make_column_selector(dtype_include=[\"number\"])),\n",
    "            (\"cat\", categorical_transformer, make_column_selector(dtype_include=[\"object\", \"bool\"])),\n",
    "        ],\n",
    "        remainder=\"drop\",  # non-specified columns are dropped\n",
    "        verbose_feature_names_out=False,  # will not prefix any feature names with the name of the transformer\n",
    "    )\n",
    "    # Append regressor to preprocessing pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    if target_transformer:\n",
    "        model_pipe1 = Pipeline(steps=[(\"preprocessor\", preprocessor),\n",
    "                                      (\"estimator\", estimator)])\n",
    "        model_pipe = TransformedTargetRegressor(regressor=model_pipe1,\n",
    "                                                func=np.log,\n",
    "                                                inverse_func=np.exp)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        model_pipe = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"estimator\", estimator)])\n",
    "        \n",
    "    # logger.info(f\"{model_pipe}\")\n",
    "    return model_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7cc41-0b64-4e06-9346-18d91804750a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:49:21.612818Z",
     "iopub.status.busy": "2024-06-28T13:49:21.612818Z",
     "iopub.status.idle": "2024-06-28T13:49:22.167229Z",
     "shell.execute_reply": "2024-06-28T13:49:22.167229Z",
     "shell.execute_reply.started": "2024-06-28T13:49:21.612818Z"
    }
   },
   "source": [
    "## Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91463e1e-d7ab-4ef7-9a85-6c0f70bf2fb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:34.207418Z",
     "iopub.status.busy": "2024-06-29T09:03:34.207418Z",
     "iopub.status.idle": "2024-06-29T09:03:34.411893Z",
     "shell.execute_reply": "2024-06-29T09:03:34.410227Z",
     "shell.execute_reply.started": "2024-06-29T09:03:34.207418Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Séparer les données en train et test (25%)\n",
    "\n",
    "FEATURES = FEATURE_NAMES if any(FEATURE_NAMES) else MODEL_PARAMS[\"DEFAULT_FEATURE_NAMES\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.loc[:, FEATURES],\n",
    "                                                    data[TARGET_NAME],\n",
    "                                                    test_size=MODEL_PARAMS[\"TEST_SIZE\"],\n",
    "                                                    random_state=SEED\n",
    "                                                   )\n",
    "\n",
    "logger.info(f\"\\nX train: {x_train.shape}\\nY train: {y_train.shape}\\n\"\n",
    "            f\"X test: {x_test.shape}\\nY test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7dc726-5f4d-4946-a18c-c2863a3c9b31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf8583-552c-45e8-ab42-577a26d16d4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:34.413622Z",
     "iopub.status.busy": "2024-06-29T09:03:34.412615Z",
     "iopub.status.idle": "2024-06-29T09:03:34.740259Z",
     "shell.execute_reply": "2024-06-29T09:03:34.738747Z",
     "shell.execute_reply.started": "2024-06-29T09:03:34.413622Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "reg = define_pipeline(numerical_transformer=[SimpleImputer(strategy=\"median\"),\n",
    "                                             RobustScaler()],\n",
    "                      categorical_transformer=[SimpleImputer(strategy=\"constant\", fill_value=\"undefined\"),\n",
    "                                               OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\")],\n",
    "                      target_transformer=False,\n",
    "                      estimator=RandomForestRegressor(n_estimators=30)\n",
    "                 )\n",
    "\n",
    "reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ce9dc-c14b-4a77-86d6-b8214079f356",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4263c-0d38-4476-850b-f22d51ddbd44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:34.742259Z",
     "iopub.status.busy": "2024-06-29T09:03:34.741260Z",
     "iopub.status.idle": "2024-06-29T09:03:35.849072Z",
     "shell.execute_reply": "2024-06-29T09:03:35.847573Z",
     "shell.execute_reply.started": "2024-06-29T09:03:34.742259Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model training and selection\n",
    "\n",
    "reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c96099-82a7-4cff-bb0f-fea189e53c66",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc722d2-0daa-47c3-8b0b-073ebe87f0d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:35.851572Z",
     "iopub.status.busy": "2024-06-29T09:03:35.851073Z",
     "iopub.status.idle": "2024-06-29T09:03:36.085631Z",
     "shell.execute_reply": "2024-06-29T09:03:36.084635Z",
     "shell.execute_reply.started": "2024-06-29T09:03:35.851572Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate trained model: sur le train et le test set\n",
    "\n",
    "# Calcule the evaluation metrics\n",
    "y_train_pred = reg.predict(x_train)\n",
    "y_test_pred = reg.predict(x_test)\n",
    "train_metrics = eval_metrics(y_train, y_train_pred)\n",
    "test_metrics = eval_metrics(y_test, y_test_pred)\n",
    "\n",
    "# log out metrics\n",
    "logger.info(f\"\"\"Performances\\n{pd.DataFrame({\"train\": train_metrics, \"test\": test_metrics}).T}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf2a99-ccc0-4a28-af31-da6e540fe712",
   "metadata": {},
   "source": [
    "#### Residuals analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeee113-3b2e-40de-9944-0e4bdca09b3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:36.086631Z",
     "iopub.status.busy": "2024-06-29T09:03:36.086631Z",
     "iopub.status.idle": "2024-06-29T09:03:36.958024Z",
     "shell.execute_reply": "2024-06-29T09:03:36.958024Z",
     "shell.execute_reply.started": "2024-06-29T09:03:36.086631Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(reg, is_fitted=\"auto\")\n",
    "\n",
    "visualizer.fit(x_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(x_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show();                # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f7409-c714-4427-8728-2ab836e15aaa",
   "metadata": {},
   "source": [
    "#### Prediction plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae95eeb-f7df-44b0-9af2-c298c8772ff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:36.960061Z",
     "iopub.status.busy": "2024-06-29T09:03:36.958979Z",
     "iopub.status.idle": "2024-06-29T09:03:37.395928Z",
     "shell.execute_reply": "2024-06-29T09:03:37.394929Z",
     "shell.execute_reply.started": "2024-06-29T09:03:36.960061Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualizer = PredictionError(reg, is_fitted=\"auto\", bestfit=True, identity=True)\n",
    "\n",
    "visualizer.fit(x_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(x_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show();                # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79824f-4813-4855-9945-2b0e7e5462f5",
   "metadata": {},
   "source": [
    "### Tester l'un des packages suivants:\n",
    "\n",
    "__Exo 1__:\n",
    "\n",
    "1- [PyCaret](https://pycaret.org/): An open source, low-code machine learning library in Python.\n",
    "\n",
    "2- [LazyPredict](https://pypi.org/project/lazypredict/): Lazy Predict help build a lot of basic models without much code and helps understand which models works better without any parameter tuning\n",
    "\n",
    "\n",
    "__Exo 2__:\n",
    "\n",
    "1- Tester la prédiction avec la variable logarithmique\n",
    "\n",
    "\n",
    "__Exo3__: \n",
    "1- Consulter la document de mlflow via les liens précisés au début du notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ae6d7-ef85-4619-a521-8a797e55f1ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:37.397935Z",
     "iopub.status.busy": "2024-06-29T09:03:37.396935Z",
     "iopub.status.idle": "2024-06-29T09:03:40.458415Z",
     "shell.execute_reply": "2024-06-29T09:03:40.455882Z",
     "shell.execute_reply.started": "2024-06-29T09:03:37.397935Z"
    }
   },
   "source": [
    "import pycaret.regression as pyr\n",
    "from lazypredict.Supervised import LazyRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94c440b-b6ca-429c-8d77-168787cc495f",
   "metadata": {},
   "source": [
    "### Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756326b-bbf2-4f3a-9f40-b9cad9b1126b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:50:22.393907Z",
     "iopub.status.busy": "2024-06-29T09:50:22.392902Z",
     "iopub.status.idle": "2024-06-29T09:50:22.809396Z",
     "shell.execute_reply": "2024-06-29T09:50:22.808570Z",
     "shell.execute_reply.started": "2024-06-29T09:50:22.393907Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an experiment if not exists\n",
    "exp_name = \"house-price\"\n",
    "experiment = mlflow.get_experiment_by_name(exp_name)\n",
    "if not experiment:\n",
    "    experiment_id = mlflow.create_experiment(exp_name)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "    \n",
    "logger.info(f\"Experience id: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0548df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "for e in mlflow.search_experiments():\n",
    "    print(f\"{e.experiment_id} - {e.name} - {e.artifact_location}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27b9889c-d2fc-4914-b6cd-2c225871c01d",
   "metadata": {},
   "source": [
    "# single model\n",
    "# Useful for multiple runs (only doing one run in this sample notebook)\n",
    "with mlflow.start_run(run_name=f\"{EXECUTION_DATE.strftime('%Y%m%d_%H%m%S')}-house_price\",\n",
    "                      experiment_id=experiment_id,\n",
    "                      tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
    "                      description=\"house price modeling\",\n",
    "                     ) as mlf_run:\n",
    "    print(f\"run_id: {mlf_run.info.run_id}\")\n",
    "    print(f\"version tag value: {mlf_run.data.tags.get('version')}\")\n",
    "    print(\"--\")\n",
    "\n",
    "    # Select number of estimator\n",
    "    n_estimators = 10  # int(input(\"Estimator(s): \"))\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    # Model definition\n",
    "    reg = define_pipeline(numerical_transformer=[SimpleImputer(strategy=\"median\"),\n",
    "                                                 RobustScaler()],\n",
    "                          categorical_transformer=[SimpleImputer(strategy=\"constant\", fill_value=\"undefined\"),\n",
    "                                                   OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\")],\n",
    "                          target_transformer=False,\n",
    "                          estimator=RandomForestClassifier(n_estimators=n_estimators)\n",
    "                     )\n",
    "\n",
    "    reg.fit(x_train, y_train)\n",
    "\n",
    "    # Evaluate Metrics\n",
    "    y_train_pred = reg.predict(x_train)\n",
    "    y_test_pred = reg.predict(x_test)\n",
    "    train_metrics = eval_metrics(y_train, y_train_pred)\n",
    "    test_metrics = eval_metrics(y_test, y_test_pred)\n",
    "\n",
    "    # log out metrics\n",
    "    logger.info(f\"Train: {train_metrics}\")\n",
    "    logger.info(f\"Test: {test_metrics}\")\n",
    "    \n",
    "    # Infer model signature\n",
    "    predictions = reg.predict(x_train)\n",
    "    signature = infer_signature(x_train, predictions)\n",
    "\n",
    "    # Log parameter, metrics, and model to MLflow\n",
    "    for group_name, set_metrics in [(\"train\", train_metrics),\n",
    "                                    (\"test\", test_metrics),\n",
    "                                   ]:\n",
    "        for metric_name, metric_value in set_metrics.items():\n",
    "            mlflow.log_metric(f\"{group_name}_{metric_name}\", metric_value)\n",
    "    mlflow.sklearn.log_model(reg,\n",
    "                             artifact_path=reg[-1].__class__.__name__,\n",
    "                             signature=signature,\n",
    "                             input_example=x_train[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56cfa6-a78b-40a3-871c-c08d0aad7002",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:59:18.914000Z",
     "iopub.status.busy": "2024-06-29T09:59:18.913001Z",
     "iopub.status.idle": "2024-06-29T09:59:19.347053Z",
     "shell.execute_reply": "2024-06-29T09:59:19.346058Z",
     "shell.execute_reply.started": "2024-06-29T09:59:18.914000Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define models and parameters to benchmark\n",
    "ESTIMATOR_PARAMS = {DummyRegressor.__name__: {\"estimator\": DummyRegressor,\n",
    "                                              \"params\": {\"strategy\": \"median\"}\n",
    "                                             },\n",
    "                    RandomForestRegressor.__name__: {\"estimator\": RandomForestRegressor,\n",
    "                                                     \"params\": {\"n_estimators\": 30,\n",
    "                                                                \"max_depth\": 3,\n",
    "                                                                \"random_state\": SEED\n",
    "                                                               }\n",
    "                                             },\n",
    "                    GradientBoostingRegressor.__name__: {\"estimator\": GradientBoostingRegressor,\n",
    "                                                         \"params\": {\"n_estimators\": 30,\n",
    "                                                                    \"learning_rate\": 0.01,\n",
    "                                                                    \"max_depth\": 3,\n",
    "                                                                    \"random_state\": SEED\n",
    "                                                                   }\n",
    "                                                        }\n",
    "}\n",
    "\n",
    "ESTIMATOR_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d178497-ed38-49bc-b6c7-38602dae8e87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:59:29.590801Z",
     "iopub.status.busy": "2024-06-29T09:59:29.589801Z",
     "iopub.status.idle": "2024-06-29T09:59:40.078236Z",
     "shell.execute_reply": "2024-06-29T09:59:40.078236Z",
     "shell.execute_reply.started": "2024-06-29T09:59:29.590801Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name, model_configs in ESTIMATOR_PARAMS.items():\n",
    "    logger.info(f\"{model_name} \\n{model_configs}\")\n",
    "    \n",
    "    estimator = model_configs[\"estimator\"]\n",
    "    params = model_configs[\"params\"]\n",
    "    \n",
    "    # Useful for multiple runs (only doing one run in this sample notebook)\n",
    "    with mlflow.start_run(run_name=f\"{CURRENT_DATE.strftime('%Y%m%d_%H%m%S')}-house_price-{model_name}\",\n",
    "                          experiment_id=experiment_id,\n",
    "                          tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
    "                          description=\"house price modeling\",\n",
    "                         ) as mlf_run:\n",
    "        logger.info(f\"run_id: {mlf_run.info.run_id}\")\n",
    "        logger.info(f\"version tag value: {mlf_run.data.tags.get('version')} -------------------------------\")\n",
    "\n",
    "        # log parameters\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        \n",
    "        # Model definition\n",
    "        reg = define_pipeline(numerical_transformer=[SimpleImputer(strategy=\"median\"),\n",
    "                                                     RobustScaler()],\n",
    "                              categorical_transformer=[SimpleImputer(strategy=\"constant\", fill_value=\"undefined\"),\n",
    "                                                       OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\")],\n",
    "                              target_transformer=False,\n",
    "                              estimator=estimator(**params)\n",
    "                         )\n",
    "\n",
    "        reg.fit(x_train, y_train)\n",
    "\n",
    "        # Evaluate Metrics\n",
    "        y_train_pred = reg.predict(x_train)\n",
    "        y_test_pred = reg.predict(x_test)\n",
    "        train_metrics = eval_metrics(y_train, y_train_pred)\n",
    "        test_metrics = eval_metrics(y_test, y_test_pred)\n",
    "        \n",
    "        # log out metrics\n",
    "        logger.info(f\"Train: {train_metrics}\")\n",
    "        logger.info(f\"Test: {test_metrics}\")\n",
    "\n",
    "        # Infer model signature with a sample\n",
    "        predictions = reg.predict(x_train[:30])\n",
    "        signature = mlflow.models.infer_signature(x_train[:30], predictions)\n",
    "\n",
    "        # Log  metrics, and model to MLflow\n",
    "        mlflow.log_metrics(test_metrics)\n",
    "        mlflow.sklearn.log_model(reg,\n",
    "                                 artifact_path=reg[-1].__class__.__name__,\n",
    "                                 signature=signature,\n",
    "                                 input_example=x_train[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d1510-1899-48eb-b8c2-e2003151aefc",
   "metadata": {},
   "source": [
    "\n",
    "Tester les modèles suivants:\n",
    "    \n",
    "    - DummyRegressor\n",
    "    - Regression ridge\n",
    "    - Bosting ou autre modèle ensembliste\n",
    "    \n",
    "30 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc9b16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecd77309-deba-4104-89d0-1d02702a7f06",
   "metadata": {},
   "source": [
    "### Performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b3e9d-1438-43cf-962a-c53c7ccd2364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b6e82-5d54-4207-a339-e48182902431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b4dedac-d9fa-4115-968e-825fdd73ecc0",
   "metadata": {},
   "source": [
    "### Features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd327f-5d38-495c-bd70-9e2c2cdfc8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940d99e-3ca9-4e84-867a-9986174bef42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3febfc98-87c9-455f-8f33-f4f1f0293780",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a783eb4c-6485-495f-9069-d8ac2548316e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713431b-acbf-4af0-a743-34f1f7397187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab203dd8-0f9d-48e3-856e-38a0fb74852d",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba05791-5bd4-4c82-ab8a-35c078a3e6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5aafea-f72e-48c6-a82a-ec64bd936a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb78d159-74c1-42f2-8abc-8527c95a0ad5",
   "metadata": {},
   "source": [
    "## Session info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3cf074-9b48-4f51-9bb4-49a634f3c3c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:21:20.166776Z",
     "iopub.status.busy": "2024-06-29T09:21:20.166776Z",
     "iopub.status.idle": "2024-06-29T09:21:20.398608Z",
     "shell.execute_reply": "2024-06-29T09:21:20.397663Z",
     "shell.execute_reply.started": "2024-06-29T09:21:20.166776Z"
    }
   },
   "outputs": [],
   "source": [
    "import session_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96807e83-3988-411e-a201-f8fe161c5075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:21:47.889858Z",
     "iopub.status.busy": "2024-06-29T09:21:47.889858Z",
     "iopub.status.idle": "2024-06-29T09:21:48.095512Z",
     "shell.execute_reply": "2024-06-29T09:21:48.094516Z",
     "shell.execute_reply.started": "2024-06-29T09:21:47.889858Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "session_info.show(dependencies=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb725205-9e32-42ed-8a1c-0dda168398e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
