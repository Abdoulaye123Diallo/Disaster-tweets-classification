{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a46012-b136-43a9-ad81-76ac62a83136",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Pour la suite du cours, nous allons faire l'apprentissage par la pratique avec un cas r√©el de projet de Machine Learning\n",
    "\n",
    "Description du projet: Pr√©dire le prix des biens immobiliers √† Ames, Iowa.\n",
    "\n",
    "Les donn√©es sont disponibles dans: [sklearn.datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml)\n",
    "\n",
    "\n",
    "Voici un r√©capitulatif des objectifs :\n",
    "- R√©aliser une analyse exploratoire.\n",
    "- Tester diff√©rents mod√®les de pr√©diction afin de r√©pondre au mieux √† la probl√©matique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f714cf0-f5bc-491c-80e4-38472f4d409d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T12:28:27.081285Z",
     "iopub.status.busy": "2024-06-28T12:28:27.081285Z",
     "iopub.status.idle": "2024-06-28T12:28:27.098789Z",
     "shell.execute_reply": "2024-06-28T12:28:27.097789Z",
     "shell.execute_reply.started": "2024-06-28T12:28:27.081285Z"
    }
   },
   "source": [
    "__Sources utiles__\n",
    "\n",
    "- [Introduction √† MLOps](https://ashutoshtripathi.com/2021/08/18/mlops-a-complete-guide-to-machine-learning-operations-mlops-vs-devops/)\n",
    "\n",
    "- [MLFLOW - Site de r√©f√©rence](https://mlflow.org/docs/latest/index.html)\n",
    "- [MLFLOW - Tutorial](https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html)\n",
    "- [MLFLOW - Tracking](https://mlflow.org/docs/latest/tracking.html)\n",
    "- [MLFLOW - Model Registry](https://mlflow.org/docs/latest/model-registry.html#)\n",
    "- [MLFLOW - Serve a model](https://mlflow.org/docs/latest/model-registry.html#serving-an-mlflow-model-from-model-registry)\n",
    "\n",
    "- [Evidently - tutorial d'analyse de Data drift](https://github.com/evidentlyai/evidently/tree/main/examples/sample_notebooks)\n",
    "- [API Flask - D√©marche de mise en oeuvre](http://web.univ-ubs.fr/lmba/lardjane/python/c4.pdf)\n",
    "- [FastAPI - D√©marche de mise en oeuvre](https://towardsdatascience.com/how-to-build-and-deploy-a-machine-learning-model-with-fastapi-64c505213857)\n",
    "- [Azure - Tuto d√©ploiement application web ](https://learn.microsoft.com/fr-fr/azure/app-service/quickstart-python?tabs=flask%2Cwindows%2Cazure-portal%2Cvscode-deploy%2Cdeploy-instructions-azportal%2Cterminal-bash%2Cdeploy-instructions-zip-azcli)\n",
    "- [Tests unitaires - Unittest ou Pytest](https://www.sitepoint.com/python-unit-testing-unittest-pytest/)\n",
    "\n",
    "- [Pythonanywhere](https://www.pythonanywhere.com/)\n",
    "- [Heroku](https://www.heroku.com/)\n",
    "-[Azure webapp - D√©ploiement automatis√© via Github](https://learn.microsoft.com/fr-fr/azure/app-service/deploy-continuous-deployment?tabs=github)\n",
    "- Streamlit ou gradio pour la mise en place d'un dashbord\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14612581-a3ef-4522-8ddd-64adf0221e56",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038e700c-aabe-4e62-9b37-d8ffa755cf32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T10:02:04.633195Z",
     "iopub.status.busy": "2024-06-29T10:02:04.633195Z",
     "iopub.status.idle": "2024-06-29T10:02:05.612841Z",
     "shell.execute_reply": "2024-06-29T10:02:05.611845Z",
     "shell.execute_reply.started": "2024-06-29T10:02:04.633195Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pendulum\n",
    "import plotly.express as px\n",
    "import ppscore as pps\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, TransformedTargetRegressor\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, VotingRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.metrics import (r2_score,\n",
    "                             root_mean_squared_error,\n",
    "                             mean_absolute_percentage_error,\n",
    "                             mean_absolute_error,\n",
    "                             max_error,\n",
    "                            )\n",
    "from sklearn.model_selection import train_test_split, learning_curve, LearningCurveDisplay\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from ydata_profiling import ProfileReport\n",
    "from yellowbrick.regressor import PredictionError, ResidualsPlot\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from settings.params import MODEL_PARAMS, SEED\n",
    "from src.make_dataset import load_data\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "set_config(display=\"diagram\", print_changed_only=False)  # display sklearn pipeline as diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d2c00-ebe7-409c-b936-a3a36a10a6b0",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c12ca6-6ee6-4a5b-bd46-b567a6c5c34b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:21.715720Z",
     "iopub.status.busy": "2024-06-29T09:03:21.714721Z",
     "iopub.status.idle": "2024-06-29T09:03:21.867996Z",
     "shell.execute_reply": "2024-06-29T09:03:21.867826Z",
     "shell.execute_reply.started": "2024-06-29T09:03:21.715720Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-30 21:23:58.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - Target name: target\n",
      "\u001b[32m2025-07-30 21:23:58.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \n",
      "Project directory: c:\\Users\\DELL\\Desktop\\mlops-project - Tweets \n",
      "Reports dir: c:\\Users\\DELL\\Desktop\\mlops-project - Tweets\\reports\n"
     ]
    }
   ],
   "source": [
    "# Set logging format\n",
    "log_fmt = \"<green>{time:YYYY-MM-DD HH:mm:ss.SSS!UTC}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - {message}\"\n",
    "logger.configure(handlers=[{\"sink\": sys.stderr, \"format\": log_fmt}])\n",
    "\n",
    "# current data\n",
    "CURRENT_DATE = pendulum.now(tz=\"UTC\")\n",
    "\n",
    "# target name definition\n",
    "TARGET_NAME = MODEL_PARAMS[\"TARGET_NAME\"]\n",
    "logger.info(f\"Target name: {TARGET_NAME}\")\n",
    "\n",
    "\n",
    "# directories\n",
    "PROJECT_DIR = Path.cwd().parent\n",
    "REPORTS_DIR = Path(PROJECT_DIR, \"reports\")\n",
    "\n",
    "logger.info(f\"\\nProject directory: {PROJECT_DIR} \\nReports dir: {REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9b5106-eb95-4b15-8655-836f5b458283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:21.869006Z",
     "iopub.status.busy": "2024-06-29T09:03:21.869006Z",
     "iopub.status.idle": "2024-06-29T09:03:22.009874Z",
     "shell.execute_reply": "2024-06-29T09:03:22.008866Z",
     "shell.execute_reply.started": "2024-06-29T09:03:21.869006Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TARGET_NAME': 'target',\n",
       " 'MIN_COMPLETION_RATE': 0.75,\n",
       " 'MIN_PPS': 0.1,\n",
       " 'DEFAULT_FEATURE_NAMES': ['Alley',\n",
       "  'BsmtQual',\n",
       "  'ExterQual',\n",
       "  'Foundation',\n",
       "  'FullBath',\n",
       "  'GarageArea',\n",
       "  'GarageCars',\n",
       "  'GarageFinish',\n",
       "  'GarageType',\n",
       "  'GrLivArea',\n",
       "  'KitchenQualMSSubClass',\n",
       "  'Neighborhood',\n",
       "  'OverallQual',\n",
       "  'TotRmsAbvGrd',\n",
       "  'building_age',\n",
       "  'remodel_age',\n",
       "  'garage_age'],\n",
       " 'TEST_SIZE': 0.25}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17884a5-99c8-452a-b3fb-7cf45931ca1b",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7288f4e1-0a82-4ab1-ad61-a4f80aeb4289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.011873Z",
     "iopub.status.busy": "2024-06-29T09:03:22.010873Z",
     "iopub.status.idle": "2024-06-29T09:03:22.199509Z",
     "shell.execute_reply": "2024-06-29T09:03:22.198509Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.011873Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data like pandas.DataFrame\n",
    "data = load_data(dataset_name=\"tweets\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071fe3c2-3617-43b6-aca2-045339e61256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.201512Z",
     "iopub.status.busy": "2024-06-29T09:03:22.200509Z",
     "iopub.status.idle": "2024-06-29T09:03:22.373482Z",
     "shell.execute_reply": "2024-06-29T09:03:22.372486Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.201512Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f0fb9-7a60-447c-81bd-3fa18149fa7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.374486Z",
     "iopub.status.busy": "2024-06-29T09:03:22.373482Z",
     "iopub.status.idle": "2024-06-29T09:03:22.514516Z",
     "shell.execute_reply": "2024-06-29T09:03:22.514516Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.374486Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c5b45-80f1-48d6-bad9-5d95640cb17d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.674645Z",
     "iopub.status.busy": "2024-06-29T09:03:22.674645Z",
     "iopub.status.idle": "2024-06-29T09:03:22.938731Z",
     "shell.execute_reply": "2024-06-29T09:03:22.937730Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.674645Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.describe(include=\"all\") #, datetime_is_numeric=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b46232-40eb-4568-be4f-9c9f2a8a0d4d",
   "metadata": {},
   "source": [
    "# EDA: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a178177-9ff1-4b6d-b967-b328cae9a06b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.940731Z",
     "iopub.status.busy": "2024-06-29T09:03:22.939726Z",
     "iopub.status.idle": "2024-06-29T09:03:24.434116Z",
     "shell.execute_reply": "2024-06-29T09:03:24.434116Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.940731Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# barplot for missing value rate\n",
    "msno.bar(data,\n",
    "         filter=\"top\",\n",
    "         p=MODEL_PARAMS[\"MIN_COMPLETION_RATE\"],\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1e6dc-d2a7-4750-902c-79516b698c3b",
   "metadata": {},
   "source": [
    "## Target analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb20318-d0eb-4dd6-94d2-a3c06bdcff13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:24.437117Z",
     "iopub.status.busy": "2024-06-29T09:03:24.436116Z",
     "iopub.status.idle": "2024-06-29T09:03:24.576283Z",
     "shell.execute_reply": "2024-06-29T09:03:24.576283Z",
     "shell.execute_reply.started": "2024-06-29T09:03:24.437117Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_PARAMS[\"TARGET_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08927768",
   "metadata": {},
   "source": [
    "### Distribution des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1d76b-ac16-4a60-a4ce-af3b8c225e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:24.578283Z",
     "iopub.status.busy": "2024-06-29T09:03:24.577279Z",
     "iopub.status.idle": "2024-06-29T09:03:24.733931Z",
     "shell.execute_reply": "2024-06-29T09:03:24.732887Z",
     "shell.execute_reply.started": "2024-06-29T09:03:24.577279Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_counts = data[TARGET_NAME].value_counts()\n",
    "target_pct = data[TARGET_NAME].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"Distribution des classes:\")\n",
    "print(f\"  Classe 0 (Non-Disaster): {target_counts[0]} ({target_pct[0]:.1f}%)\")\n",
    "print(f\"  Classe 1 (Disaster): {target_counts[1]} ({target_pct[1]:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Graphique en barres\n",
    "target_counts.plot(kind='bar', ax=axes[0], color=['skyblue', 'salmon'])\n",
    "axes[0].set_title('Distribution des Classes')\n",
    "axes[0].set_xlabel('Target (0=Non-Disaster, 1=Disaster)')\n",
    "axes[0].set_ylabel('Nombre de tweets')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Graphique en secteurs\n",
    "axes[1].pie(target_counts.values, labels=['Non-Disaster', 'Disaster'], \n",
    "           autopct='%1.1f%%', colors=['skyblue', 'salmon'])\n",
    "axes[1].set_title('R√©partition des Classes')\n",
    "\n",
    "# Graphique d'√©quilibre\n",
    "class_balance = min(target_counts) / max(target_counts)\n",
    "axes[2].bar(['√âquilibre des Classes'], [class_balance], color='orange')\n",
    "axes[2].set_title(f'√âquilibre: {class_balance:.2f}')\n",
    "axes[2].set_ylabel('Ratio (min/max)')\n",
    "axes[2].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9456b",
   "metadata": {},
   "source": [
    "## Analyse des features cat√©gorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd78d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä 4. ANALYSE DES FEATURES CAT√âGORIELLES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Analyse des keywords\n",
    "print(\"üîë ANALYSE DES KEYWORDS:\")\n",
    "keyword_counts = data['keyword'].value_counts(dropna=False).head(15)\n",
    "print(f\"Top 15 keywords les plus fr√©quents:\")\n",
    "print(keyword_counts)\n",
    "\n",
    "# Keywords par classe\n",
    "keyword_disaster = data[data['target'] == 1]['keyword'].value_counts(dropna=False).head(10)\n",
    "keyword_normal = data[data['target'] == 0]['keyword'].value_counts(dropna=False).head(10)\n",
    "\n",
    "print(f\"\\nTop 10 keywords - Disaster tweets:\")\n",
    "print(keyword_disaster)\n",
    "print(f\"\\nTop 10 keywords - Normal tweets:\")\n",
    "print(keyword_normal)\n",
    "\n",
    "# Analyse des locations\n",
    "print(\"\\nüìç ANALYSE DES LOCATIONS:\")\n",
    "location_counts = data['location'].value_counts(dropna=False).head(15)\n",
    "print(f\"Top 15 locations les plus fr√©quentes:\")\n",
    "print(location_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Keywords globaux\n",
    "keyword_counts.plot(kind='barh', ax=axes[0,0], color='lightblue')\n",
    "axes[0,0].set_title('Top 15 Keywords')\n",
    "axes[0,0].set_xlabel('Fr√©quence')\n",
    "\n",
    "# Locations globales\n",
    "location_counts.plot(kind='barh', ax=axes[0,1], color='lightgreen')\n",
    "axes[0,1].set_title('Top 15 Locations')\n",
    "axes[0,1].set_xlabel('Fr√©quence')\n",
    "\n",
    "# Keywords par classe\n",
    "keyword_disaster.head(8).plot(kind='barh', ax=axes[1,0], color='salmon')\n",
    "axes[1,0].set_title('Top Keywords - Disaster Tweets')\n",
    "axes[1,0].set_xlabel('Fr√©quence')\n",
    "\n",
    "keyword_normal.head(8).plot(kind='barh', ax=axes[1,1], color='skyblue')\n",
    "axes[1,1].set_title('Top Keywords - Normal Tweets')\n",
    "axes[1,1].set_xlabel('Fr√©quence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0080ae36",
   "metadata": {},
   "source": [
    "### Analyse de texte approfondie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99642702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä 5. ANALYSE APPROFONDIE DU TEXTE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Longueur des textes\n",
    "data['text_length'] = data['text'].str.len()\n",
    "data['word_count'] = data['text'].str.split().str.len()\n",
    "\n",
    "print(\"üìè STATISTIQUES DE LONGUEUR:\")\n",
    "length_stats = data.groupby('target')[['text_length', 'word_count']].describe()\n",
    "print(length_stats)\n",
    "\n",
    "# Caract√®res sp√©ciaux\n",
    "data['url_count'] = data['text'].str.count(r'http\\S+|www\\S+')\n",
    "data['mention_count'] = data['text'].str.count(r'@\\w+')\n",
    "data['hashtag_count'] = data['text'].str.count(r'#\\w+')\n",
    "data['exclamation_count'] = data['text'].str.count(r'!')\n",
    "data['question_count'] = data['text'].str.count(r'\\?')\n",
    "data['caps_count'] = data['text'].str.count(r'[A-Z]')\n",
    "\n",
    "print(\"\\nüîç CARACT√àRES SP√âCIAUX par classe:\")\n",
    "special_chars = ['url_count', 'mention_count', 'hashtag_count', 'exclamation_count', 'question_count', 'caps_count']\n",
    "special_stats = data.groupby('target')[special_chars].mean()\n",
    "print(special_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7299e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des longueurs\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Distribution longueur caract√®res\n",
    "for i, target in enumerate([0, 1]):\n",
    "    df = data[data[TARGET_NAME] == 0]['text_length']\n",
    "    axes[0, 0].hist(df, bins=30, alpha=0.7, label=f'Target {target}')\n",
    "axes[0, 0].set_title('Distribution - Longueur en Caract√®res')\n",
    "axes[0, 0].set_xlabel('Nombre de caract√®res')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Distribution nombre de mots\n",
    "for i, target in enumerate([0, 1]):\n",
    "    df = data[data[TARGET_NAME] == target]['word_count']\n",
    "    axes[0, 1].hist(df, bins=30, alpha=0.7, label=f'Target {target}')\n",
    "axes[0, 1].set_title('Distribution - Nombre de Mots')\n",
    "axes[0, 1].set_xlabel('Nombre de mots')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Boxplot longueurs par classe\n",
    "data.boxplot(column='text_length', by='target', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Longueur par Classe')\n",
    "axes[0, 2].set_xlabel('Target')\n",
    "\n",
    "# Caract√®res sp√©ciaux\n",
    "special_stats.T.plot(kind='bar', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Caract√®res Sp√©ciaux par Classe')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].legend(['Non-Disaster', 'Disaster'])\n",
    "\n",
    "# Corr√©lation longueur vs target\n",
    "correlation_length = data[['text_length', 'word_count', 'target']].corr()\n",
    "sns.heatmap(correlation_length, annot=True, ax=axes[1, 1], cmap='coolwarm')\n",
    "axes[1, 1].set_title('Corr√©lation Longueur vs Target')\n",
    "\n",
    "# Distribution caps par classe\n",
    "for i, target in enumerate([0, 1]):\n",
    "    df = data[data[TARGET_NAME] == target]['caps_count']\n",
    "    axes[1, 2].hist(df, bins=20, alpha=0.7, label=f'Target {target}')\n",
    "axes[1, 2].set_title('Distribution - Lettres Majuscules')\n",
    "axes[1, 2].set_xlabel('Nombre de majuscules')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757fb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## T√©l√©charger NLTK\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648e7c2",
   "metadata": {},
   "source": [
    "### Analyse des mots les plus fr√©quents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour l'analyse textuelle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "print(\"\\nüìä 6. ANALYSE DES MOTS LES PLUS FR√âQUENTS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def clean_text_for_analysis(text):\n",
    "    \"\"\"Nettoyage basique pour l'analyse des mots\"\"\"\n",
    "    if not text or pd.isna(text) or not text.strip():\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Pr√©parer les textes\n",
    "disaster_texts = data[data['target'] == 1]['text'].apply(clean_text_for_analysis)\n",
    "normal_texts = data[data['target'] == 0]['text'].apply(clean_text_for_analysis)\n",
    "\n",
    "# Mots les plus fr√©quents\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_top_words(texts, n=20):\n",
    "    \"\"\"Obtenir les mots les plus fr√©quents\"\"\"\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "        all_words.extend(words)\n",
    "    return Counter(all_words).most_common(n)\n",
    "\n",
    "disaster_words = get_top_words(disaster_texts)\n",
    "normal_words = get_top_words(normal_texts)\n",
    "\n",
    "\n",
    "# Visualisation des mots fr√©quents\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Disaster words\n",
    "disaster_df = pd.DataFrame(disaster_words, columns=['Word', 'Count'])\n",
    "disaster_df.plot(x='Word', y='Count', kind='barh', ax=axes[0], color='salmon')\n",
    "axes[0].set_title('Top 20 Mots - Disaster Tweets')\n",
    "axes[0].set_xlabel('Fr√©quence')\n",
    "\n",
    "# Normal words\n",
    "normal_df = pd.DataFrame(normal_words, columns=['Word', 'Count'])\n",
    "normal_df.plot(x='Word', y='Count', kind='barh', ax=axes[1], color='skyblue')\n",
    "axes[1].set_title('Top 20 Mots - Normal Tweets')\n",
    "axes[1].set_xlabel('Fr√©quence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2652f89",
   "metadata": {},
   "source": [
    "### Word Clouds (Nuages de points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de6d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nüìä 7. NUAGES DE MOTS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Word cloud pour disaster tweets\n",
    "    disaster_text_combined = ' '.join(disaster_texts)\n",
    "    wordcloud_disaster = WordCloud(width=800, height=400, \n",
    "                                 background_color='white',\n",
    "                                 stopwords=stop_words,\n",
    "                                 max_words=100).generate(disaster_text_combined)\n",
    "\n",
    "    # Word cloud pour normal tweets\n",
    "    normal_text_combined = ' '.join(normal_texts)\n",
    "    wordcloud_normal = WordCloud(width=800, height=400, \n",
    "                               background_color='white',\n",
    "                               stopwords=stop_words,\n",
    "                               max_words=100).generate(normal_text_combined)\n",
    "\n",
    "    # Affichage\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "    axes[0].imshow(wordcloud_disaster, interpolation='bilinear')\n",
    "    axes[0].set_title('Word Cloud - Disaster Tweets', fontsize=16)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(wordcloud_normal, interpolation='bilinear')\n",
    "    axes[1].set_title('Word Cloud - Normal Tweets', fontsize=16)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  WordCloud non disponible. Installer avec: pip install wordcloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517cfc70",
   "metadata": {},
   "source": [
    "### Analyse des corr√©lations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä 8. ANALYSE DE CORR√âLATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Matrice de corr√©lation pour les variables num√©riques\n",
    "numeric_features = ['text_length', 'word_count', 'url_count', 'mention_count', \n",
    "                   'hashtag_count', 'exclamation_count', 'question_count', 'caps_count', 'target']\n",
    "\n",
    "correlation_matrix = data[numeric_features].corr()\n",
    "\n",
    "print(\"üîó Corr√©lations avec la variable target:\")\n",
    "target_correlations = correlation_matrix['target'].sort_values(key=abs, ascending=False)\n",
    "print(target_correlations)\n",
    "\n",
    "# Heatmap de corr√©lation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Matrice de Corr√©lation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96b26f-08a8-491f-88f6-1758dffba7fe",
   "metadata": {},
   "source": [
    "## Pr√©traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f69ca4-89d7-4b96-9924-22b66335612b",
   "metadata": {},
   "source": [
    "#### IMPORTS COMPLETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b121fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Pour Word2Vec et FastText\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "import re\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637abaee",
   "metadata": {},
   "source": [
    "#### PREPROCESSING COMPLET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d42b5258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'keyword', 'location', 'text', 'target', 'url_count',\n",
      "       'text_length', 'mention_count', 'question_count', 'caps_count',\n",
      "       'exclamation_count', 'hashtag_count', 'word_count', 'urgency_score',\n",
      "       'social_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_features(df):\n",
    "    \"\"\"Extrait TOUTES les features importantes selon votre analyse\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    # Top features (dans l'ordre de corr√©lation)\n",
    "    features['url_count'] = df['text'].str.count(r'http\\S+|www\\S+')           # +0.196\n",
    "    features['text_length'] = df['text'].str.len()                            # +0.182\n",
    "    features['mention_count'] = df['text'].str.count(r'@\\w+')                 # -0.103\n",
    "    features['question_count'] = df['text'].str.count(r'\\?')                  # -0.084\n",
    "    features['caps_count'] = df['text'].str.count(r'[A-Z]')                   # +0.078\n",
    "    features['exclamation_count'] = df['text'].str.count(r'!')                # -0.075\n",
    "    features['hashtag_count'] = df['text'].str.count(r'#\\w+')                 # +0.052\n",
    "    features['word_count'] = df['text'].str.split().str.len()                 # +0.040\n",
    "    \n",
    "    # Features compos√©es\n",
    "    features['urgency_score'] = features['url_count'] + features['caps_count']\n",
    "    features['social_score'] = features['mention_count'] + features['hashtag_count']\n",
    "    \n",
    "    return features\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Nettoyage intelligent du texte\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocessing complet - remplace votre classe\"\"\"\n",
    "    print(\"üîß Preprocessing complet...\")\n",
    "    \n",
    "    # Features num√©riques\n",
    "    df_features = extract_features(df)\n",
    "    \n",
    "    # Nettoyage du texte\n",
    "    df_features['clean_text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    # Encodage des cat√©gories\n",
    "    le_location = LabelEncoder()\n",
    "    le_keyword = LabelEncoder()\n",
    "    \n",
    "    df_features['location_encoded'] = le_location.fit_transform(df['location'].fillna('unknown'))\n",
    "    df_features['keyword_encoded'] = le_keyword.fit_transform(df['keyword'].fillna('none'))\n",
    "    \n",
    "    return df_features, le_location, le_keyword\n",
    "\n",
    "print(extract_features(data).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944ca47",
   "metadata": {},
   "source": [
    "#### TOUS LES VECTORIZERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e54e221-7315-4a73-9991-2445cc7ae18a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:25.675870Z",
     "iopub.status.busy": "2024-06-29T09:03:25.674867Z",
     "iopub.status.idle": "2024-06-29T09:03:25.848597Z",
     "shell.execute_reply": "2024-06-29T09:03:25.848597Z",
     "shell.execute_reply.started": "2024-06-29T09:03:25.675870Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tfidf_features(X_train, X_test, max_features=5000):\n",
    "    \"\"\"TF-IDF Vectorization\"\"\"\n",
    "    print(\"  üìù TF-IDF...\")\n",
    "    tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(1,2))\n",
    "    X_train_vec = tfidf.fit_transform(X_train['clean_text'])\n",
    "    X_test_vec = tfidf.transform(X_test['clean_text'])\n",
    "    return X_train_vec, X_test_vec, tfidf\n",
    "\n",
    "def get_word2vec_features(X_train, X_test, vector_size=100):\n",
    "    \"\"\"Word2Vec Vectorization\"\"\"\n",
    "    print(\"  üß† Word2Vec...\")\n",
    "    \n",
    "    # Tokenisation\n",
    "    train_tokens = [text.split() for text in X_train['clean_text']]\n",
    "    test_tokens = [text.split() for text in X_test['clean_text']]\n",
    "    \n",
    "    # Entra√Ænement Word2Vec\n",
    "    w2v_model = Word2Vec(sentences=train_tokens, vector_size=vector_size, window=5, min_count=2, workers=4)\n",
    "    \n",
    "    # Moyenne des vecteurs par document\n",
    "    def average_vectors(tokens, model):\n",
    "        vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "    \n",
    "    X_train_vec = np.array([average_vectors(tokens, w2v_model) for tokens in train_tokens])\n",
    "    X_test_vec = np.array([average_vectors(tokens, w2v_model) for tokens in test_tokens])\n",
    "    \n",
    "    return csr_matrix(X_train_vec), csr_matrix(X_test_vec), w2v_model\n",
    "\n",
    "def get_fasttext_features(X_train, X_test, vector_size=100):\n",
    "    \"\"\"FastText Vectorization\"\"\"\n",
    "    print(\"  ‚ö° FastText...\")\n",
    "    \n",
    "    # Tokenisation\n",
    "    train_tokens = [text.split() for text in X_train['clean_text']]\n",
    "    test_tokens = [text.split() for text in X_test['clean_text']]\n",
    "    \n",
    "    # Entra√Ænement FastText\n",
    "    ft_model = FastText(sentences=train_tokens, vector_size=vector_size, window=5, min_count=2, workers=4)\n",
    "    \n",
    "    # Moyenne des vecteurs par document\n",
    "    def average_vectors(tokens, model):\n",
    "        vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "    \n",
    "    X_train_vec = np.array([average_vectors(tokens, ft_model) for tokens in train_tokens])\n",
    "    X_test_vec = np.array([average_vectors(tokens, ft_model) for tokens in test_tokens])\n",
    "    \n",
    "    return csr_matrix(X_train_vec), csr_matrix(X_test_vec), ft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036992e",
   "metadata": {},
   "source": [
    "#### MOD√àLES √Ä TESTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "536e9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    'Logistic': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss', verbosity=0),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "vectorizers_config = {\n",
    "    'TF-IDF': get_tfidf_features,\n",
    "    'Word2Vec': get_word2vec_features,\n",
    "    'FastText': get_fasttext_features\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52765c",
   "metadata": {},
   "source": [
    "#### TEST TOUTES COMBINAISONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56050150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_combinations(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Test TOUTES les combinaisons mod√®le + vectorizer\"\"\"\n",
    "    \n",
    "    print(\"üöÄ TEST DE TOUTES LES COMBINAISONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Features num√©riques (communes √† tous)\n",
    "    num_features = ['url_count', 'text_length', 'mention_count', 'question_count',\n",
    "                   'caps_count', 'exclamation_count', 'hashtag_count', 'word_count',\n",
    "                   'urgency_score', 'social_score', 'location_encoded', 'keyword_encoded']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train[num_features])\n",
    "    X_test_num = scaler.transform(X_test[num_features])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Boucle sur tous les vectorizers\n",
    "    for vec_name, vec_func in vectorizers_config.items():\n",
    "        print(f\"\\nüìä VECTORIZER: {vec_name}\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        try:\n",
    "            # Obtenir les features textuelles\n",
    "            X_train_text, X_test_text, vectorizer = vec_func(X_train, X_test)\n",
    "            \n",
    "            # Combiner avec les features num√©riques\n",
    "            X_train_final = hstack([X_train_text, csr_matrix(X_train_num)])\n",
    "            X_test_final = hstack([X_test_text, csr_matrix(X_test_num)])\n",
    "            \n",
    "            # Tester tous les mod√®les avec ce vectorizer\n",
    "            for model_name, model in models_config.items():\n",
    "                print(f\"  üéØ {model_name}...\", end=\" \")\n",
    "                \n",
    "                try:\n",
    "                    # Entra√Ænement\n",
    "                    model.fit(X_train_final, y_train)\n",
    "                    \n",
    "                    # Pr√©dictions\n",
    "                    y_pred = model.predict(X_test_final)\n",
    "                    \n",
    "                    # M√©triques\n",
    "                    f1 = f1_score(y_test, y_pred)\n",
    "                    accuracy = accuracy_score(y_test, y_pred)\n",
    "                    \n",
    "                    # Cross-validation\n",
    "                    cv_scores = cross_val_score(model, X_train_final, y_train, cv=3, scoring='f1')\n",
    "                    cv_mean = cv_scores.mean()\n",
    "                    cv_std = cv_scores.std()\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Vectorizer': vec_name,\n",
    "                        'Model': model_name,\n",
    "                        'F1_Test': f1,\n",
    "                        'Accuracy_Test': accuracy,\n",
    "                        'F1_CV_Mean': cv_mean,\n",
    "                        'F1_CV_Std': cv_std,\n",
    "                        'Combination': f\"{vec_name} + {model_name}\"\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"F1: {f1:.4f} | CV: {cv_mean:.4f}¬±{cv_std:.4f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Erreur: {str(e)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur avec {vec_name}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed611f",
   "metadata": {},
   "source": [
    "#### Analyse des r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46160ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyse compl√®te des r√©sultats\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ùå Aucun r√©sultat √† analyser\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df_sorted = df.sort_values('F1_Test', ascending=False)\n",
    "    \n",
    "    print(\"\\nüèÜ TOP 10 COMBINAISONS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(df_sorted.head(10)[['Combination', 'F1_Test', 'Accuracy_Test', 'F1_CV_Mean']].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nü•á MEILLEURE COMBINAISON:\")\n",
    "    best = df_sorted.iloc[0]\n",
    "    print(f\"   {best['Combination']}\")\n",
    "    print(f\"   F1 Test: {best['F1_Test']:.4f}\")\n",
    "    print(f\"   Accuracy: {best['Accuracy_Test']:.4f}\")\n",
    "    print(f\"   F1 CV: {best['F1_CV_Mean']:.4f} ¬± {best['F1_CV_Std']:.4f}\")\n",
    "    \n",
    "    # Analyse par vectorizer\n",
    "    print(f\"\\nüìà ANALYSE PAR VECTORIZER:\")\n",
    "    vec_analysis = df.groupby('Vectorizer').agg({\n",
    "        'F1_Test': ['mean', 'max', 'std'],\n",
    "        'F1_CV_Mean': 'mean'\n",
    "    }).round(4)\n",
    "    print(vec_analysis)\n",
    "    \n",
    "    # Analyse par mod√®le\n",
    "    print(f\"\\nü§ñ ANALYSE PAR MOD√àLE:\")\n",
    "    model_analysis = df.groupby('Model').agg({\n",
    "        'F1_Test': ['mean', 'max', 'std'],\n",
    "        'F1_CV_Mean': 'mean'\n",
    "    }).round(4)\n",
    "    print(model_analysis)\n",
    "    \n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f499d",
   "metadata": {},
   "source": [
    "#### Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "558ad004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° QUICK START - Test complet automatique\n",
      "==================================================\n",
      "üîß Preprocessing complet...\n",
      "üîß Preprocessing complet...\n",
      "üöÄ TEST DE TOUTES LES COMBINAISONS\n",
      "==================================================\n",
      "\n",
      "üìä VECTORIZER: TF-IDF\n",
      "-------------------------\n",
      "  üìù TF-IDF...\n",
      "  üéØ Logistic... F1: 0.7530 | CV: 0.7277¬±0.0058\n",
      "  üéØ RandomForest... F1: 0.6955 | CV: 0.6758¬±0.0092\n",
      "  üéØ XGBoost... F1: 0.7221 | CV: 0.7003¬±0.0086\n",
      "  üéØ LightGBM... F1: 0.7331 | CV: 0.6987¬±0.0083\n",
      "\n",
      "üìä VECTORIZER: Word2Vec\n",
      "-------------------------\n",
      "  üß† Word2Vec...\n",
      "  üéØ Logistic... F1: 0.6096 | CV: 0.6003¬±0.0038\n",
      "  üéØ RandomForest... F1: 0.6486 | CV: 0.6320¬±0.0052\n",
      "  üéØ XGBoost... F1: 0.6832 | CV: 0.6573¬±0.0030\n",
      "  üéØ LightGBM... F1: 0.6618 | CV: 0.6547¬±0.0052\n",
      "\n",
      "üìä VECTORIZER: FastText\n",
      "-------------------------\n",
      "  ‚ö° FastText...\n",
      "  üéØ Logistic... F1: 0.5767 | CV: 0.5633¬±0.0089\n",
      "  üéØ RandomForest... F1: 0.6513 | CV: 0.6299¬±0.0045\n",
      "  üéØ XGBoost... F1: 0.6759 | CV: 0.6572¬±0.0028\n",
      "  üéØ LightGBM... F1: 0.6887 | CV: 0.6605¬±0.0042\n",
      "\n",
      "üèÜ TOP 10 COMBINAISONS\n",
      "==================================================\n",
      "            Combination  F1_Test  Accuracy_Test  F1_CV_Mean\n",
      "      TF-IDF + Logistic 0.753036       0.799737    0.727661\n",
      "      TF-IDF + LightGBM 0.733061       0.785292    0.698744\n",
      "       TF-IDF + XGBoost 0.722130       0.780696    0.700260\n",
      "  TF-IDF + RandomForest 0.695499       0.773473    0.675841\n",
      "    FastText + LightGBM 0.688710       0.746553    0.660538\n",
      "     Word2Vec + XGBoost 0.683230       0.732108    0.657272\n",
      "     FastText + XGBoost 0.675947       0.724885    0.657223\n",
      "    Word2Vec + LightGBM 0.661789       0.726855    0.654733\n",
      "FastText + RandomForest 0.651278       0.722259    0.629894\n",
      "Word2Vec + RandomForest 0.648560       0.719632    0.632006\n",
      "\n",
      "ü•á MEILLEURE COMBINAISON:\n",
      "   TF-IDF + Logistic\n",
      "   F1 Test: 0.7530\n",
      "   Accuracy: 0.7997\n",
      "   F1 CV: 0.7277 ¬± 0.0058\n",
      "\n",
      "üìà ANALYSE PAR VECTORIZER:\n",
      "           F1_Test                 F1_CV_Mean\n",
      "              mean     max     std       mean\n",
      "Vectorizer                                   \n",
      "FastText    0.6482  0.6887  0.0501     0.6277\n",
      "TF-IDF      0.7259  0.7530  0.0240     0.7006\n",
      "Word2Vec    0.6508  0.6832  0.0310     0.6361\n",
      "\n",
      "ü§ñ ANALYSE PAR MOD√àLE:\n",
      "             F1_Test                 F1_CV_Mean\n",
      "                mean     max     std       mean\n",
      "Model                                          \n",
      "LightGBM      0.6945  0.7331  0.0360     0.6713\n",
      "Logistic      0.6465  0.7530  0.0938     0.6304\n",
      "RandomForest  0.6651  0.6955  0.0264     0.6459\n",
      "XGBoost       0.6938  0.7221  0.0248     0.6716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   Vectorizer         Model   F1_Test  Accuracy_Test  F1_CV_Mean  F1_CV_Std  \\\n",
       " 0      TF-IDF      Logistic  0.753036       0.799737    0.727661   0.005801   \n",
       " 3      TF-IDF      LightGBM  0.733061       0.785292    0.698744   0.008280   \n",
       " 2      TF-IDF       XGBoost  0.722130       0.780696    0.700260   0.008590   \n",
       " 1      TF-IDF  RandomForest  0.695499       0.773473    0.675841   0.009174   \n",
       " 11   FastText      LightGBM  0.688710       0.746553    0.660538   0.004212   \n",
       " 6    Word2Vec       XGBoost  0.683230       0.732108    0.657272   0.002953   \n",
       " 10   FastText       XGBoost  0.675947       0.724885    0.657223   0.002762   \n",
       " 7    Word2Vec      LightGBM  0.661789       0.726855    0.654733   0.005187   \n",
       " 9    FastText  RandomForest  0.651278       0.722259    0.629894   0.004510   \n",
       " 5    Word2Vec  RandomForest  0.648560       0.719632    0.632006   0.005177   \n",
       " 4    Word2Vec      Logistic  0.609600       0.679580    0.600327   0.003843   \n",
       " 8    FastText      Logistic  0.576728       0.670387    0.563323   0.008872   \n",
       " \n",
       "                 Combination  \n",
       " 0         TF-IDF + Logistic  \n",
       " 3         TF-IDF + LightGBM  \n",
       " 2          TF-IDF + XGBoost  \n",
       " 1     TF-IDF + RandomForest  \n",
       " 11      FastText + LightGBM  \n",
       " 6        Word2Vec + XGBoost  \n",
       " 10       FastText + XGBoost  \n",
       " 7       Word2Vec + LightGBM  \n",
       " 9   FastText + RandomForest  \n",
       " 5   Word2Vec + RandomForest  \n",
       " 4       Word2Vec + Logistic  \n",
       " 8       FastText + Logistic  ,\n",
       " [{'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'Logistic',\n",
       "   'F1_Test': 0.7530364372469636,\n",
       "   'Accuracy_Test': 0.7997373604727511,\n",
       "   'F1_CV_Mean': 0.7276606237936297,\n",
       "   'F1_CV_Std': 0.005801002031715946,\n",
       "   'Combination': 'TF-IDF + Logistic'},\n",
       "  {'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'RandomForest',\n",
       "   'F1_Test': 0.6954986760812003,\n",
       "   'Accuracy_Test': 0.7734734077478661,\n",
       "   'F1_CV_Mean': 0.6758405939711695,\n",
       "   'F1_CV_Std': 0.009173514875022739,\n",
       "   'Combination': 'TF-IDF + RandomForest'},\n",
       "  {'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'XGBoost',\n",
       "   'F1_Test': 0.7221297836938436,\n",
       "   'Accuracy_Test': 0.7806959947472094,\n",
       "   'F1_CV_Mean': 0.7002604639506317,\n",
       "   'F1_CV_Std': 0.008590499205535725,\n",
       "   'Combination': 'TF-IDF + XGBoost'},\n",
       "  {'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'LightGBM',\n",
       "   'F1_Test': 0.7330612244897959,\n",
       "   'Accuracy_Test': 0.7852921864740644,\n",
       "   'F1_CV_Mean': 0.6987440000633877,\n",
       "   'F1_CV_Std': 0.008279563091651485,\n",
       "   'Combination': 'TF-IDF + LightGBM'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'Logistic',\n",
       "   'F1_Test': 0.6096,\n",
       "   'Accuracy_Test': 0.6795797767564018,\n",
       "   'F1_CV_Mean': 0.6003273151034345,\n",
       "   'F1_CV_Std': 0.003843342905352083,\n",
       "   'Combination': 'Word2Vec + Logistic'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'RandomForest',\n",
       "   'F1_Test': 0.648559670781893,\n",
       "   'Accuracy_Test': 0.7196323046618516,\n",
       "   'F1_CV_Mean': 0.6320058422038578,\n",
       "   'F1_CV_Std': 0.0051772984771402564,\n",
       "   'Combination': 'Word2Vec + RandomForest'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'XGBoost',\n",
       "   'F1_Test': 0.6832298136645962,\n",
       "   'Accuracy_Test': 0.7321076822061721,\n",
       "   'F1_CV_Mean': 0.6572720989104005,\n",
       "   'F1_CV_Std': 0.0029529212302806126,\n",
       "   'Combination': 'Word2Vec + XGBoost'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'LightGBM',\n",
       "   'F1_Test': 0.6617886178861788,\n",
       "   'Accuracy_Test': 0.726854891661195,\n",
       "   'F1_CV_Mean': 0.6547331206341411,\n",
       "   'F1_CV_Std': 0.00518662576196552,\n",
       "   'Combination': 'Word2Vec + LightGBM'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'Logistic',\n",
       "   'F1_Test': 0.5767284991568297,\n",
       "   'Accuracy_Test': 0.670387393302692,\n",
       "   'F1_CV_Mean': 0.5633227413649674,\n",
       "   'F1_CV_Std': 0.008871980551601993,\n",
       "   'Combination': 'FastText + Logistic'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'RandomForest',\n",
       "   'F1_Test': 0.651277823577906,\n",
       "   'Accuracy_Test': 0.7222586999343401,\n",
       "   'F1_CV_Mean': 0.6298935099960262,\n",
       "   'F1_CV_Std': 0.004510108129895286,\n",
       "   'Combination': 'FastText + RandomForest'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'XGBoost',\n",
       "   'F1_Test': 0.6759474091260634,\n",
       "   'Accuracy_Test': 0.7248850952068286,\n",
       "   'F1_CV_Mean': 0.657223054378926,\n",
       "   'F1_CV_Std': 0.002762483888838669,\n",
       "   'Combination': 'FastText + XGBoost'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'LightGBM',\n",
       "   'F1_Test': 0.6887096774193548,\n",
       "   'Accuracy_Test': 0.7465528562048588,\n",
       "   'F1_CV_Mean': 0.6605381381404728,\n",
       "   'F1_CV_Std': 0.004211659462919232,\n",
       "   'Combination': 'FastText + LightGBM'}])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quick_start(df, target_col='target'):\n",
    "    \"\"\"D√©marrage rapide - teste tout automatiquement\"\"\"\n",
    "    \n",
    "    print(\"‚ö° QUICK START - Test complet automatique\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Split des donn√©es\n",
    "    X = df[['text', 'location', 'keyword']]\n",
    "    y = df[target_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Preprocessing\n",
    "    X_train_processed, _, _ = preprocess_data(X_train)\n",
    "    X_test_processed, _, _ = preprocess_data(X_test)\n",
    "    \n",
    "    # Test toutes les combinaisons\n",
    "    results = test_all_combinations(X_train_processed, X_test_processed, y_train, y_test)\n",
    "    \n",
    "    # Analyse\n",
    "    df_results = analyze_results(results)\n",
    "    \n",
    "    return df_results, results \n",
    "\n",
    "quick_start(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0149b-accd-4144-b485-f8b5d0050b5d",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf730cb1",
   "metadata": {},
   "source": [
    "#### Pr√©traitement sur les donn√©es (Split donn√©es + TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f3ccf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Preprocessing complet...\n",
      "üîß Preprocessing complet...\n",
      "  üìù TF-IDF...\n"
     ]
    }
   ],
   "source": [
    "# √âtape 1 : split des colonnes utiles\n",
    "X = data[['text', 'location', 'keyword']]\n",
    "y = data['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# √âtape 2 : preprocessing (d√©j√† d√©fini dans ton code)\n",
    "X_train_processed, _, _ = preprocess_data(X_train_raw)\n",
    "X_test_processed, _, _ = preprocess_data(X_test_raw)\n",
    "\n",
    "# √âtape 3 : vectorisation TF-IDF (d√©j√† d√©finie aussi)\n",
    "X_train_vec, X_test_vec, tfidf = get_tfidf_features(X_train_processed, X_test_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df35727",
   "metadata": {},
   "source": [
    "#### Optimisation : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "290b332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params LR: {'C': 1.5, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best F1 CV LR: 0.7390213038494786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 1.5, 2],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    param_grid,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_lr.fit(X_train_vec, y_train)\n",
    "print(\"Best params LR:\", grid_lr.best_params_)\n",
    "print(\"Best F1 CV LR:\", grid_lr.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076888e8",
   "metadata": {},
   "source": [
    "#### Optimisation : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params XGB: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 800}\n",
      "Best F1 CV XGB: 0.715595017305107\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [700, 800, 600],\n",
    "    'max_depth': [7, 5, 6],\n",
    "    'learning_rate': [0.5, 0.3, 0.2]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    param_grid_xgb,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_xgb.fit(X_train_vec, y_train)\n",
    "print(\"Best params XGB:\", grid_xgb.best_params_)\n",
    "print(\"Best F1 CV XGB:\", grid_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6258c1b",
   "metadata": {},
   "source": [
    "#### Optimisation : LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5eed4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params LGBM: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}\n",
      "Best F1 CV LGBM: 0.6870690104510798\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "param_grid_lgb = {\n",
    "    'n_estimators': [600, 400, 200],\n",
    "    'max_depth': [-1, -3, 1],\n",
    "    'learning_rate': [0.05, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_lgb = GridSearchCV(\n",
    "    lgb.LGBMClassifier(random_state=42),\n",
    "    param_grid_lgb,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_lgb.fit(X_train_vec, y_train)\n",
    "print(\"Best params LGBM:\", grid_lgb.best_params_)\n",
    "print(\"Best F1 CV LGBM:\", grid_lgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e7c45",
   "metadata": {},
   "source": [
    "#### √âvaluation sur test set du meilleur mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5743b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 test: 0.7583081570996979\n",
      "Accuracy test: 0.7898883782009193\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.81       869\n",
      "           1       0.75      0.77      0.76       654\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.79      0.79      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "best_model = grid_lr.best_estimator_\n",
    "y_pred = best_model.predict(X_test_vec)\n",
    "\n",
    "print(\"F1 test:\", f1_score(y_test, y_pred))\n",
    "print(\"Accuracy test:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94c440b-b6ca-429c-8d77-168787cc495f",
   "metadata": {},
   "source": [
    "### Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756326b-bbf2-4f3a-9f40-b9cad9b1126b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:50:22.393907Z",
     "iopub.status.busy": "2024-06-29T09:50:22.392902Z",
     "iopub.status.idle": "2024-06-29T09:50:22.809396Z",
     "shell.execute_reply": "2024-06-29T09:50:22.808570Z",
     "shell.execute_reply.started": "2024-06-29T09:50:22.393907Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an experiment if not exists\n",
    "exp_name = \"house-price\"\n",
    "experiment = mlflow.get_experiment_by_name(exp_name)\n",
    "if not experiment:\n",
    "    experiment_id = mlflow.create_experiment(exp_name)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "    \n",
    "logger.info(f\"Experience id: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3febfc98-87c9-455f-8f33-f4f1f0293780",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a783eb4c-6485-495f-9069-d8ac2548316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save vectoriser\n",
    "import joblib\n",
    "\n",
    "#joblib.dump(tfidf, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713431b-acbf-4af0-a743-34f1f7397187",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "# Sauvegarde\n",
    "joblib.dump(best_model, 'best_model.pkl')\n",
    "\n",
    "# Chargement plus tard\n",
    "loaded_model = joblib.load('best_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559083c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Preprocessing complet...\n",
      "  üìù TF-IDF...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['le_keyword.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# √âtape 1 : tout le dataset\n",
    "import joblib\n",
    "\n",
    "X_all = data[['text', 'location', 'keyword']]\n",
    "y_all = data['target']\n",
    "\n",
    "# √âtape 2 : preprocess complet\n",
    "X_all_processed, le_location, le_keyword = preprocess_data(X_all)\n",
    "\n",
    "# √âtape 3 : vectorisation\n",
    "X_all_vec, _, tfidf = get_tfidf_features(X_all_processed, X_all_processed)  # X_test est ignor√© ici\n",
    "\n",
    "# √âtape 4 : mod√®le avec meilleurs hyperparams (ex: logistic regression avec C=1.0)\n",
    "\n",
    "final_model = LogisticRegression(C=1.5, class_weight='balanced', max_iter=1000, random_state=42, penalty = 'l2', solver= 'lbfgs')\n",
    "\n",
    "final_model.fit(X_all_vec, y_all)\n",
    "\n",
    "# √âtape 5 : sauvegarde\n",
    "joblib.dump(final_model, 'model.pkl')\n",
    "joblib.dump(tfidf, 'tfidf.pkl')\n",
    "joblib.dump(le_location, 'le_location.pkl')\n",
    "joblib.dump(le_keyword, 'le_keyword.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8be508b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "data['clean_text'] = data['text'].apply(clean_text)\n",
    "X_train_vec = tfidf.fit_transform(data['clean_text'])\n",
    "joblib.dump(tfidf, 'tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea6214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfer = joblib.load('tfidf.pkl')\n",
    "tfidfer.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6eb95476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf.transform()\n",
    "type(data[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfa6d35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7613x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 99740 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.transform(data[\"clean_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
