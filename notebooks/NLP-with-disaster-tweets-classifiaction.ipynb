{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a46012-b136-43a9-ad81-76ac62a83136",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Pour la suite du cours, nous allons faire l'apprentissage par la pratique avec un cas r√©el de projet de Machine Learning\n",
    "\n",
    "Description du projet: Pr√©dire le prix des biens immobiliers √† Ames, Iowa.\n",
    "\n",
    "Les donn√©es sont disponibles dans: [sklearn.datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml)\n",
    "\n",
    "\n",
    "Voici un r√©capitulatif des objectifs :\n",
    "- R√©aliser une analyse exploratoire.\n",
    "- Tester diff√©rents mod√®les de pr√©diction afin de r√©pondre au mieux √† la probl√©matique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f714cf0-f5bc-491c-80e4-38472f4d409d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T12:28:27.081285Z",
     "iopub.status.busy": "2024-06-28T12:28:27.081285Z",
     "iopub.status.idle": "2024-06-28T12:28:27.098789Z",
     "shell.execute_reply": "2024-06-28T12:28:27.097789Z",
     "shell.execute_reply.started": "2024-06-28T12:28:27.081285Z"
    }
   },
   "source": [
    "__Sources utiles__\n",
    "\n",
    "- [Introduction √† MLOps](https://ashutoshtripathi.com/2021/08/18/mlops-a-complete-guide-to-machine-learning-operations-mlops-vs-devops/)\n",
    "\n",
    "- [MLFLOW - Site de r√©f√©rence](https://mlflow.org/docs/latest/index.html)\n",
    "- [MLFLOW - Tutorial](https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html)\n",
    "- [MLFLOW - Tracking](https://mlflow.org/docs/latest/tracking.html)\n",
    "- [MLFLOW - Model Registry](https://mlflow.org/docs/latest/model-registry.html#)\n",
    "- [MLFLOW - Serve a model](https://mlflow.org/docs/latest/model-registry.html#serving-an-mlflow-model-from-model-registry)\n",
    "\n",
    "- [Evidently - tutorial d'analyse de Data drift](https://github.com/evidentlyai/evidently/tree/main/examples/sample_notebooks)\n",
    "- [API Flask - D√©marche de mise en oeuvre](http://web.univ-ubs.fr/lmba/lardjane/python/c4.pdf)\n",
    "- [FastAPI - D√©marche de mise en oeuvre](https://towardsdatascience.com/how-to-build-and-deploy-a-machine-learning-model-with-fastapi-64c505213857)\n",
    "- [Azure - Tuto d√©ploiement application web ](https://learn.microsoft.com/fr-fr/azure/app-service/quickstart-python?tabs=flask%2Cwindows%2Cazure-portal%2Cvscode-deploy%2Cdeploy-instructions-azportal%2Cterminal-bash%2Cdeploy-instructions-zip-azcli)\n",
    "- [Tests unitaires - Unittest ou Pytest](https://www.sitepoint.com/python-unit-testing-unittest-pytest/)\n",
    "\n",
    "- [Pythonanywhere](https://www.pythonanywhere.com/)\n",
    "- [Heroku](https://www.heroku.com/)\n",
    "-[Azure webapp - D√©ploiement automatis√© via Github](https://learn.microsoft.com/fr-fr/azure/app-service/deploy-continuous-deployment?tabs=github)\n",
    "- Streamlit ou gradio pour la mise en place d'un dashbord\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14612581-a3ef-4522-8ddd-64adf0221e56",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038e700c-aabe-4e62-9b37-d8ffa755cf32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T10:02:04.633195Z",
     "iopub.status.busy": "2024-06-29T10:02:04.633195Z",
     "iopub.status.idle": "2024-06-29T10:02:05.612841Z",
     "shell.execute_reply": "2024-06-29T10:02:05.611845Z",
     "shell.execute_reply.started": "2024-06-29T10:02:04.633195Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pendulum\n",
    "import plotly.express as px\n",
    "import ppscore as pps\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, TransformedTargetRegressor\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, VotingRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.metrics import (r2_score,\n",
    "                             root_mean_squared_error,\n",
    "                             mean_absolute_percentage_error,\n",
    "                             mean_absolute_error,\n",
    "                             max_error,\n",
    "                            )\n",
    "from sklearn.model_selection import train_test_split, learning_curve, LearningCurveDisplay\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from ydata_profiling import ProfileReport\n",
    "from yellowbrick.regressor import PredictionError, ResidualsPlot\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from settings.params import MODEL_PARAMS, SEED\n",
    "from src.make_dataset import load_data\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "set_config(display=\"diagram\", print_changed_only=False)  # display sklearn pipeline as diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d2c00-ebe7-409c-b936-a3a36a10a6b0",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c12ca6-6ee6-4a5b-bd46-b567a6c5c34b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:21.715720Z",
     "iopub.status.busy": "2024-06-29T09:03:21.714721Z",
     "iopub.status.idle": "2024-06-29T09:03:21.867996Z",
     "shell.execute_reply": "2024-06-29T09:03:21.867826Z",
     "shell.execute_reply.started": "2024-06-29T09:03:21.715720Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-30 10:35:54.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - Target name: target\n",
      "\u001b[32m2025-07-30 10:35:54.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \n",
      "Project directory: c:\\Users\\DELL\\Desktop\\mlops-project - Tweets \n",
      "Reports dir: c:\\Users\\DELL\\Desktop\\mlops-project - Tweets\\reports\n"
     ]
    }
   ],
   "source": [
    "# Set logging format\n",
    "log_fmt = \"<green>{time:YYYY-MM-DD HH:mm:ss.SSS!UTC}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - {message}\"\n",
    "logger.configure(handlers=[{\"sink\": sys.stderr, \"format\": log_fmt}])\n",
    "\n",
    "# current data\n",
    "CURRENT_DATE = pendulum.now(tz=\"UTC\")\n",
    "\n",
    "# target name definition\n",
    "TARGET_NAME = MODEL_PARAMS[\"TARGET_NAME\"]\n",
    "logger.info(f\"Target name: {TARGET_NAME}\")\n",
    "\n",
    "\n",
    "# directories\n",
    "PROJECT_DIR = Path.cwd().parent\n",
    "REPORTS_DIR = Path(PROJECT_DIR, \"reports\")\n",
    "\n",
    "logger.info(f\"\\nProject directory: {PROJECT_DIR} \\nReports dir: {REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9b5106-eb95-4b15-8655-836f5b458283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:21.869006Z",
     "iopub.status.busy": "2024-06-29T09:03:21.869006Z",
     "iopub.status.idle": "2024-06-29T09:03:22.009874Z",
     "shell.execute_reply": "2024-06-29T09:03:22.008866Z",
     "shell.execute_reply.started": "2024-06-29T09:03:21.869006Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TARGET_NAME': 'target',\n",
       " 'MIN_COMPLETION_RATE': 0.75,\n",
       " 'MIN_PPS': 0.1,\n",
       " 'DEFAULT_FEATURE_NAMES': ['Alley',\n",
       "  'BsmtQual',\n",
       "  'ExterQual',\n",
       "  'Foundation',\n",
       "  'FullBath',\n",
       "  'GarageArea',\n",
       "  'GarageCars',\n",
       "  'GarageFinish',\n",
       "  'GarageType',\n",
       "  'GrLivArea',\n",
       "  'KitchenQualMSSubClass',\n",
       "  'Neighborhood',\n",
       "  'OverallQual',\n",
       "  'TotRmsAbvGrd',\n",
       "  'building_age',\n",
       "  'remodel_age',\n",
       "  'garage_age'],\n",
       " 'TEST_SIZE': 0.25}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17884a5-99c8-452a-b3fb-7cf45931ca1b",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7288f4e1-0a82-4ab1-ad61-a4f80aeb4289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.011873Z",
     "iopub.status.busy": "2024-06-29T09:03:22.010873Z",
     "iopub.status.idle": "2024-06-29T09:03:22.199509Z",
     "shell.execute_reply": "2024-06-29T09:03:22.198509Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.011873Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data like pandas.DataFrame\n",
    "data = load_data(dataset_name=\"tweets\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "071fe3c2-3617-43b6-aca2-045339e61256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.201512Z",
     "iopub.status.busy": "2024-06-29T09:03:22.200509Z",
     "iopub.status.idle": "2024-06-29T09:03:22.373482Z",
     "shell.execute_reply": "2024-06-29T09:03:22.372486Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.201512Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f0fb9-7a60-447c-81bd-3fa18149fa7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.374486Z",
     "iopub.status.busy": "2024-06-29T09:03:22.373482Z",
     "iopub.status.idle": "2024-06-29T09:03:22.514516Z",
     "shell.execute_reply": "2024-06-29T09:03:22.514516Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.374486Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c5b45-80f1-48d6-bad9-5d95640cb17d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.674645Z",
     "iopub.status.busy": "2024-06-29T09:03:22.674645Z",
     "iopub.status.idle": "2024-06-29T09:03:22.938731Z",
     "shell.execute_reply": "2024-06-29T09:03:22.937730Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.674645Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.describe(include=\"all\") #, datetime_is_numeric=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b46232-40eb-4568-be4f-9c9f2a8a0d4d",
   "metadata": {},
   "source": [
    "# EDA: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a178177-9ff1-4b6d-b967-b328cae9a06b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.940731Z",
     "iopub.status.busy": "2024-06-29T09:03:22.939726Z",
     "iopub.status.idle": "2024-06-29T09:03:24.434116Z",
     "shell.execute_reply": "2024-06-29T09:03:24.434116Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.940731Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# barplot for missing value rate\n",
    "msno.bar(data,\n",
    "         filter=\"top\",\n",
    "         p=MODEL_PARAMS[\"MIN_COMPLETION_RATE\"],\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1e6dc-d2a7-4750-902c-79516b698c3b",
   "metadata": {},
   "source": [
    "## Target analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed730084-8eed-4d15-962a-0f1c1fc98d49",
   "metadata": {
    "tags": []
   },
   "source": [
    "Todo:\n",
    "\n",
    "    1- Ajouter un parameter dans le dictionnaire MODEL_PARAMS, appel√© TARGET_NAME, qui prend la valeur \"SalePrice\"\n",
    "\n",
    "    2- Construite l'histogramme de la variable cible en r√©cup√©rant le nom depuis le dictionnaire MODEL_PARAMS\n",
    "    3- Construire un 2e graphique sur la m√™me ligne (variable target transformer en log)\n",
    "    4- Continuer l'analyse exploratoire des variables  explicatives (quanti et quali)\n",
    "    5- Faire une premi√®re s√©lection des variables explicatives √† utiliser pour le mod√®le \n",
    "    6- Ajouter dans MODEL_PARAMS, un key appel√© FEATURE_NAMES, qui stocke la liste des variables explicatives s√©lectionn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb20318-d0eb-4dd6-94d2-a3c06bdcff13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:24.437117Z",
     "iopub.status.busy": "2024-06-29T09:03:24.436116Z",
     "iopub.status.idle": "2024-06-29T09:03:24.576283Z",
     "shell.execute_reply": "2024-06-29T09:03:24.576283Z",
     "shell.execute_reply.started": "2024-06-29T09:03:24.437117Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_PARAMS[\"TARGET_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08927768",
   "metadata": {},
   "source": [
    "### Distribution des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1d76b-ac16-4a60-a4ce-af3b8c225e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:24.578283Z",
     "iopub.status.busy": "2024-06-29T09:03:24.577279Z",
     "iopub.status.idle": "2024-06-29T09:03:24.733931Z",
     "shell.execute_reply": "2024-06-29T09:03:24.732887Z",
     "shell.execute_reply.started": "2024-06-29T09:03:24.577279Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_counts = data[TARGET_NAME].value_counts()\n",
    "target_pct = data[TARGET_NAME].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"Distribution des classes:\")\n",
    "print(f\"  Classe 0 (Non-Disaster): {target_counts[0]} ({target_pct[0]:.1f}%)\")\n",
    "print(f\"  Classe 1 (Disaster): {target_counts[1]} ({target_pct[1]:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Graphique en barres\n",
    "target_counts.plot(kind='bar', ax=axes[0], color=['skyblue', 'salmon'])\n",
    "axes[0].set_title('Distribution des Classes')\n",
    "axes[0].set_xlabel('Target (0=Non-Disaster, 1=Disaster)')\n",
    "axes[0].set_ylabel('Nombre de tweets')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Graphique en secteurs\n",
    "axes[1].pie(target_counts.values, labels=['Non-Disaster', 'Disaster'], \n",
    "           autopct='%1.1f%%', colors=['skyblue', 'salmon'])\n",
    "axes[1].set_title('R√©partition des Classes')\n",
    "\n",
    "# Graphique d'√©quilibre\n",
    "class_balance = min(target_counts) / max(target_counts)\n",
    "axes[2].bar(['√âquilibre des Classes'], [class_balance], color='orange')\n",
    "axes[2].set_title(f'√âquilibre: {class_balance:.2f}')\n",
    "axes[2].set_ylabel('Ratio (min/max)')\n",
    "axes[2].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9456b",
   "metadata": {},
   "source": [
    "## Analyse des features cat√©gorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd78d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä 4. ANALYSE DES FEATURES CAT√âGORIELLES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Analyse des keywords\n",
    "print(\"üîë ANALYSE DES KEYWORDS:\")\n",
    "keyword_counts = data['keyword'].value_counts(dropna=False).head(15)\n",
    "print(f\"Top 15 keywords les plus fr√©quents:\")\n",
    "print(keyword_counts)\n",
    "\n",
    "# Keywords par classe\n",
    "keyword_disaster = data[data['target'] == 1]['keyword'].value_counts(dropna=False).head(10)\n",
    "keyword_normal = data[data['target'] == 0]['keyword'].value_counts(dropna=False).head(10)\n",
    "\n",
    "print(f\"\\nTop 10 keywords - Disaster tweets:\")\n",
    "print(keyword_disaster)\n",
    "print(f\"\\nTop 10 keywords - Normal tweets:\")\n",
    "print(keyword_normal)\n",
    "\n",
    "# Analyse des locations\n",
    "print(\"\\nüìç ANALYSE DES LOCATIONS:\")\n",
    "location_counts = data['location'].value_counts(dropna=False).head(15)\n",
    "print(f\"Top 15 locations les plus fr√©quentes:\")\n",
    "print(location_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Keywords globaux\n",
    "keyword_counts.plot(kind='barh', ax=axes[0,0], color='lightblue')\n",
    "axes[0,0].set_title('Top 15 Keywords')\n",
    "axes[0,0].set_xlabel('Fr√©quence')\n",
    "\n",
    "# Locations globales\n",
    "location_counts.plot(kind='barh', ax=axes[0,1], color='lightgreen')\n",
    "axes[0,1].set_title('Top 15 Locations')\n",
    "axes[0,1].set_xlabel('Fr√©quence')\n",
    "\n",
    "# Keywords par classe\n",
    "keyword_disaster.head(8).plot(kind='barh', ax=axes[1,0], color='salmon')\n",
    "axes[1,0].set_title('Top Keywords - Disaster Tweets')\n",
    "axes[1,0].set_xlabel('Fr√©quence')\n",
    "\n",
    "keyword_normal.head(8).plot(kind='barh', ax=axes[1,1], color='skyblue')\n",
    "axes[1,1].set_title('Top Keywords - Normal Tweets')\n",
    "axes[1,1].set_xlabel('Fr√©quence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0080ae36",
   "metadata": {},
   "source": [
    "### Analyse de texte approfondie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99642702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä 5. ANALYSE APPROFONDIE DU TEXTE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Longueur des textes\n",
    "data['text_length'] = data['text'].str.len()\n",
    "data['word_count'] = data['text'].str.split().str.len()\n",
    "\n",
    "print(\"üìè STATISTIQUES DE LONGUEUR:\")\n",
    "length_stats = data.groupby('target')[['text_length', 'word_count']].describe()\n",
    "print(length_stats)\n",
    "\n",
    "# Caract√®res sp√©ciaux\n",
    "data['url_count'] = data['text'].str.count(r'http\\S+|www\\S+')\n",
    "data['mention_count'] = data['text'].str.count(r'@\\w+')\n",
    "data['hashtag_count'] = data['text'].str.count(r'#\\w+')\n",
    "data['exclamation_count'] = data['text'].str.count(r'!')\n",
    "data['question_count'] = data['text'].str.count(r'\\?')\n",
    "data['caps_count'] = data['text'].str.count(r'[A-Z]')\n",
    "\n",
    "print(\"\\nüîç CARACT√àRES SP√âCIAUX par classe:\")\n",
    "special_chars = ['url_count', 'mention_count', 'hashtag_count', 'exclamation_count', 'question_count', 'caps_count']\n",
    "special_stats = data.groupby('target')[special_chars].mean()\n",
    "print(special_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7299e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des longueurs\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Distribution longueur caract√®res\n",
    "for i, target in enumerate([0, 1]):\n",
    "    df = data[data[TARGET_NAME] == 0]['text_length']\n",
    "    axes[0, 0].hist(df, bins=30, alpha=0.7, label=f'Target {target}')\n",
    "axes[0, 0].set_title('Distribution - Longueur en Caract√®res')\n",
    "axes[0, 0].set_xlabel('Nombre de caract√®res')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Distribution nombre de mots\n",
    "for i, target in enumerate([0, 1]):\n",
    "    df = data[data[TARGET_NAME] == target]['word_count']\n",
    "    axes[0, 1].hist(df, bins=30, alpha=0.7, label=f'Target {target}')\n",
    "axes[0, 1].set_title('Distribution - Nombre de Mots')\n",
    "axes[0, 1].set_xlabel('Nombre de mots')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Boxplot longueurs par classe\n",
    "data.boxplot(column='text_length', by='target', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Longueur par Classe')\n",
    "axes[0, 2].set_xlabel('Target')\n",
    "\n",
    "# Caract√®res sp√©ciaux\n",
    "special_stats.T.plot(kind='bar', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Caract√®res Sp√©ciaux par Classe')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].legend(['Non-Disaster', 'Disaster'])\n",
    "\n",
    "# Corr√©lation longueur vs target\n",
    "correlation_length = data[['text_length', 'word_count', 'target']].corr()\n",
    "sns.heatmap(correlation_length, annot=True, ax=axes[1, 1], cmap='coolwarm')\n",
    "axes[1, 1].set_title('Corr√©lation Longueur vs Target')\n",
    "\n",
    "# Distribution caps par classe\n",
    "for i, target in enumerate([0, 1]):\n",
    "    df = data[data[TARGET_NAME] == target]['caps_count']\n",
    "    axes[1, 2].hist(df, bins=20, alpha=0.7, label=f'Target {target}')\n",
    "axes[1, 2].set_title('Distribution - Lettres Majuscules')\n",
    "axes[1, 2].set_xlabel('Nombre de majuscules')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757fb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## T√©l√©charger NLTK\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648e7c2",
   "metadata": {},
   "source": [
    "### Analyse des mots les plus fr√©quents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour l'analyse textuelle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "print(\"\\nüìä 6. ANALYSE DES MOTS LES PLUS FR√âQUENTS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def clean_text_for_analysis(text):\n",
    "    \"\"\"Nettoyage basique pour l'analyse des mots\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Pr√©parer les textes\n",
    "disaster_texts = data[data['target'] == 1]['text'].apply(clean_text_for_analysis)\n",
    "normal_texts = data[data['target'] == 0]['text'].apply(clean_text_for_analysis)\n",
    "\n",
    "# Mots les plus fr√©quents\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_top_words(texts, n=20):\n",
    "    \"\"\"Obtenir les mots les plus fr√©quents\"\"\"\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "        all_words.extend(words)\n",
    "    return Counter(all_words).most_common(n)\n",
    "\n",
    "disaster_words = get_top_words(disaster_texts)\n",
    "normal_words = get_top_words(normal_texts)\n",
    "\n",
    "\n",
    "# Visualisation des mots fr√©quents\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Disaster words\n",
    "disaster_df = pd.DataFrame(disaster_words, columns=['Word', 'Count'])\n",
    "disaster_df.plot(x='Word', y='Count', kind='barh', ax=axes[0], color='salmon')\n",
    "axes[0].set_title('Top 20 Mots - Disaster Tweets')\n",
    "axes[0].set_xlabel('Fr√©quence')\n",
    "\n",
    "# Normal words\n",
    "normal_df = pd.DataFrame(normal_words, columns=['Word', 'Count'])\n",
    "normal_df.plot(x='Word', y='Count', kind='barh', ax=axes[1], color='skyblue')\n",
    "axes[1].set_title('Top 20 Mots - Normal Tweets')\n",
    "axes[1].set_xlabel('Fr√©quence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2652f89",
   "metadata": {},
   "source": [
    "### Word Clouds (Nuages de points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de6d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nüìä 7. NUAGES DE MOTS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Word cloud pour disaster tweets\n",
    "    disaster_text_combined = ' '.join(disaster_texts)\n",
    "    wordcloud_disaster = WordCloud(width=800, height=400, \n",
    "                                 background_color='white',\n",
    "                                 stopwords=stop_words,\n",
    "                                 max_words=100).generate(disaster_text_combined)\n",
    "\n",
    "    # Word cloud pour normal tweets\n",
    "    normal_text_combined = ' '.join(normal_texts)\n",
    "    wordcloud_normal = WordCloud(width=800, height=400, \n",
    "                               background_color='white',\n",
    "                               stopwords=stop_words,\n",
    "                               max_words=100).generate(normal_text_combined)\n",
    "\n",
    "    # Affichage\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "    axes[0].imshow(wordcloud_disaster, interpolation='bilinear')\n",
    "    axes[0].set_title('Word Cloud - Disaster Tweets', fontsize=16)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(wordcloud_normal, interpolation='bilinear')\n",
    "    axes[1].set_title('Word Cloud - Normal Tweets', fontsize=16)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  WordCloud non disponible. Installer avec: pip install wordcloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517cfc70",
   "metadata": {},
   "source": [
    "### Analyse des corr√©lations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä 8. ANALYSE DE CORR√âLATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Matrice de corr√©lation pour les variables num√©riques\n",
    "numeric_features = ['text_length', 'word_count', 'url_count', 'mention_count', \n",
    "                   'hashtag_count', 'exclamation_count', 'question_count', 'caps_count', 'target']\n",
    "\n",
    "correlation_matrix = data[numeric_features].corr()\n",
    "\n",
    "print(\"üîó Corr√©lations avec la variable target:\")\n",
    "target_correlations = correlation_matrix['target'].sort_values(key=abs, ascending=False)\n",
    "print(target_correlations)\n",
    "\n",
    "# Heatmap de corr√©lation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Matrice de Corr√©lation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617dc2b0",
   "metadata": {},
   "source": [
    "### Analyse Comparative D√©taill√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e85b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä 9. ANALYSE COMPARATIVE D√âTAILL√âE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Tests statistiques\n",
    "from scipy.stats import ttest_ind, chi2_contingency\n",
    "\n",
    "print(\"üß™ TESTS STATISTIQUES:\")\n",
    "\n",
    "# Test t pour longueur de texte\n",
    "disaster_lengths = data[data['target'] == 1]['text_length']\n",
    "normal_lengths = data[data['target'] == 0]['text_length']\n",
    "t_stat, p_value = ttest_ind(disaster_lengths, normal_lengths)\n",
    "\n",
    "print(f\"Test t - Longueur texte: t={t_stat:.3f}, p={p_value:.6f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"  ‚úÖ Diff√©rence significative dans la longueur des textes\")\n",
    "else:\n",
    "    print(\"  ‚ùå Pas de diff√©rence significative dans la longueur des textes\")\n",
    "\n",
    "# Test du chi-carr√© pour keywords\n",
    "if not data['keyword'].isna().all():\n",
    "    keyword_target_crosstab = pd.crosstab(data['keyword'].fillna('Missing'), data['target'])\n",
    "    chi2, p_val, dof, expected = chi2_contingency(keyword_target_crosstab)\n",
    "    print(f\"Test Chi¬≤ - Keywords: œá¬≤={chi2:.3f}, p={p_val:.6f}\")\n",
    "    if p_val < 0.05:\n",
    "        print(\"  ‚úÖ Association significative entre keywords et target\")\n",
    "    else:\n",
    "        print(\"  ‚ùå Pas d'association significative entre keywords et target\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96b26f-08a8-491f-88f6-1758dffba7fe",
   "metadata": {},
   "source": [
    "## Pr√©traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f69ca4-89d7-4b96-9924-22b66335612b",
   "metadata": {},
   "source": [
    "#### IMPORTS COMPLETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b121fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Pour Word2Vec et FastText\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "import re\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637abaee",
   "metadata": {},
   "source": [
    "#### PREPROCESSING COMPLET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d42b5258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'keyword', 'location', 'text', 'target', 'url_count',\n",
      "       'text_length', 'mention_count', 'question_count', 'caps_count',\n",
      "       'exclamation_count', 'hashtag_count', 'word_count', 'urgency_score',\n",
      "       'social_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_features(df):\n",
    "    \"\"\"Extrait TOUTES les features importantes selon votre analyse\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    # Top features (dans l'ordre de corr√©lation)\n",
    "    features['url_count'] = df['text'].str.count(r'http\\S+|www\\S+')           # +0.196\n",
    "    features['text_length'] = df['text'].str.len()                            # +0.182\n",
    "    features['mention_count'] = df['text'].str.count(r'@\\w+')                 # -0.103\n",
    "    features['question_count'] = df['text'].str.count(r'\\?')                  # -0.084\n",
    "    features['caps_count'] = df['text'].str.count(r'[A-Z]')                   # +0.078\n",
    "    features['exclamation_count'] = df['text'].str.count(r'!')                # -0.075\n",
    "    features['hashtag_count'] = df['text'].str.count(r'#\\w+')                 # +0.052\n",
    "    features['word_count'] = df['text'].str.split().str.len()                 # +0.040\n",
    "    \n",
    "    # Features compos√©es\n",
    "    features['urgency_score'] = features['url_count'] + features['caps_count']\n",
    "    features['social_score'] = features['mention_count'] + features['hashtag_count']\n",
    "    \n",
    "    return features\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Nettoyage intelligent du texte\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocessing complet - remplace votre classe\"\"\"\n",
    "    print(\"üîß Preprocessing complet...\")\n",
    "    \n",
    "    # Features num√©riques\n",
    "    df_features = extract_features(df)\n",
    "    \n",
    "    # Nettoyage du texte\n",
    "    df_features['clean_text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    # Encodage des cat√©gories\n",
    "    le_location = LabelEncoder()\n",
    "    le_keyword = LabelEncoder()\n",
    "    \n",
    "    df_features['location_encoded'] = le_location.fit_transform(df['location'].fillna('unknown'))\n",
    "    df_features['keyword_encoded'] = le_keyword.fit_transform(df['keyword'].fillna('none'))\n",
    "    \n",
    "    return df_features, le_location, le_keyword\n",
    "\n",
    "print(extract_features(data).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944ca47",
   "metadata": {},
   "source": [
    "#### TOUS LES VECTORIZERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e54e221-7315-4a73-9991-2445cc7ae18a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:25.675870Z",
     "iopub.status.busy": "2024-06-29T09:03:25.674867Z",
     "iopub.status.idle": "2024-06-29T09:03:25.848597Z",
     "shell.execute_reply": "2024-06-29T09:03:25.848597Z",
     "shell.execute_reply.started": "2024-06-29T09:03:25.675870Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tfidf_features(X_train, X_test, max_features=5000):\n",
    "    \"\"\"TF-IDF Vectorization\"\"\"\n",
    "    print(\"  üìù TF-IDF...\")\n",
    "    tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(1,2))\n",
    "    X_train_vec = tfidf.fit_transform(X_train['clean_text'])\n",
    "    X_test_vec = tfidf.transform(X_test['clean_text'])\n",
    "    return X_train_vec, X_test_vec, tfidf\n",
    "\n",
    "def get_word2vec_features(X_train, X_test, vector_size=100):\n",
    "    \"\"\"Word2Vec Vectorization\"\"\"\n",
    "    print(\"  üß† Word2Vec...\")\n",
    "    \n",
    "    # Tokenisation\n",
    "    train_tokens = [text.split() for text in X_train['clean_text']]\n",
    "    test_tokens = [text.split() for text in X_test['clean_text']]\n",
    "    \n",
    "    # Entra√Ænement Word2Vec\n",
    "    w2v_model = Word2Vec(sentences=train_tokens, vector_size=vector_size, window=5, min_count=2, workers=4)\n",
    "    \n",
    "    # Moyenne des vecteurs par document\n",
    "    def average_vectors(tokens, model):\n",
    "        vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "    \n",
    "    X_train_vec = np.array([average_vectors(tokens, w2v_model) for tokens in train_tokens])\n",
    "    X_test_vec = np.array([average_vectors(tokens, w2v_model) for tokens in test_tokens])\n",
    "    \n",
    "    return csr_matrix(X_train_vec), csr_matrix(X_test_vec), w2v_model\n",
    "\n",
    "def get_fasttext_features(X_train, X_test, vector_size=100):\n",
    "    \"\"\"FastText Vectorization\"\"\"\n",
    "    print(\"  ‚ö° FastText...\")\n",
    "    \n",
    "    # Tokenisation\n",
    "    train_tokens = [text.split() for text in X_train['clean_text']]\n",
    "    test_tokens = [text.split() for text in X_test['clean_text']]\n",
    "    \n",
    "    # Entra√Ænement FastText\n",
    "    ft_model = FastText(sentences=train_tokens, vector_size=vector_size, window=5, min_count=2, workers=4)\n",
    "    \n",
    "    # Moyenne des vecteurs par document\n",
    "    def average_vectors(tokens, model):\n",
    "        vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "    \n",
    "    X_train_vec = np.array([average_vectors(tokens, ft_model) for tokens in train_tokens])\n",
    "    X_test_vec = np.array([average_vectors(tokens, ft_model) for tokens in test_tokens])\n",
    "    \n",
    "    return csr_matrix(X_train_vec), csr_matrix(X_test_vec), ft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036992e",
   "metadata": {},
   "source": [
    "#### MOD√àLES √Ä TESTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "536e9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    'Logistic': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss', verbosity=0),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "vectorizers_config = {\n",
    "    'TF-IDF': get_tfidf_features,\n",
    "    'Word2Vec': get_word2vec_features,\n",
    "    'FastText': get_fasttext_features\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52765c",
   "metadata": {},
   "source": [
    "#### TEST TOUTES COMBINAISONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56050150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_combinations(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Test TOUTES les combinaisons mod√®le + vectorizer\"\"\"\n",
    "    \n",
    "    print(\"üöÄ TEST DE TOUTES LES COMBINAISONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Features num√©riques (communes √† tous)\n",
    "    num_features = ['url_count', 'text_length', 'mention_count', 'question_count',\n",
    "                   'caps_count', 'exclamation_count', 'hashtag_count', 'word_count',\n",
    "                   'urgency_score', 'social_score', 'location_encoded', 'keyword_encoded']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train[num_features])\n",
    "    X_test_num = scaler.transform(X_test[num_features])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Boucle sur tous les vectorizers\n",
    "    for vec_name, vec_func in vectorizers_config.items():\n",
    "        print(f\"\\nüìä VECTORIZER: {vec_name}\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        try:\n",
    "            # Obtenir les features textuelles\n",
    "            X_train_text, X_test_text, vectorizer = vec_func(X_train, X_test)\n",
    "            \n",
    "            # Combiner avec les features num√©riques\n",
    "            X_train_final = hstack([X_train_text, csr_matrix(X_train_num)])\n",
    "            X_test_final = hstack([X_test_text, csr_matrix(X_test_num)])\n",
    "            \n",
    "            # Tester tous les mod√®les avec ce vectorizer\n",
    "            for model_name, model in models_config.items():\n",
    "                print(f\"  üéØ {model_name}...\", end=\" \")\n",
    "                \n",
    "                try:\n",
    "                    # Entra√Ænement\n",
    "                    model.fit(X_train_final, y_train)\n",
    "                    \n",
    "                    # Pr√©dictions\n",
    "                    y_pred = model.predict(X_test_final)\n",
    "                    \n",
    "                    # M√©triques\n",
    "                    f1 = f1_score(y_test, y_pred)\n",
    "                    accuracy = accuracy_score(y_test, y_pred)\n",
    "                    \n",
    "                    # Cross-validation\n",
    "                    cv_scores = cross_val_score(model, X_train_final, y_train, cv=3, scoring='f1')\n",
    "                    cv_mean = cv_scores.mean()\n",
    "                    cv_std = cv_scores.std()\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Vectorizer': vec_name,\n",
    "                        'Model': model_name,\n",
    "                        'F1_Test': f1,\n",
    "                        'Accuracy_Test': accuracy,\n",
    "                        'F1_CV_Mean': cv_mean,\n",
    "                        'F1_CV_Std': cv_std,\n",
    "                        'Combination': f\"{vec_name} + {model_name}\"\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"F1: {f1:.4f} | CV: {cv_mean:.4f}¬±{cv_std:.4f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Erreur: {str(e)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur avec {vec_name}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed611f",
   "metadata": {},
   "source": [
    "#### Analyse des r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46160ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyse compl√®te des r√©sultats\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ùå Aucun r√©sultat √† analyser\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df_sorted = df.sort_values('F1_Test', ascending=False)\n",
    "    \n",
    "    print(\"\\nüèÜ TOP 10 COMBINAISONS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(df_sorted.head(10)[['Combination', 'F1_Test', 'Accuracy_Test', 'F1_CV_Mean']].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nü•á MEILLEURE COMBINAISON:\")\n",
    "    best = df_sorted.iloc[0]\n",
    "    print(f\"   {best['Combination']}\")\n",
    "    print(f\"   F1 Test: {best['F1_Test']:.4f}\")\n",
    "    print(f\"   Accuracy: {best['Accuracy_Test']:.4f}\")\n",
    "    print(f\"   F1 CV: {best['F1_CV_Mean']:.4f} ¬± {best['F1_CV_Std']:.4f}\")\n",
    "    \n",
    "    # Analyse par vectorizer\n",
    "    print(f\"\\nüìà ANALYSE PAR VECTORIZER:\")\n",
    "    vec_analysis = df.groupby('Vectorizer').agg({\n",
    "        'F1_Test': ['mean', 'max', 'std'],\n",
    "        'F1_CV_Mean': 'mean'\n",
    "    }).round(4)\n",
    "    print(vec_analysis)\n",
    "    \n",
    "    # Analyse par mod√®le\n",
    "    print(f\"\\nü§ñ ANALYSE PAR MOD√àLE:\")\n",
    "    model_analysis = df.groupby('Model').agg({\n",
    "        'F1_Test': ['mean', 'max', 'std'],\n",
    "        'F1_CV_Mean': 'mean'\n",
    "    }).round(4)\n",
    "    print(model_analysis)\n",
    "    \n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f499d",
   "metadata": {},
   "source": [
    "#### Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "558ad004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° QUICK START - Test complet automatique\n",
      "==================================================\n",
      "üîß Preprocessing complet...\n",
      "üîß Preprocessing complet...\n",
      "üöÄ TEST DE TOUTES LES COMBINAISONS\n",
      "==================================================\n",
      "\n",
      "üìä VECTORIZER: TF-IDF\n",
      "-------------------------\n",
      "  üìù TF-IDF...\n",
      "  üéØ Logistic... F1: 0.7530 | CV: 0.7277¬±0.0058\n",
      "  üéØ RandomForest... F1: 0.6955 | CV: 0.6758¬±0.0092\n",
      "  üéØ XGBoost... F1: 0.7221 | CV: 0.7003¬±0.0086\n",
      "  üéØ LightGBM... F1: 0.7331 | CV: 0.6987¬±0.0083\n",
      "\n",
      "üìä VECTORIZER: Word2Vec\n",
      "-------------------------\n",
      "  üß† Word2Vec...\n",
      "  üéØ Logistic... F1: 0.6128 | CV: 0.5993¬±0.0061\n",
      "  üéØ RandomForest... F1: 0.6386 | CV: 0.6349¬±0.0039\n",
      "  üéØ XGBoost... F1: 0.6567 | CV: 0.6498¬±0.0105\n",
      "  üéØ LightGBM... F1: 0.6889 | CV: 0.6596¬±0.0020\n",
      "\n",
      "üìä VECTORIZER: FastText\n",
      "-------------------------\n",
      "  ‚ö° FastText...\n",
      "  üéØ Logistic... F1: 0.5683 | CV: 0.5607¬±0.0085\n",
      "  üéØ RandomForest... F1: 0.6323 | CV: 0.6368¬±0.0126\n",
      "  üéØ XGBoost... F1: 0.6874 | CV: 0.6543¬±0.0118\n",
      "  üéØ LightGBM... F1: 0.6834 | CV: 0.6627¬±0.0094\n",
      "\n",
      "üèÜ TOP 10 COMBINAISONS\n",
      "==================================================\n",
      "            Combination  F1_Test  Accuracy_Test  F1_CV_Mean\n",
      "      TF-IDF + Logistic 0.753036       0.799737    0.727661\n",
      "      TF-IDF + LightGBM 0.733061       0.785292    0.698744\n",
      "       TF-IDF + XGBoost 0.722130       0.780696    0.700260\n",
      "  TF-IDF + RandomForest 0.695499       0.773473    0.675841\n",
      "    Word2Vec + LightGBM 0.688871       0.748523    0.659589\n",
      "     FastText + XGBoost 0.687403       0.736047    0.654262\n",
      "    FastText + LightGBM 0.683401       0.743270    0.662724\n",
      "     Word2Vec + XGBoost 0.656669       0.714380    0.649792\n",
      "Word2Vec + RandomForest 0.638614       0.712410    0.634936\n",
      "FastText + RandomForest 0.632280       0.709783    0.636828\n",
      "\n",
      "ü•á MEILLEURE COMBINAISON:\n",
      "   TF-IDF + Logistic\n",
      "   F1 Test: 0.7530\n",
      "   Accuracy: 0.7997\n",
      "   F1 CV: 0.7277 ¬± 0.0058\n",
      "\n",
      "üìà ANALYSE PAR VECTORIZER:\n",
      "           F1_Test                 F1_CV_Mean\n",
      "              mean     max     std       mean\n",
      "Vectorizer                                   \n",
      "FastText    0.6428  0.6874  0.0557     0.6286\n",
      "TF-IDF      0.7259  0.7530  0.0240     0.7006\n",
      "Word2Vec    0.6492  0.6889  0.0320     0.6359\n",
      "\n",
      "ü§ñ ANALYSE PAR MOD√àLE:\n",
      "             F1_Test                 F1_CV_Mean\n",
      "                mean     max     std       mean\n",
      "Model                                          \n",
      "LightGBM      0.7018  0.7331  0.0272     0.6737\n",
      "Logistic      0.6447  0.7530  0.0964     0.6292\n",
      "RandomForest  0.6555  0.6955  0.0348     0.6492\n",
      "XGBoost       0.6887  0.7221  0.0328     0.6681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   Vectorizer         Model   F1_Test  Accuracy_Test  F1_CV_Mean  F1_CV_Std  \\\n",
       " 0      TF-IDF      Logistic  0.753036       0.799737    0.727661   0.005801   \n",
       " 3      TF-IDF      LightGBM  0.733061       0.785292    0.698744   0.008280   \n",
       " 2      TF-IDF       XGBoost  0.722130       0.780696    0.700260   0.008590   \n",
       " 1      TF-IDF  RandomForest  0.695499       0.773473    0.675841   0.009174   \n",
       " 7    Word2Vec      LightGBM  0.688871       0.748523    0.659589   0.001971   \n",
       " 10   FastText       XGBoost  0.687403       0.736047    0.654262   0.011773   \n",
       " 11   FastText      LightGBM  0.683401       0.743270    0.662724   0.009449   \n",
       " 6    Word2Vec       XGBoost  0.656669       0.714380    0.649792   0.010472   \n",
       " 5    Word2Vec  RandomForest  0.638614       0.712410    0.634936   0.003852   \n",
       " 9    FastText  RandomForest  0.632280       0.709783    0.636828   0.012617   \n",
       " 4    Word2Vec      Logistic  0.612800       0.682206    0.599289   0.006132   \n",
       " 8    FastText      Logistic  0.568278       0.665791    0.560698   0.008471   \n",
       " \n",
       "                 Combination  \n",
       " 0         TF-IDF + Logistic  \n",
       " 3         TF-IDF + LightGBM  \n",
       " 2          TF-IDF + XGBoost  \n",
       " 1     TF-IDF + RandomForest  \n",
       " 7       Word2Vec + LightGBM  \n",
       " 10       FastText + XGBoost  \n",
       " 11      FastText + LightGBM  \n",
       " 6        Word2Vec + XGBoost  \n",
       " 5   Word2Vec + RandomForest  \n",
       " 9   FastText + RandomForest  \n",
       " 4       Word2Vec + Logistic  \n",
       " 8       FastText + Logistic  ,\n",
       " [{'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'Logistic',\n",
       "   'F1_Test': 0.7530364372469636,\n",
       "   'Accuracy_Test': 0.7997373604727511,\n",
       "   'F1_CV_Mean': 0.7276606237936297,\n",
       "   'F1_CV_Std': 0.005801002031715946,\n",
       "   'Combination': 'TF-IDF + Logistic'},\n",
       "  {'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'RandomForest',\n",
       "   'F1_Test': 0.6954986760812003,\n",
       "   'Accuracy_Test': 0.7734734077478661,\n",
       "   'F1_CV_Mean': 0.6758405939711695,\n",
       "   'F1_CV_Std': 0.009173514875022739,\n",
       "   'Combination': 'TF-IDF + RandomForest'},\n",
       "  {'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'XGBoost',\n",
       "   'F1_Test': 0.7221297836938436,\n",
       "   'Accuracy_Test': 0.7806959947472094,\n",
       "   'F1_CV_Mean': 0.7002604639506317,\n",
       "   'F1_CV_Std': 0.008590499205535725,\n",
       "   'Combination': 'TF-IDF + XGBoost'},\n",
       "  {'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'LightGBM',\n",
       "   'F1_Test': 0.7330612244897959,\n",
       "   'Accuracy_Test': 0.7852921864740644,\n",
       "   'F1_CV_Mean': 0.6987440000633877,\n",
       "   'F1_CV_Std': 0.008279563091651485,\n",
       "   'Combination': 'TF-IDF + LightGBM'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'Logistic',\n",
       "   'F1_Test': 0.6128,\n",
       "   'Accuracy_Test': 0.6822061720288903,\n",
       "   'F1_CV_Mean': 0.5992892813624447,\n",
       "   'F1_CV_Std': 0.006131717155633185,\n",
       "   'Combination': 'Word2Vec + Logistic'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'RandomForest',\n",
       "   'F1_Test': 0.6386138613861386,\n",
       "   'Accuracy_Test': 0.7124097176625082,\n",
       "   'F1_CV_Mean': 0.6349362567853308,\n",
       "   'F1_CV_Std': 0.0038520808575988113,\n",
       "   'Combination': 'Word2Vec + RandomForest'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'XGBoost',\n",
       "   'F1_Test': 0.6566692975532754,\n",
       "   'Accuracy_Test': 0.7143795141168746,\n",
       "   'F1_CV_Mean': 0.6497920050055791,\n",
       "   'F1_CV_Std': 0.010472076124771369,\n",
       "   'Combination': 'Word2Vec + XGBoost'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'LightGBM',\n",
       "   'F1_Test': 0.6888708367181153,\n",
       "   'Accuracy_Test': 0.7485226526592252,\n",
       "   'F1_CV_Mean': 0.6595892935854281,\n",
       "   'F1_CV_Std': 0.0019709028305658796,\n",
       "   'Combination': 'Word2Vec + LightGBM'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'Logistic',\n",
       "   'F1_Test': 0.5682782018659881,\n",
       "   'Accuracy_Test': 0.6657912015758372,\n",
       "   'F1_CV_Mean': 0.5606983547424431,\n",
       "   'F1_CV_Std': 0.008470814308984238,\n",
       "   'Combination': 'FastText + Logistic'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'RandomForest',\n",
       "   'F1_Test': 0.632279534109817,\n",
       "   'Accuracy_Test': 0.7097833223900197,\n",
       "   'F1_CV_Mean': 0.6368282487813725,\n",
       "   'F1_CV_Std': 0.012616721634197319,\n",
       "   'Combination': 'FastText + RandomForest'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'XGBoost',\n",
       "   'F1_Test': 0.687402799377916,\n",
       "   'Accuracy_Test': 0.7360472751149048,\n",
       "   'F1_CV_Mean': 0.6542622392926707,\n",
       "   'F1_CV_Std': 0.01177294641979108,\n",
       "   'Combination': 'FastText + XGBoost'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'LightGBM',\n",
       "   'F1_Test': 0.6834008097165992,\n",
       "   'Accuracy_Test': 0.7432698621142482,\n",
       "   'F1_CV_Mean': 0.6627238002203779,\n",
       "   'F1_CV_Std': 0.009448901779433827,\n",
       "   'Combination': 'FastText + LightGBM'}])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quick_start(df, target_col='target'):\n",
    "    \"\"\"D√©marrage rapide - teste tout automatiquement\"\"\"\n",
    "    \n",
    "    print(\"‚ö° QUICK START - Test complet automatique\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Split des donn√©es\n",
    "    X = df[['text', 'location', 'keyword']]\n",
    "    y = df[target_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Preprocessing\n",
    "    X_train_processed, _, _ = preprocess_data(X_train)\n",
    "    X_test_processed, _, _ = preprocess_data(X_test)\n",
    "    \n",
    "    # Test toutes les combinaisons\n",
    "    results = test_all_combinations(X_train_processed, X_test_processed, y_train, y_test)\n",
    "    \n",
    "    # Analyse\n",
    "    df_results = analyze_results(results)\n",
    "    \n",
    "    return df_results, results \n",
    "\n",
    "quick_start(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0149b-accd-4144-b485-f8b5d0050b5d",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf730cb1",
   "metadata": {},
   "source": [
    "#### Pr√©traitement sur les donn√©es (Split donn√©es + TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f3ccf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Preprocessing complet...\n",
      "üîß Preprocessing complet...\n",
      "  üìù TF-IDF...\n"
     ]
    }
   ],
   "source": [
    "# √âtape 1 : split des colonnes utiles\n",
    "X = data[['text', 'location', 'keyword']]\n",
    "y = data['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# √âtape 2 : preprocessing (d√©j√† d√©fini dans ton code)\n",
    "X_train_processed, _, _ = preprocess_data(X_train_raw)\n",
    "X_test_processed, _, _ = preprocess_data(X_test_raw)\n",
    "\n",
    "# √âtape 3 : vectorisation TF-IDF (d√©j√† d√©finie aussi)\n",
    "X_train_vec, X_test_vec, tfidf = get_tfidf_features(X_train_processed, X_test_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df35727",
   "metadata": {},
   "source": [
    "#### Optimisation : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "290b332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params LR: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best F1 CV LR: 0.7389304268413587\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 2, 3],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    param_grid,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_lr.fit(X_train_vec, y_train)\n",
    "print(\"Best params LR:\", grid_lr.best_params_)\n",
    "print(\"Best F1 CV LR:\", grid_lr.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076888e8",
   "metadata": {},
   "source": [
    "#### Optimisation : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params XGB: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 800}\n",
      "Best F1 CV XGB: 0.715595017305107\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [700, 800, 600],\n",
    "    'max_depth': [7, 5, 6],\n",
    "    'learning_rate': [0.5, 0.3, 0.2]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    param_grid_xgb,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_xgb.fit(X_train_vec, y_train)\n",
    "print(\"Best params XGB:\", grid_xgb.best_params_)\n",
    "print(\"Best F1 CV XGB:\", grid_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6258c1b",
   "metadata": {},
   "source": [
    "#### Optimisation : LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5eed4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params LGBM: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}\n",
      "Best F1 CV LGBM: 0.6870690104510798\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "param_grid_lgb = {\n",
    "    'n_estimators': [600, 400, 200],\n",
    "    'max_depth': [-1, -3, 0],\n",
    "    'learning_rate': [0.05, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "grid_lgb = GridSearchCV(\n",
    "    lgb.LGBMClassifier(random_state=42),\n",
    "    param_grid_lgb,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_lgb.fit(X_train_vec, y_train)\n",
    "print(\"Best params LGBM:\", grid_lgb.best_params_)\n",
    "print(\"Best F1 CV LGBM:\", grid_lgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e7c45",
   "metadata": {},
   "source": [
    "#### √âvaluation sur test set du meilleur mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5743b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 test: 0.75642965204236\n",
      "Accuracy test: 0.788575180564675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.81       869\n",
      "           1       0.75      0.76      0.76       654\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.78      0.79      0.78      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "best_model = grid_lr.best_estimator_\n",
    "y_pred = best_model.predict(X_test_vec)\n",
    "\n",
    "print(\"F1 test:\", f1_score(y_test, y_pred))\n",
    "print(\"Accuracy test:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b77837-3e2c-48ab-9b8c-477319114c1d",
   "metadata": {},
   "source": [
    "## Data profiling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32f3c9bb-e15f-4c6b-9bdb-eb44186a864e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# https://github.com/ydataai/ydata-profiling: g√©n√©rer le rapport des donn√©es avec ProfileReport\n",
    "# generate report\n",
    "profile = ProfileReport(data, title=\"House price - Report\")\n",
    "\n",
    "# save report to html\n",
    "profile.to_file(Path(REPORTS_DIR, \"house_price_profiling.html\"))\n",
    "\n",
    "# Display the report in the notebook\n",
    "# profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1834a8-4ff6-4ac7-bf91-1b55df9727fa",
   "metadata": {},
   "source": [
    "## Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa3d3a-08c3-4a4a-9181-9f3311ab72a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:30.433303Z",
     "iopub.status.busy": "2024-06-29T09:03:30.433303Z",
     "iopub.status.idle": "2024-06-29T09:03:30.665064Z",
     "shell.execute_reply": "2024-06-29T09:03:30.663564Z",
     "shell.execute_reply.started": "2024-06-29T09:03:30.433303Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c7e3f-7d53-4574-bdf3-73ac9831642e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:30.667064Z",
     "iopub.status.busy": "2024-06-29T09:03:30.666065Z",
     "iopub.status.idle": "2024-06-29T09:03:31.924565Z",
     "shell.execute_reply": "2024-06-29T09:03:31.923565Z",
     "shell.execute_reply.started": "2024-06-29T09:03:30.667064Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictive Power Score (PPS) : https://github.com/8080labs/ppscore/\n",
    "pps_predictors = pps.predictors(df=data.drop([\"Id\", \"YrSold\", \"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\"], axis=1),\n",
    "                                y=TARGET_NAME, output=\"df\", random_seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf8c23-ecfd-4902-a613-442bf4b304ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:31.926813Z",
     "iopub.status.busy": "2024-06-29T09:03:31.925566Z",
     "iopub.status.idle": "2024-06-29T09:03:32.207505Z",
     "shell.execute_reply": "2024-06-29T09:03:32.206333Z",
     "shell.execute_reply.started": "2024-06-29T09:03:31.926316Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pps_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56574238-6d8d-4a3a-a272-938d1a462343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:32.209158Z",
     "iopub.status.busy": "2024-06-29T09:03:32.209158Z",
     "iopub.status.idle": "2024-06-29T09:03:32.442537Z",
     "shell.execute_reply": "2024-06-29T09:03:32.442537Z",
     "shell.execute_reply.started": "2024-06-29T09:03:32.209158Z"
    }
   },
   "outputs": [],
   "source": [
    "pps.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ecc0cc-a38f-44d9-b5dd-dd490961d472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:32.443537Z",
     "iopub.status.busy": "2024-06-29T09:03:32.443537Z",
     "iopub.status.idle": "2024-06-29T09:03:32.678555Z",
     "shell.execute_reply": "2024-06-29T09:03:32.677447Z",
     "shell.execute_reply.started": "2024-06-29T09:03:32.443537Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if there are invalide pps scores in the output\n",
    "pps_predictors.is_valid_score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59092c72-7e55-4c72-8e61-6579b2b48d66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:32.679553Z",
     "iopub.status.busy": "2024-06-29T09:03:32.679553Z",
     "iopub.status.idle": "2024-06-29T09:03:32.943111Z",
     "shell.execute_reply": "2024-06-29T09:03:32.942115Z",
     "shell.execute_reply.started": "2024-06-29T09:03:32.679553Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get feature names\n",
    "FEATURE_NAMES = pps_predictors.loc[pps_predictors.ppscore >= MODEL_PARAMS[\"MIN_PPS\"], \"x\"].values\n",
    "set(FEATURE_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c5598-42bc-4d5f-9928-0fae68c34430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T09:43:54.012805Z",
     "iopub.status.busy": "2024-06-28T09:43:54.012805Z",
     "iopub.status.idle": "2024-06-28T09:43:54.523844Z",
     "shell.execute_reply": "2024-06-28T09:43:54.522757Z",
     "shell.execute_reply.started": "2024-06-28T09:43:54.012805Z"
    }
   },
   "source": [
    "__Data leakage__\n",
    "\n",
    "Attention √† la fuite des donn√©es.\n",
    "\n",
    "Des variables importantes alors qu'elles ne seront pas disponibles lors de la pr√©diction (exemple: SaleCondition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a61b0-91bd-42d0-b195-bbe0323cf57b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:32.944111Z",
     "iopub.status.busy": "2024-06-29T09:03:32.944111Z",
     "iopub.status.idle": "2024-06-29T09:03:33.802041Z",
     "shell.execute_reply": "2024-06-29T09:03:33.801041Z",
     "shell.execute_reply.started": "2024-06-29T09:03:32.944111Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.barplot(data=pps_predictors.loc[lambda dfr: dfr.ppscore > 0], y=\"x\", x=\"ppscore\", orient=\"h\")\n",
    "ax.set_title(F\"Predictive Power Score (PPS) for {TARGET_NAME}\")\n",
    "\n",
    "# add the annotation\n",
    "ax.bar_label(ax.containers[-1], fmt='%.3f', label_type='edge');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb9d01-a4ab-4480-b5f9-9c1c3bd0d3dd",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3ec99-a858-4fe7-bbbd-ad288df83f29",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c48b7-bc4d-418e-87ff-2b4fe336f1ab",
   "metadata": {},
   "source": [
    "![mlflow-tracking](https://mlflow.org/docs/latest/_images/quickstart_tracking_overview.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741737c-7f25-41f0-9c4a-ba8045d2a24a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:33.805043Z",
     "iopub.status.busy": "2024-06-29T09:03:33.804106Z",
     "iopub.status.idle": "2024-06-29T09:03:33.988965Z",
     "shell.execute_reply": "2024-06-29T09:03:33.988965Z",
     "shell.execute_reply.started": "2024-06-29T09:03:33.805043Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Union, Dict, Any\n",
    "\n",
    "\n",
    "def eval_metrics(y_actual: Union[pd.DataFrame, pd.Series, np.ndarray],\n",
    "                 y_pred: Union[pd.DataFrame, pd.Series, np.ndarray]\n",
    "                 ) -> Dict[str, float]:\n",
    "    \"\"\"Compute evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        y_actual: Ground truth (correct) target values\n",
    "        y_pred: Estimated target values.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: dictionary of evaluation metrics.\n",
    "            Expected keys are: \"rmse\", \"mae\", \"mape\", \"r2\", \"max_error\"\n",
    "\n",
    "    \"\"\"\n",
    "    # Calculate Root mean squared error, named rmse\n",
    "    rmse = root_mean_squared_error(y_actual, y_pred)\n",
    "    # Calculate mean absolute error, named mae\n",
    "    mae = mean_absolute_error(y_actual, y_pred)\n",
    "    # Mean absolute percentage error (MAPE)\n",
    "    mape = mean_absolute_percentage_error(y_actual, y_pred)\n",
    "    # Calculate R-squared: coefficient of determination, named r2\n",
    "    r2 = r2_score(y_actual, y_pred)\n",
    "    # Calculate max error: maximum value of absolute error (y_actual - y_pred), named maxerror\n",
    "    maxerror = max_error(y_actual, y_pred)\n",
    "    return {\"rmse\": rmse,\n",
    "            \"mae\": mae,\n",
    "            \"mape\": mape,\n",
    "            \"r2\": r2,\n",
    "            \"max_error\": maxerror\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999fb2c4-4d5c-4083-a31b-f850232e418b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:33.993959Z",
     "iopub.status.busy": "2024-06-29T09:03:33.993959Z",
     "iopub.status.idle": "2024-06-29T09:03:34.206410Z",
     "shell.execute_reply": "2024-06-29T09:03:34.205911Z",
     "shell.execute_reply.started": "2024-06-29T09:03:33.993959Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_pipeline(numerical_transformer: list,\n",
    "                    categorical_transformer: list,\n",
    "                    estimator: Pipeline,\n",
    "                    target_transformer: bool=False,\n",
    "                    **kwargs: dict) -> Pipeline:\n",
    "    \"\"\"Define pipeline for modeling.\n",
    "\n",
    "    Args:\n",
    "        numerical_transformer:\n",
    "        categorical_transformer:\n",
    "        target_transformer:\n",
    "        estimator:\n",
    "        kwargs:\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: sklearn pipeline\n",
    "    \"\"\"\n",
    "    numerical_transformer = make_pipeline(*numerical_transformer)\n",
    "\n",
    "    categorical_transformer = make_pipeline(*categorical_transformer)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numerical_transformer, make_column_selector(dtype_include=[\"number\"])),\n",
    "            (\"cat\", categorical_transformer, make_column_selector(dtype_include=[\"object\", \"bool\"])),\n",
    "        ],\n",
    "        remainder=\"drop\",  # non-specified columns are dropped\n",
    "        verbose_feature_names_out=False,  # will not prefix any feature names with the name of the transformer\n",
    "    )\n",
    "    # Append regressor to preprocessing pipeline.\n",
    "    # Now we have a full prediction pipeline.\n",
    "    if target_transformer:\n",
    "        model_pipe1 = Pipeline(steps=[(\"preprocessor\", preprocessor),\n",
    "                                      (\"estimator\", estimator)])\n",
    "        model_pipe = TransformedTargetRegressor(regressor=model_pipe1,\n",
    "                                                func=np.log,\n",
    "                                                inverse_func=np.exp)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        model_pipe = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"estimator\", estimator)])\n",
    "        \n",
    "    # logger.info(f\"{model_pipe}\")\n",
    "    return model_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7cc41-0b64-4e06-9346-18d91804750a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:49:21.612818Z",
     "iopub.status.busy": "2024-06-28T13:49:21.612818Z",
     "iopub.status.idle": "2024-06-28T13:49:22.167229Z",
     "shell.execute_reply": "2024-06-28T13:49:22.167229Z",
     "shell.execute_reply.started": "2024-06-28T13:49:21.612818Z"
    }
   },
   "source": [
    "## Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91463e1e-d7ab-4ef7-9a85-6c0f70bf2fb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:34.207418Z",
     "iopub.status.busy": "2024-06-29T09:03:34.207418Z",
     "iopub.status.idle": "2024-06-29T09:03:34.411893Z",
     "shell.execute_reply": "2024-06-29T09:03:34.410227Z",
     "shell.execute_reply.started": "2024-06-29T09:03:34.207418Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S√©parer les donn√©es en train et test (25%)\n",
    "\n",
    "FEATURES = FEATURE_NAMES if any(FEATURE_NAMES) else MODEL_PARAMS[\"DEFAULT_FEATURE_NAMES\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.loc[:, FEATURES],\n",
    "                                                    data[TARGET_NAME],\n",
    "                                                    test_size=MODEL_PARAMS[\"TEST_SIZE\"],\n",
    "                                                    random_state=SEED\n",
    "                                                   )\n",
    "\n",
    "logger.info(f\"\\nX train: {x_train.shape}\\nY train: {y_train.shape}\\n\"\n",
    "            f\"X test: {x_test.shape}\\nY test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7dc726-5f4d-4946-a18c-c2863a3c9b31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf8583-552c-45e8-ab42-577a26d16d4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:34.413622Z",
     "iopub.status.busy": "2024-06-29T09:03:34.412615Z",
     "iopub.status.idle": "2024-06-29T09:03:34.740259Z",
     "shell.execute_reply": "2024-06-29T09:03:34.738747Z",
     "shell.execute_reply.started": "2024-06-29T09:03:34.413622Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "reg = define_pipeline(numerical_transformer=[SimpleImputer(strategy=\"median\"),\n",
    "                                             RobustScaler()],\n",
    "                      categorical_transformer=[SimpleImputer(strategy=\"constant\", fill_value=\"undefined\"),\n",
    "                                               OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\")],\n",
    "                      target_transformer=False,\n",
    "                      estimator=RandomForestRegressor(n_estimators=30)\n",
    "                 )\n",
    "\n",
    "reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ce9dc-c14b-4a77-86d6-b8214079f356",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4263c-0d38-4476-850b-f22d51ddbd44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:34.742259Z",
     "iopub.status.busy": "2024-06-29T09:03:34.741260Z",
     "iopub.status.idle": "2024-06-29T09:03:35.849072Z",
     "shell.execute_reply": "2024-06-29T09:03:35.847573Z",
     "shell.execute_reply.started": "2024-06-29T09:03:34.742259Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model training and selection\n",
    "\n",
    "reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c96099-82a7-4cff-bb0f-fea189e53c66",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc722d2-0daa-47c3-8b0b-073ebe87f0d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:35.851572Z",
     "iopub.status.busy": "2024-06-29T09:03:35.851073Z",
     "iopub.status.idle": "2024-06-29T09:03:36.085631Z",
     "shell.execute_reply": "2024-06-29T09:03:36.084635Z",
     "shell.execute_reply.started": "2024-06-29T09:03:35.851572Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate trained model: sur le train et le test set\n",
    "\n",
    "# Calcule the evaluation metrics\n",
    "y_train_pred = reg.predict(x_train)\n",
    "y_test_pred = reg.predict(x_test)\n",
    "train_metrics = eval_metrics(y_train, y_train_pred)\n",
    "test_metrics = eval_metrics(y_test, y_test_pred)\n",
    "\n",
    "# log out metrics\n",
    "logger.info(f\"\"\"Performances\\n{pd.DataFrame({\"train\": train_metrics, \"test\": test_metrics}).T}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf2a99-ccc0-4a28-af31-da6e540fe712",
   "metadata": {},
   "source": [
    "#### Residuals analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeee113-3b2e-40de-9944-0e4bdca09b3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:36.086631Z",
     "iopub.status.busy": "2024-06-29T09:03:36.086631Z",
     "iopub.status.idle": "2024-06-29T09:03:36.958024Z",
     "shell.execute_reply": "2024-06-29T09:03:36.958024Z",
     "shell.execute_reply.started": "2024-06-29T09:03:36.086631Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(reg, is_fitted=\"auto\")\n",
    "\n",
    "visualizer.fit(x_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(x_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show();                # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f7409-c714-4427-8728-2ab836e15aaa",
   "metadata": {},
   "source": [
    "#### Prediction plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae95eeb-f7df-44b0-9af2-c298c8772ff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:36.960061Z",
     "iopub.status.busy": "2024-06-29T09:03:36.958979Z",
     "iopub.status.idle": "2024-06-29T09:03:37.395928Z",
     "shell.execute_reply": "2024-06-29T09:03:37.394929Z",
     "shell.execute_reply.started": "2024-06-29T09:03:36.960061Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualizer = PredictionError(reg, is_fitted=\"auto\", bestfit=True, identity=True)\n",
    "\n",
    "visualizer.fit(x_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(x_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show();                # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79824f-4813-4855-9945-2b0e7e5462f5",
   "metadata": {},
   "source": [
    "### Tester l'un des packages suivants:\n",
    "\n",
    "__Exo 1__:\n",
    "\n",
    "1- [PyCaret](https://pycaret.org/): An open source, low-code machine learning library in Python.\n",
    "\n",
    "2- [LazyPredict](https://pypi.org/project/lazypredict/): Lazy Predict help build a lot of basic models without much code and helps understand which models works better without any parameter tuning\n",
    "\n",
    "\n",
    "__Exo 2__:\n",
    "\n",
    "1- Tester la pr√©diction avec la variable logarithmique\n",
    "\n",
    "\n",
    "__Exo3__: \n",
    "1- Consulter la document de mlflow via les liens pr√©cis√©s au d√©but du notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ae6d7-ef85-4619-a521-8a797e55f1ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:37.397935Z",
     "iopub.status.busy": "2024-06-29T09:03:37.396935Z",
     "iopub.status.idle": "2024-06-29T09:03:40.458415Z",
     "shell.execute_reply": "2024-06-29T09:03:40.455882Z",
     "shell.execute_reply.started": "2024-06-29T09:03:37.397935Z"
    }
   },
   "source": [
    "import pycaret.regression as pyr\n",
    "from lazypredict.Supervised import LazyRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94c440b-b6ca-429c-8d77-168787cc495f",
   "metadata": {},
   "source": [
    "### Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756326b-bbf2-4f3a-9f40-b9cad9b1126b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:50:22.393907Z",
     "iopub.status.busy": "2024-06-29T09:50:22.392902Z",
     "iopub.status.idle": "2024-06-29T09:50:22.809396Z",
     "shell.execute_reply": "2024-06-29T09:50:22.808570Z",
     "shell.execute_reply.started": "2024-06-29T09:50:22.393907Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an experiment if not exists\n",
    "exp_name = \"house-price\"\n",
    "experiment = mlflow.get_experiment_by_name(exp_name)\n",
    "if not experiment:\n",
    "    experiment_id = mlflow.create_experiment(exp_name)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "    \n",
    "logger.info(f\"Experience id: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0548df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "for e in mlflow.search_experiments():\n",
    "    print(f\"{e.experiment_id} - {e.name} - {e.artifact_location}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27b9889c-d2fc-4914-b6cd-2c225871c01d",
   "metadata": {},
   "source": [
    "# single model\n",
    "# Useful for multiple runs (only doing one run in this sample notebook)\n",
    "with mlflow.start_run(run_name=f\"{EXECUTION_DATE.strftime('%Y%m%d_%H%m%S')}-house_price\",\n",
    "                      experiment_id=experiment_id,\n",
    "                      tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
    "                      description=\"house price modeling\",\n",
    "                     ) as mlf_run:\n",
    "    print(f\"run_id: {mlf_run.info.run_id}\")\n",
    "    print(f\"version tag value: {mlf_run.data.tags.get('version')}\")\n",
    "    print(\"--\")\n",
    "\n",
    "    # Select number of estimator\n",
    "    n_estimators = 10  # int(input(\"Estimator(s): \"))\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    # Model definition\n",
    "    reg = define_pipeline(numerical_transformer=[SimpleImputer(strategy=\"median\"),\n",
    "                                                 RobustScaler()],\n",
    "                          categorical_transformer=[SimpleImputer(strategy=\"constant\", fill_value=\"undefined\"),\n",
    "                                                   OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\")],\n",
    "                          target_transformer=False,\n",
    "                          estimator=RandomForestClassifier(n_estimators=n_estimators)\n",
    "                     )\n",
    "\n",
    "    reg.fit(x_train, y_train)\n",
    "\n",
    "    # Evaluate Metrics\n",
    "    y_train_pred = reg.predict(x_train)\n",
    "    y_test_pred = reg.predict(x_test)\n",
    "    train_metrics = eval_metrics(y_train, y_train_pred)\n",
    "    test_metrics = eval_metrics(y_test, y_test_pred)\n",
    "\n",
    "    # log out metrics\n",
    "    logger.info(f\"Train: {train_metrics}\")\n",
    "    logger.info(f\"Test: {test_metrics}\")\n",
    "    \n",
    "    # Infer model signature\n",
    "    predictions = reg.predict(x_train)\n",
    "    signature = infer_signature(x_train, predictions)\n",
    "\n",
    "    # Log parameter, metrics, and model to MLflow\n",
    "    for group_name, set_metrics in [(\"train\", train_metrics),\n",
    "                                    (\"test\", test_metrics),\n",
    "                                   ]:\n",
    "        for metric_name, metric_value in set_metrics.items():\n",
    "            mlflow.log_metric(f\"{group_name}_{metric_name}\", metric_value)\n",
    "    mlflow.sklearn.log_model(reg,\n",
    "                             artifact_path=reg[-1].__class__.__name__,\n",
    "                             signature=signature,\n",
    "                             input_example=x_train[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56cfa6-a78b-40a3-871c-c08d0aad7002",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:59:18.914000Z",
     "iopub.status.busy": "2024-06-29T09:59:18.913001Z",
     "iopub.status.idle": "2024-06-29T09:59:19.347053Z",
     "shell.execute_reply": "2024-06-29T09:59:19.346058Z",
     "shell.execute_reply.started": "2024-06-29T09:59:18.914000Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define models and parameters to benchmark\n",
    "ESTIMATOR_PARAMS = {DummyRegressor.__name__: {\"estimator\": DummyRegressor,\n",
    "                                              \"params\": {\"strategy\": \"median\"}\n",
    "                                             },\n",
    "                    RandomForestRegressor.__name__: {\"estimator\": RandomForestRegressor,\n",
    "                                                     \"params\": {\"n_estimators\": 30,\n",
    "                                                                \"max_depth\": 3,\n",
    "                                                                \"random_state\": SEED\n",
    "                                                               }\n",
    "                                             },\n",
    "                    GradientBoostingRegressor.__name__: {\"estimator\": GradientBoostingRegressor,\n",
    "                                                         \"params\": {\"n_estimators\": 30,\n",
    "                                                                    \"learning_rate\": 0.01,\n",
    "                                                                    \"max_depth\": 3,\n",
    "                                                                    \"random_state\": SEED\n",
    "                                                                   }\n",
    "                                                        }\n",
    "}\n",
    "\n",
    "ESTIMATOR_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d178497-ed38-49bc-b6c7-38602dae8e87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:59:29.590801Z",
     "iopub.status.busy": "2024-06-29T09:59:29.589801Z",
     "iopub.status.idle": "2024-06-29T09:59:40.078236Z",
     "shell.execute_reply": "2024-06-29T09:59:40.078236Z",
     "shell.execute_reply.started": "2024-06-29T09:59:29.590801Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name, model_configs in ESTIMATOR_PARAMS.items():\n",
    "    logger.info(f\"{model_name} \\n{model_configs}\")\n",
    "    \n",
    "    estimator = model_configs[\"estimator\"]\n",
    "    params = model_configs[\"params\"]\n",
    "    \n",
    "    # Useful for multiple runs (only doing one run in this sample notebook)\n",
    "    with mlflow.start_run(run_name=f\"{CURRENT_DATE.strftime('%Y%m%d_%H%m%S')}-house_price-{model_name}\",\n",
    "                          experiment_id=experiment_id,\n",
    "                          tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
    "                          description=\"house price modeling\",\n",
    "                         ) as mlf_run:\n",
    "        logger.info(f\"run_id: {mlf_run.info.run_id}\")\n",
    "        logger.info(f\"version tag value: {mlf_run.data.tags.get('version')} -------------------------------\")\n",
    "\n",
    "        # log parameters\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        \n",
    "        # Model definition\n",
    "        reg = define_pipeline(numerical_transformer=[SimpleImputer(strategy=\"median\"),\n",
    "                                                     RobustScaler()],\n",
    "                              categorical_transformer=[SimpleImputer(strategy=\"constant\", fill_value=\"undefined\"),\n",
    "                                                       OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\")],\n",
    "                              target_transformer=False,\n",
    "                              estimator=estimator(**params)\n",
    "                         )\n",
    "\n",
    "        reg.fit(x_train, y_train)\n",
    "\n",
    "        # Evaluate Metrics\n",
    "        y_train_pred = reg.predict(x_train)\n",
    "        y_test_pred = reg.predict(x_test)\n",
    "        train_metrics = eval_metrics(y_train, y_train_pred)\n",
    "        test_metrics = eval_metrics(y_test, y_test_pred)\n",
    "        \n",
    "        # log out metrics\n",
    "        logger.info(f\"Train: {train_metrics}\")\n",
    "        logger.info(f\"Test: {test_metrics}\")\n",
    "\n",
    "        # Infer model signature with a sample\n",
    "        predictions = reg.predict(x_train[:30])\n",
    "        signature = mlflow.models.infer_signature(x_train[:30], predictions)\n",
    "\n",
    "        # Log  metrics, and model to MLflow\n",
    "        mlflow.log_metrics(test_metrics)\n",
    "        mlflow.sklearn.log_model(reg,\n",
    "                                 artifact_path=reg[-1].__class__.__name__,\n",
    "                                 signature=signature,\n",
    "                                 input_example=x_train[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d1510-1899-48eb-b8c2-e2003151aefc",
   "metadata": {},
   "source": [
    "\n",
    "Tester les mod√®les suivants:\n",
    "    \n",
    "    - DummyRegressor\n",
    "    - Regression ridge\n",
    "    - Bosting ou autre mod√®le ensembliste\n",
    "    \n",
    "30 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc9b16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecd77309-deba-4104-89d0-1d02702a7f06",
   "metadata": {},
   "source": [
    "### Performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b3e9d-1438-43cf-962a-c53c7ccd2364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b6e82-5d54-4207-a339-e48182902431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b4dedac-d9fa-4115-968e-825fdd73ecc0",
   "metadata": {},
   "source": [
    "### Features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd327f-5d38-495c-bd70-9e2c2cdfc8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940d99e-3ca9-4e84-867a-9986174bef42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3febfc98-87c9-455f-8f33-f4f1f0293780",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a783eb4c-6485-495f-9069-d8ac2548316e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713431b-acbf-4af0-a743-34f1f7397187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab203dd8-0f9d-48e3-856e-38a0fb74852d",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba05791-5bd4-4c82-ab8a-35c078a3e6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5aafea-f72e-48c6-a82a-ec64bd936a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb78d159-74c1-42f2-8abc-8527c95a0ad5",
   "metadata": {},
   "source": [
    "## Session info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3cf074-9b48-4f51-9bb4-49a634f3c3c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:21:20.166776Z",
     "iopub.status.busy": "2024-06-29T09:21:20.166776Z",
     "iopub.status.idle": "2024-06-29T09:21:20.398608Z",
     "shell.execute_reply": "2024-06-29T09:21:20.397663Z",
     "shell.execute_reply.started": "2024-06-29T09:21:20.166776Z"
    }
   },
   "outputs": [],
   "source": [
    "import session_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96807e83-3988-411e-a201-f8fe161c5075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:21:47.889858Z",
     "iopub.status.busy": "2024-06-29T09:21:47.889858Z",
     "iopub.status.idle": "2024-06-29T09:21:48.095512Z",
     "shell.execute_reply": "2024-06-29T09:21:48.094516Z",
     "shell.execute_reply.started": "2024-06-29T09:21:47.889858Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "session_info.show(dependencies=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb725205-9e32-42ed-8a1c-0dda168398e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
