{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a46012-b136-43a9-ad81-76ac62a83136",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Pour la suite du cours, nous allons faire l'apprentissage par la pratique avec un cas réel de projet de Machine Learning\n",
    "\n",
    "Description du projet: Prédire le prix des biens immobiliers à Ames, Iowa.\n",
    "\n",
    "Les données sont disponibles dans: [sklearn.datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml)\n",
    "\n",
    "\n",
    "Voici un récapitulatif des objectifs :\n",
    "- Réaliser une analyse exploratoire.\n",
    "- Tester différents modèles de prédiction afin de répondre au mieux à la problématique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f714cf0-f5bc-491c-80e4-38472f4d409d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T12:28:27.081285Z",
     "iopub.status.busy": "2024-06-28T12:28:27.081285Z",
     "iopub.status.idle": "2024-06-28T12:28:27.098789Z",
     "shell.execute_reply": "2024-06-28T12:28:27.097789Z",
     "shell.execute_reply.started": "2024-06-28T12:28:27.081285Z"
    }
   },
   "source": [
    "__Sources utiles__\n",
    "\n",
    "- [Introduction à MLOps](https://ashutoshtripathi.com/2021/08/18/mlops-a-complete-guide-to-machine-learning-operations-mlops-vs-devops/)\n",
    "\n",
    "- [MLFLOW - Site de référence](https://mlflow.org/docs/latest/index.html)\n",
    "- [MLFLOW - Tutorial](https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html)\n",
    "- [MLFLOW - Tracking](https://mlflow.org/docs/latest/tracking.html)\n",
    "- [MLFLOW - Model Registry](https://mlflow.org/docs/latest/model-registry.html#)\n",
    "- [MLFLOW - Serve a model](https://mlflow.org/docs/latest/model-registry.html#serving-an-mlflow-model-from-model-registry)\n",
    "\n",
    "- [Evidently - tutorial d'analyse de Data drift](https://github.com/evidentlyai/evidently/tree/main/examples/sample_notebooks)\n",
    "- [API Flask - Démarche de mise en oeuvre](http://web.univ-ubs.fr/lmba/lardjane/python/c4.pdf)\n",
    "- [FastAPI - Démarche de mise en oeuvre](https://towardsdatascience.com/how-to-build-and-deploy-a-machine-learning-model-with-fastapi-64c505213857)\n",
    "- [Azure - Tuto déploiement application web ](https://learn.microsoft.com/fr-fr/azure/app-service/quickstart-python?tabs=flask%2Cwindows%2Cazure-portal%2Cvscode-deploy%2Cdeploy-instructions-azportal%2Cterminal-bash%2Cdeploy-instructions-zip-azcli)\n",
    "- [Tests unitaires - Unittest ou Pytest](https://www.sitepoint.com/python-unit-testing-unittest-pytest/)\n",
    "\n",
    "- [Pythonanywhere](https://www.pythonanywhere.com/)\n",
    "- [Heroku](https://www.heroku.com/)\n",
    "-[Azure webapp - Déploiement automatisé via Github](https://learn.microsoft.com/fr-fr/azure/app-service/deploy-continuous-deployment?tabs=github)\n",
    "- Streamlit ou gradio pour la mise en place d'un dashbord\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14612581-a3ef-4522-8ddd-64adf0221e56",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038e700c-aabe-4e62-9b37-d8ffa755cf32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T10:02:04.633195Z",
     "iopub.status.busy": "2024-06-29T10:02:04.633195Z",
     "iopub.status.idle": "2024-06-29T10:02:05.612841Z",
     "shell.execute_reply": "2024-06-29T10:02:05.611845Z",
     "shell.execute_reply.started": "2024-06-29T10:02:04.633195Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pendulum\n",
    "import plotly.express as px\n",
    "import ppscore as pps\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, TransformedTargetRegressor\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, VotingRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.metrics import (r2_score,\n",
    "                             root_mean_squared_error,\n",
    "                             mean_absolute_percentage_error,\n",
    "                             mean_absolute_error,\n",
    "                             max_error,\n",
    "                            )\n",
    "from sklearn.model_selection import train_test_split, learning_curve, LearningCurveDisplay\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from ydata_profiling import ProfileReport\n",
    "from yellowbrick.regressor import PredictionError, ResidualsPlot\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from settings.params import MODEL_PARAMS, SEED\n",
    "from src.make_dataset import load_data\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "set_config(display=\"diagram\", print_changed_only=False)  # display sklearn pipeline as diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d2c00-ebe7-409c-b936-a3a36a10a6b0",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c12ca6-6ee6-4a5b-bd46-b567a6c5c34b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:21.715720Z",
     "iopub.status.busy": "2024-06-29T09:03:21.714721Z",
     "iopub.status.idle": "2024-06-29T09:03:21.867996Z",
     "shell.execute_reply": "2024-06-29T09:03:21.867826Z",
     "shell.execute_reply.started": "2024-06-29T09:03:21.715720Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-30 21:23:58.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - Target name: target\n",
      "\u001b[32m2025-07-30 21:23:58.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \n",
      "Project directory: c:\\Users\\DELL\\Desktop\\mlops-project - Tweets \n",
      "Reports dir: c:\\Users\\DELL\\Desktop\\mlops-project - Tweets\\reports\n"
     ]
    }
   ],
   "source": [
    "# Set logging format\n",
    "log_fmt = \"<green>{time:YYYY-MM-DD HH:mm:ss.SSS!UTC}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - {message}\"\n",
    "logger.configure(handlers=[{\"sink\": sys.stderr, \"format\": log_fmt}])\n",
    "\n",
    "# current data\n",
    "CURRENT_DATE = pendulum.now(tz=\"UTC\")\n",
    "\n",
    "# target name definition\n",
    "TARGET_NAME = MODEL_PARAMS[\"TARGET_NAME\"]\n",
    "logger.info(f\"Target name: {TARGET_NAME}\")\n",
    "\n",
    "\n",
    "# directories\n",
    "PROJECT_DIR = Path.cwd().parent\n",
    "REPORTS_DIR = Path(PROJECT_DIR, \"reports\")\n",
    "\n",
    "logger.info(f\"\\nProject directory: {PROJECT_DIR} \\nReports dir: {REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9b5106-eb95-4b15-8655-836f5b458283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:21.869006Z",
     "iopub.status.busy": "2024-06-29T09:03:21.869006Z",
     "iopub.status.idle": "2024-06-29T09:03:22.009874Z",
     "shell.execute_reply": "2024-06-29T09:03:22.008866Z",
     "shell.execute_reply.started": "2024-06-29T09:03:21.869006Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TARGET_NAME': 'target',\n",
       " 'MIN_COMPLETION_RATE': 0.75,\n",
       " 'MIN_PPS': 0.1,\n",
       " 'DEFAULT_FEATURE_NAMES': ['Alley',\n",
       "  'BsmtQual',\n",
       "  'ExterQual',\n",
       "  'Foundation',\n",
       "  'FullBath',\n",
       "  'GarageArea',\n",
       "  'GarageCars',\n",
       "  'GarageFinish',\n",
       "  'GarageType',\n",
       "  'GrLivArea',\n",
       "  'KitchenQualMSSubClass',\n",
       "  'Neighborhood',\n",
       "  'OverallQual',\n",
       "  'TotRmsAbvGrd',\n",
       "  'building_age',\n",
       "  'remodel_age',\n",
       "  'garage_age'],\n",
       " 'TEST_SIZE': 0.25}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17884a5-99c8-452a-b3fb-7cf45931ca1b",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7288f4e1-0a82-4ab1-ad61-a4f80aeb4289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.011873Z",
     "iopub.status.busy": "2024-06-29T09:03:22.010873Z",
     "iopub.status.idle": "2024-06-29T09:03:22.199509Z",
     "shell.execute_reply": "2024-06-29T09:03:22.198509Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.011873Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data like pandas.DataFrame\n",
    "data = load_data(dataset_name=\"tweets\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071fe3c2-3617-43b6-aca2-045339e61256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.201512Z",
     "iopub.status.busy": "2024-06-29T09:03:22.200509Z",
     "iopub.status.idle": "2024-06-29T09:03:22.373482Z",
     "shell.execute_reply": "2024-06-29T09:03:22.372486Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.201512Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f0fb9-7a60-447c-81bd-3fa18149fa7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.374486Z",
     "iopub.status.busy": "2024-06-29T09:03:22.373482Z",
     "iopub.status.idle": "2024-06-29T09:03:22.514516Z",
     "shell.execute_reply": "2024-06-29T09:03:22.514516Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.374486Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c5b45-80f1-48d6-bad9-5d95640cb17d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.674645Z",
     "iopub.status.busy": "2024-06-29T09:03:22.674645Z",
     "iopub.status.idle": "2024-06-29T09:03:22.938731Z",
     "shell.execute_reply": "2024-06-29T09:03:22.937730Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.674645Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.describe(include=\"all\") #, datetime_is_numeric=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b46232-40eb-4568-be4f-9c9f2a8a0d4d",
   "metadata": {},
   "source": [
    "# EDA: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a178177-9ff1-4b6d-b967-b328cae9a06b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:22.940731Z",
     "iopub.status.busy": "2024-06-29T09:03:22.939726Z",
     "iopub.status.idle": "2024-06-29T09:03:24.434116Z",
     "shell.execute_reply": "2024-06-29T09:03:24.434116Z",
     "shell.execute_reply.started": "2024-06-29T09:03:22.940731Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# barplot for missing value rate\n",
    "msno.bar(data,\n",
    "         filter=\"top\",\n",
    "         p=MODEL_PARAMS[\"MIN_COMPLETION_RATE\"],\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1e6dc-d2a7-4750-902c-79516b698c3b",
   "metadata": {},
   "source": [
    "## Target analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb20318-d0eb-4dd6-94d2-a3c06bdcff13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:24.437117Z",
     "iopub.status.busy": "2024-06-29T09:03:24.436116Z",
     "iopub.status.idle": "2024-06-29T09:03:24.576283Z",
     "shell.execute_reply": "2024-06-29T09:03:24.576283Z",
     "shell.execute_reply.started": "2024-06-29T09:03:24.437117Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_PARAMS[\"TARGET_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08927768",
   "metadata": {},
   "source": [
    "### Distribution des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1d76b-ac16-4a60-a4ce-af3b8c225e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:24.578283Z",
     "iopub.status.busy": "2024-06-29T09:03:24.577279Z",
     "iopub.status.idle": "2024-06-29T09:03:24.733931Z",
     "shell.execute_reply": "2024-06-29T09:03:24.732887Z",
     "shell.execute_reply.started": "2024-06-29T09:03:24.577279Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_counts = data[TARGET_NAME].value_counts()\n",
    "target_pct = data[TARGET_NAME].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"Distribution des classes:\")\n",
    "print(f\"  Classe 0 (Non-Disaster): {target_counts[0]} ({target_pct[0]:.1f}%)\")\n",
    "print(f\"  Classe 1 (Disaster): {target_counts[1]} ({target_pct[1]:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Graphique en barres\n",
    "target_counts.plot(kind='bar', ax=axes[0], color=['skyblue', 'salmon'])\n",
    "axes[0].set_title('Distribution des Classes')\n",
    "axes[0].set_xlabel('Target (0=Non-Disaster, 1=Disaster)')\n",
    "axes[0].set_ylabel('Nombre de tweets')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Graphique en secteurs\n",
    "axes[1].pie(target_counts.values, labels=['Non-Disaster', 'Disaster'], \n",
    "           autopct='%1.1f%%', colors=['skyblue', 'salmon'])\n",
    "axes[1].set_title('Répartition des Classes')\n",
    "\n",
    "# Graphique d'équilibre\n",
    "class_balance = min(target_counts) / max(target_counts)\n",
    "axes[2].bar(['Équilibre des Classes'], [class_balance], color='orange')\n",
    "axes[2].set_title(f'Équilibre: {class_balance:.2f}')\n",
    "axes[2].set_ylabel('Ratio (min/max)')\n",
    "axes[2].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9456b",
   "metadata": {},
   "source": [
    "## Analyse des features catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd78d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📊 4. ANALYSE DES FEATURES CATÉGORIELLES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Analyse des keywords\n",
    "print(\"🔑 ANALYSE DES KEYWORDS:\")\n",
    "keyword_counts = data['keyword'].value_counts(dropna=False).head(15)\n",
    "print(f\"Top 15 keywords les plus fréquents:\")\n",
    "print(keyword_counts)\n",
    "\n",
    "# Keywords par classe\n",
    "keyword_disaster = data[data['target'] == 1]['keyword'].value_counts(dropna=False).head(10)\n",
    "keyword_normal = data[data['target'] == 0]['keyword'].value_counts(dropna=False).head(10)\n",
    "\n",
    "print(f\"\\nTop 10 keywords - Disaster tweets:\")\n",
    "print(keyword_disaster)\n",
    "print(f\"\\nTop 10 keywords - Normal tweets:\")\n",
    "print(keyword_normal)\n",
    "\n",
    "# Analyse des locations\n",
    "print(\"\\n📍 ANALYSE DES LOCATIONS:\")\n",
    "location_counts = data['location'].value_counts(dropna=False).head(15)\n",
    "print(f\"Top 15 locations les plus fréquentes:\")\n",
    "print(location_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Keywords globaux\n",
    "keyword_counts.plot(kind='barh', ax=axes[0,0], color='lightblue')\n",
    "axes[0,0].set_title('Top 15 Keywords')\n",
    "axes[0,0].set_xlabel('Fréquence')\n",
    "\n",
    "# Locations globales\n",
    "location_counts.plot(kind='barh', ax=axes[0,1], color='lightgreen')\n",
    "axes[0,1].set_title('Top 15 Locations')\n",
    "axes[0,1].set_xlabel('Fréquence')\n",
    "\n",
    "# Keywords par classe\n",
    "keyword_disaster.head(8).plot(kind='barh', ax=axes[1,0], color='salmon')\n",
    "axes[1,0].set_title('Top Keywords - Disaster Tweets')\n",
    "axes[1,0].set_xlabel('Fréquence')\n",
    "\n",
    "keyword_normal.head(8).plot(kind='barh', ax=axes[1,1], color='skyblue')\n",
    "axes[1,1].set_title('Top Keywords - Normal Tweets')\n",
    "axes[1,1].set_xlabel('Fréquence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0080ae36",
   "metadata": {},
   "source": [
    "### Analyse de texte approfondie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99642702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📊 5. ANALYSE APPROFONDIE DU TEXTE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Longueur des textes\n",
    "data['text_length'] = data['text'].str.len()\n",
    "data['word_count'] = data['text'].str.split().str.len()\n",
    "\n",
    "print(\"📏 STATISTIQUES DE LONGUEUR:\")\n",
    "length_stats = data.groupby('target')[['text_length', 'word_count']].describe()\n",
    "print(length_stats)\n",
    "\n",
    "# Caractères spéciaux\n",
    "data['url_count'] = data['text'].str.count(r'http\\S+|www\\S+')\n",
    "data['mention_count'] = data['text'].str.count(r'@\\w+')\n",
    "data['hashtag_count'] = data['text'].str.count(r'#\\w+')\n",
    "data['exclamation_count'] = data['text'].str.count(r'!')\n",
    "data['question_count'] = data['text'].str.count(r'\\?')\n",
    "data['caps_count'] = data['text'].str.count(r'[A-Z]')\n",
    "\n",
    "print(\"\\n🔍 CARACTÈRES SPÉCIAUX par classe:\")\n",
    "special_chars = ['url_count', 'mention_count', 'hashtag_count', 'exclamation_count', 'question_count', 'caps_count']\n",
    "special_stats = data.groupby('target')[special_chars].mean()\n",
    "print(special_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7299e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des longueurs\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Distribution longueur caractères\n",
    "for i, target in enumerate([0, 1]):\n",
    "    df = data[data[TARGET_NAME] == 0]['text_length']\n",
    "    axes[0, 0].hist(df, bins=30, alpha=0.7, label=f'Target {target}')\n",
    "axes[0, 0].set_title('Distribution - Longueur en Caractères')\n",
    "axes[0, 0].set_xlabel('Nombre de caractères')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Distribution nombre de mots\n",
    "for i, target in enumerate([0, 1]):\n",
    "    df = data[data[TARGET_NAME] == target]['word_count']\n",
    "    axes[0, 1].hist(df, bins=30, alpha=0.7, label=f'Target {target}')\n",
    "axes[0, 1].set_title('Distribution - Nombre de Mots')\n",
    "axes[0, 1].set_xlabel('Nombre de mots')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Boxplot longueurs par classe\n",
    "data.boxplot(column='text_length', by='target', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Longueur par Classe')\n",
    "axes[0, 2].set_xlabel('Target')\n",
    "\n",
    "# Caractères spéciaux\n",
    "special_stats.T.plot(kind='bar', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Caractères Spéciaux par Classe')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].legend(['Non-Disaster', 'Disaster'])\n",
    "\n",
    "# Corrélation longueur vs target\n",
    "correlation_length = data[['text_length', 'word_count', 'target']].corr()\n",
    "sns.heatmap(correlation_length, annot=True, ax=axes[1, 1], cmap='coolwarm')\n",
    "axes[1, 1].set_title('Corrélation Longueur vs Target')\n",
    "\n",
    "# Distribution caps par classe\n",
    "for i, target in enumerate([0, 1]):\n",
    "    df = data[data[TARGET_NAME] == target]['caps_count']\n",
    "    axes[1, 2].hist(df, bins=20, alpha=0.7, label=f'Target {target}')\n",
    "axes[1, 2].set_title('Distribution - Lettres Majuscules')\n",
    "axes[1, 2].set_xlabel('Nombre de majuscules')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757fb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Télécharger NLTK\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648e7c2",
   "metadata": {},
   "source": [
    "### Analyse des mots les plus fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour l'analyse textuelle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "print(\"\\n📊 6. ANALYSE DES MOTS LES PLUS FRÉQUENTS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def clean_text_for_analysis(text):\n",
    "    \"\"\"Nettoyage basique pour l'analyse des mots\"\"\"\n",
    "    if not text or pd.isna(text) or not text.strip():\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Préparer les textes\n",
    "disaster_texts = data[data['target'] == 1]['text'].apply(clean_text_for_analysis)\n",
    "normal_texts = data[data['target'] == 0]['text'].apply(clean_text_for_analysis)\n",
    "\n",
    "# Mots les plus fréquents\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_top_words(texts, n=20):\n",
    "    \"\"\"Obtenir les mots les plus fréquents\"\"\"\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "        all_words.extend(words)\n",
    "    return Counter(all_words).most_common(n)\n",
    "\n",
    "disaster_words = get_top_words(disaster_texts)\n",
    "normal_words = get_top_words(normal_texts)\n",
    "\n",
    "\n",
    "# Visualisation des mots fréquents\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Disaster words\n",
    "disaster_df = pd.DataFrame(disaster_words, columns=['Word', 'Count'])\n",
    "disaster_df.plot(x='Word', y='Count', kind='barh', ax=axes[0], color='salmon')\n",
    "axes[0].set_title('Top 20 Mots - Disaster Tweets')\n",
    "axes[0].set_xlabel('Fréquence')\n",
    "\n",
    "# Normal words\n",
    "normal_df = pd.DataFrame(normal_words, columns=['Word', 'Count'])\n",
    "normal_df.plot(x='Word', y='Count', kind='barh', ax=axes[1], color='skyblue')\n",
    "axes[1].set_title('Top 20 Mots - Normal Tweets')\n",
    "axes[1].set_xlabel('Fréquence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2652f89",
   "metadata": {},
   "source": [
    "### Word Clouds (Nuages de points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de6d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n📊 7. NUAGES DE MOTS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Word cloud pour disaster tweets\n",
    "    disaster_text_combined = ' '.join(disaster_texts)\n",
    "    wordcloud_disaster = WordCloud(width=800, height=400, \n",
    "                                 background_color='white',\n",
    "                                 stopwords=stop_words,\n",
    "                                 max_words=100).generate(disaster_text_combined)\n",
    "\n",
    "    # Word cloud pour normal tweets\n",
    "    normal_text_combined = ' '.join(normal_texts)\n",
    "    wordcloud_normal = WordCloud(width=800, height=400, \n",
    "                               background_color='white',\n",
    "                               stopwords=stop_words,\n",
    "                               max_words=100).generate(normal_text_combined)\n",
    "\n",
    "    # Affichage\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "    axes[0].imshow(wordcloud_disaster, interpolation='bilinear')\n",
    "    axes[0].set_title('Word Cloud - Disaster Tweets', fontsize=16)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(wordcloud_normal, interpolation='bilinear')\n",
    "    axes[1].set_title('Word Cloud - Normal Tweets', fontsize=16)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠️  WordCloud non disponible. Installer avec: pip install wordcloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517cfc70",
   "metadata": {},
   "source": [
    "### Analyse des corrélations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📊 8. ANALYSE DE CORRÉLATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Matrice de corrélation pour les variables numériques\n",
    "numeric_features = ['text_length', 'word_count', 'url_count', 'mention_count', \n",
    "                   'hashtag_count', 'exclamation_count', 'question_count', 'caps_count', 'target']\n",
    "\n",
    "correlation_matrix = data[numeric_features].corr()\n",
    "\n",
    "print(\"🔗 Corrélations avec la variable target:\")\n",
    "target_correlations = correlation_matrix['target'].sort_values(key=abs, ascending=False)\n",
    "print(target_correlations)\n",
    "\n",
    "# Heatmap de corrélation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Matrice de Corrélation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96b26f-08a8-491f-88f6-1758dffba7fe",
   "metadata": {},
   "source": [
    "## Prétraitement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f69ca4-89d7-4b96-9924-22b66335612b",
   "metadata": {},
   "source": [
    "#### IMPORTS COMPLETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b121fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Pour Word2Vec et FastText\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "import re\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637abaee",
   "metadata": {},
   "source": [
    "#### PREPROCESSING COMPLET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d42b5258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'keyword', 'location', 'text', 'target', 'url_count',\n",
      "       'text_length', 'mention_count', 'question_count', 'caps_count',\n",
      "       'exclamation_count', 'hashtag_count', 'word_count', 'urgency_score',\n",
      "       'social_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_features(df):\n",
    "    \"\"\"Extrait TOUTES les features importantes selon votre analyse\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    # Top features (dans l'ordre de corrélation)\n",
    "    features['url_count'] = df['text'].str.count(r'http\\S+|www\\S+')           # +0.196\n",
    "    features['text_length'] = df['text'].str.len()                            # +0.182\n",
    "    features['mention_count'] = df['text'].str.count(r'@\\w+')                 # -0.103\n",
    "    features['question_count'] = df['text'].str.count(r'\\?')                  # -0.084\n",
    "    features['caps_count'] = df['text'].str.count(r'[A-Z]')                   # +0.078\n",
    "    features['exclamation_count'] = df['text'].str.count(r'!')                # -0.075\n",
    "    features['hashtag_count'] = df['text'].str.count(r'#\\w+')                 # +0.052\n",
    "    features['word_count'] = df['text'].str.split().str.len()                 # +0.040\n",
    "    \n",
    "    # Features composées\n",
    "    features['urgency_score'] = features['url_count'] + features['caps_count']\n",
    "    features['social_score'] = features['mention_count'] + features['hashtag_count']\n",
    "    \n",
    "    return features\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Nettoyage intelligent du texte\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocessing complet - remplace votre classe\"\"\"\n",
    "    print(\"🔧 Preprocessing complet...\")\n",
    "    \n",
    "    # Features numériques\n",
    "    df_features = extract_features(df)\n",
    "    \n",
    "    # Nettoyage du texte\n",
    "    df_features['clean_text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    # Encodage des catégories\n",
    "    le_location = LabelEncoder()\n",
    "    le_keyword = LabelEncoder()\n",
    "    \n",
    "    df_features['location_encoded'] = le_location.fit_transform(df['location'].fillna('unknown'))\n",
    "    df_features['keyword_encoded'] = le_keyword.fit_transform(df['keyword'].fillna('none'))\n",
    "    \n",
    "    return df_features, le_location, le_keyword\n",
    "\n",
    "print(extract_features(data).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944ca47",
   "metadata": {},
   "source": [
    "#### TOUS LES VECTORIZERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e54e221-7315-4a73-9991-2445cc7ae18a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:03:25.675870Z",
     "iopub.status.busy": "2024-06-29T09:03:25.674867Z",
     "iopub.status.idle": "2024-06-29T09:03:25.848597Z",
     "shell.execute_reply": "2024-06-29T09:03:25.848597Z",
     "shell.execute_reply.started": "2024-06-29T09:03:25.675870Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tfidf_features(X_train, X_test, max_features=5000):\n",
    "    \"\"\"TF-IDF Vectorization\"\"\"\n",
    "    print(\"  📝 TF-IDF...\")\n",
    "    tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(1,2))\n",
    "    X_train_vec = tfidf.fit_transform(X_train['clean_text'])\n",
    "    X_test_vec = tfidf.transform(X_test['clean_text'])\n",
    "    return X_train_vec, X_test_vec, tfidf\n",
    "\n",
    "def get_word2vec_features(X_train, X_test, vector_size=100):\n",
    "    \"\"\"Word2Vec Vectorization\"\"\"\n",
    "    print(\"  🧠 Word2Vec...\")\n",
    "    \n",
    "    # Tokenisation\n",
    "    train_tokens = [text.split() for text in X_train['clean_text']]\n",
    "    test_tokens = [text.split() for text in X_test['clean_text']]\n",
    "    \n",
    "    # Entraînement Word2Vec\n",
    "    w2v_model = Word2Vec(sentences=train_tokens, vector_size=vector_size, window=5, min_count=2, workers=4)\n",
    "    \n",
    "    # Moyenne des vecteurs par document\n",
    "    def average_vectors(tokens, model):\n",
    "        vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "    \n",
    "    X_train_vec = np.array([average_vectors(tokens, w2v_model) for tokens in train_tokens])\n",
    "    X_test_vec = np.array([average_vectors(tokens, w2v_model) for tokens in test_tokens])\n",
    "    \n",
    "    return csr_matrix(X_train_vec), csr_matrix(X_test_vec), w2v_model\n",
    "\n",
    "def get_fasttext_features(X_train, X_test, vector_size=100):\n",
    "    \"\"\"FastText Vectorization\"\"\"\n",
    "    print(\"  ⚡ FastText...\")\n",
    "    \n",
    "    # Tokenisation\n",
    "    train_tokens = [text.split() for text in X_train['clean_text']]\n",
    "    test_tokens = [text.split() for text in X_test['clean_text']]\n",
    "    \n",
    "    # Entraînement FastText\n",
    "    ft_model = FastText(sentences=train_tokens, vector_size=vector_size, window=5, min_count=2, workers=4)\n",
    "    \n",
    "    # Moyenne des vecteurs par document\n",
    "    def average_vectors(tokens, model):\n",
    "        vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "    \n",
    "    X_train_vec = np.array([average_vectors(tokens, ft_model) for tokens in train_tokens])\n",
    "    X_test_vec = np.array([average_vectors(tokens, ft_model) for tokens in test_tokens])\n",
    "    \n",
    "    return csr_matrix(X_train_vec), csr_matrix(X_test_vec), ft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036992e",
   "metadata": {},
   "source": [
    "#### MODÈLES À TESTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "536e9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    'Logistic': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss', verbosity=0),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "vectorizers_config = {\n",
    "    'TF-IDF': get_tfidf_features,\n",
    "    'Word2Vec': get_word2vec_features,\n",
    "    'FastText': get_fasttext_features\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52765c",
   "metadata": {},
   "source": [
    "#### TEST TOUTES COMBINAISONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56050150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_combinations(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Test TOUTES les combinaisons modèle + vectorizer\"\"\"\n",
    "    \n",
    "    print(\"🚀 TEST DE TOUTES LES COMBINAISONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Features numériques (communes à tous)\n",
    "    num_features = ['url_count', 'text_length', 'mention_count', 'question_count',\n",
    "                   'caps_count', 'exclamation_count', 'hashtag_count', 'word_count',\n",
    "                   'urgency_score', 'social_score', 'location_encoded', 'keyword_encoded']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train[num_features])\n",
    "    X_test_num = scaler.transform(X_test[num_features])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Boucle sur tous les vectorizers\n",
    "    for vec_name, vec_func in vectorizers_config.items():\n",
    "        print(f\"\\n📊 VECTORIZER: {vec_name}\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        try:\n",
    "            # Obtenir les features textuelles\n",
    "            X_train_text, X_test_text, vectorizer = vec_func(X_train, X_test)\n",
    "            \n",
    "            # Combiner avec les features numériques\n",
    "            X_train_final = hstack([X_train_text, csr_matrix(X_train_num)])\n",
    "            X_test_final = hstack([X_test_text, csr_matrix(X_test_num)])\n",
    "            \n",
    "            # Tester tous les modèles avec ce vectorizer\n",
    "            for model_name, model in models_config.items():\n",
    "                print(f\"  🎯 {model_name}...\", end=\" \")\n",
    "                \n",
    "                try:\n",
    "                    # Entraînement\n",
    "                    model.fit(X_train_final, y_train)\n",
    "                    \n",
    "                    # Prédictions\n",
    "                    y_pred = model.predict(X_test_final)\n",
    "                    \n",
    "                    # Métriques\n",
    "                    f1 = f1_score(y_test, y_pred)\n",
    "                    accuracy = accuracy_score(y_test, y_pred)\n",
    "                    \n",
    "                    # Cross-validation\n",
    "                    cv_scores = cross_val_score(model, X_train_final, y_train, cv=3, scoring='f1')\n",
    "                    cv_mean = cv_scores.mean()\n",
    "                    cv_std = cv_scores.std()\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Vectorizer': vec_name,\n",
    "                        'Model': model_name,\n",
    "                        'F1_Test': f1,\n",
    "                        'Accuracy_Test': accuracy,\n",
    "                        'F1_CV_Mean': cv_mean,\n",
    "                        'F1_CV_Std': cv_std,\n",
    "                        'Combination': f\"{vec_name} + {model_name}\"\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"F1: {f1:.4f} | CV: {cv_mean:.4f}±{cv_std:.4f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Erreur: {str(e)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur avec {vec_name}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed611f",
   "metadata": {},
   "source": [
    "#### Analyse des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46160ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyse complète des résultats\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"❌ Aucun résultat à analyser\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df_sorted = df.sort_values('F1_Test', ascending=False)\n",
    "    \n",
    "    print(\"\\n🏆 TOP 10 COMBINAISONS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(df_sorted.head(10)[['Combination', 'F1_Test', 'Accuracy_Test', 'F1_CV_Mean']].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n🥇 MEILLEURE COMBINAISON:\")\n",
    "    best = df_sorted.iloc[0]\n",
    "    print(f\"   {best['Combination']}\")\n",
    "    print(f\"   F1 Test: {best['F1_Test']:.4f}\")\n",
    "    print(f\"   Accuracy: {best['Accuracy_Test']:.4f}\")\n",
    "    print(f\"   F1 CV: {best['F1_CV_Mean']:.4f} ± {best['F1_CV_Std']:.4f}\")\n",
    "    \n",
    "    # Analyse par vectorizer\n",
    "    print(f\"\\n📈 ANALYSE PAR VECTORIZER:\")\n",
    "    vec_analysis = df.groupby('Vectorizer').agg({\n",
    "        'F1_Test': ['mean', 'max', 'std'],\n",
    "        'F1_CV_Mean': 'mean'\n",
    "    }).round(4)\n",
    "    print(vec_analysis)\n",
    "    \n",
    "    # Analyse par modèle\n",
    "    print(f\"\\n🤖 ANALYSE PAR MODÈLE:\")\n",
    "    model_analysis = df.groupby('Model').agg({\n",
    "        'F1_Test': ['mean', 'max', 'std'],\n",
    "        'F1_CV_Mean': 'mean'\n",
    "    }).round(4)\n",
    "    print(model_analysis)\n",
    "    \n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f499d",
   "metadata": {},
   "source": [
    "#### Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "558ad004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ QUICK START - Test complet automatique\n",
      "==================================================\n",
      "🔧 Preprocessing complet...\n",
      "🔧 Preprocessing complet...\n",
      "🚀 TEST DE TOUTES LES COMBINAISONS\n",
      "==================================================\n",
      "\n",
      "📊 VECTORIZER: TF-IDF\n",
      "-------------------------\n",
      "  📝 TF-IDF...\n",
      "  🎯 Logistic... F1: 0.7530 | CV: 0.7277±0.0058\n",
      "  🎯 RandomForest... F1: 0.6955 | CV: 0.6758±0.0092\n",
      "  🎯 XGBoost... F1: 0.7221 | CV: 0.7003±0.0086\n",
      "  🎯 LightGBM... F1: 0.7331 | CV: 0.6987±0.0083\n",
      "\n",
      "📊 VECTORIZER: Word2Vec\n",
      "-------------------------\n",
      "  🧠 Word2Vec...\n",
      "  🎯 Logistic... F1: 0.6096 | CV: 0.6003±0.0038\n",
      "  🎯 RandomForest... F1: 0.6486 | CV: 0.6320±0.0052\n",
      "  🎯 XGBoost... F1: 0.6832 | CV: 0.6573±0.0030\n",
      "  🎯 LightGBM... F1: 0.6618 | CV: 0.6547±0.0052\n",
      "\n",
      "📊 VECTORIZER: FastText\n",
      "-------------------------\n",
      "  ⚡ FastText...\n",
      "  🎯 Logistic... F1: 0.5767 | CV: 0.5633±0.0089\n",
      "  🎯 RandomForest... F1: 0.6513 | CV: 0.6299±0.0045\n",
      "  🎯 XGBoost... F1: 0.6759 | CV: 0.6572±0.0028\n",
      "  🎯 LightGBM... F1: 0.6887 | CV: 0.6605±0.0042\n",
      "\n",
      "🏆 TOP 10 COMBINAISONS\n",
      "==================================================\n",
      "            Combination  F1_Test  Accuracy_Test  F1_CV_Mean\n",
      "      TF-IDF + Logistic 0.753036       0.799737    0.727661\n",
      "      TF-IDF + LightGBM 0.733061       0.785292    0.698744\n",
      "       TF-IDF + XGBoost 0.722130       0.780696    0.700260\n",
      "  TF-IDF + RandomForest 0.695499       0.773473    0.675841\n",
      "    FastText + LightGBM 0.688710       0.746553    0.660538\n",
      "     Word2Vec + XGBoost 0.683230       0.732108    0.657272\n",
      "     FastText + XGBoost 0.675947       0.724885    0.657223\n",
      "    Word2Vec + LightGBM 0.661789       0.726855    0.654733\n",
      "FastText + RandomForest 0.651278       0.722259    0.629894\n",
      "Word2Vec + RandomForest 0.648560       0.719632    0.632006\n",
      "\n",
      "🥇 MEILLEURE COMBINAISON:\n",
      "   TF-IDF + Logistic\n",
      "   F1 Test: 0.7530\n",
      "   Accuracy: 0.7997\n",
      "   F1 CV: 0.7277 ± 0.0058\n",
      "\n",
      "📈 ANALYSE PAR VECTORIZER:\n",
      "           F1_Test                 F1_CV_Mean\n",
      "              mean     max     std       mean\n",
      "Vectorizer                                   \n",
      "FastText    0.6482  0.6887  0.0501     0.6277\n",
      "TF-IDF      0.7259  0.7530  0.0240     0.7006\n",
      "Word2Vec    0.6508  0.6832  0.0310     0.6361\n",
      "\n",
      "🤖 ANALYSE PAR MODÈLE:\n",
      "             F1_Test                 F1_CV_Mean\n",
      "                mean     max     std       mean\n",
      "Model                                          \n",
      "LightGBM      0.6945  0.7331  0.0360     0.6713\n",
      "Logistic      0.6465  0.7530  0.0938     0.6304\n",
      "RandomForest  0.6651  0.6955  0.0264     0.6459\n",
      "XGBoost       0.6938  0.7221  0.0248     0.6716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   Vectorizer         Model   F1_Test  Accuracy_Test  F1_CV_Mean  F1_CV_Std  \\\n",
       " 0      TF-IDF      Logistic  0.753036       0.799737    0.727661   0.005801   \n",
       " 3      TF-IDF      LightGBM  0.733061       0.785292    0.698744   0.008280   \n",
       " 2      TF-IDF       XGBoost  0.722130       0.780696    0.700260   0.008590   \n",
       " 1      TF-IDF  RandomForest  0.695499       0.773473    0.675841   0.009174   \n",
       " 11   FastText      LightGBM  0.688710       0.746553    0.660538   0.004212   \n",
       " 6    Word2Vec       XGBoost  0.683230       0.732108    0.657272   0.002953   \n",
       " 10   FastText       XGBoost  0.675947       0.724885    0.657223   0.002762   \n",
       " 7    Word2Vec      LightGBM  0.661789       0.726855    0.654733   0.005187   \n",
       " 9    FastText  RandomForest  0.651278       0.722259    0.629894   0.004510   \n",
       " 5    Word2Vec  RandomForest  0.648560       0.719632    0.632006   0.005177   \n",
       " 4    Word2Vec      Logistic  0.609600       0.679580    0.600327   0.003843   \n",
       " 8    FastText      Logistic  0.576728       0.670387    0.563323   0.008872   \n",
       " \n",
       "                 Combination  \n",
       " 0         TF-IDF + Logistic  \n",
       " 3         TF-IDF + LightGBM  \n",
       " 2          TF-IDF + XGBoost  \n",
       " 1     TF-IDF + RandomForest  \n",
       " 11      FastText + LightGBM  \n",
       " 6        Word2Vec + XGBoost  \n",
       " 10       FastText + XGBoost  \n",
       " 7       Word2Vec + LightGBM  \n",
       " 9   FastText + RandomForest  \n",
       " 5   Word2Vec + RandomForest  \n",
       " 4       Word2Vec + Logistic  \n",
       " 8       FastText + Logistic  ,\n",
       " [{'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'Logistic',\n",
       "   'F1_Test': 0.7530364372469636,\n",
       "   'Accuracy_Test': 0.7997373604727511,\n",
       "   'F1_CV_Mean': 0.7276606237936297,\n",
       "   'F1_CV_Std': 0.005801002031715946,\n",
       "   'Combination': 'TF-IDF + Logistic'},\n",
       "  {'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'RandomForest',\n",
       "   'F1_Test': 0.6954986760812003,\n",
       "   'Accuracy_Test': 0.7734734077478661,\n",
       "   'F1_CV_Mean': 0.6758405939711695,\n",
       "   'F1_CV_Std': 0.009173514875022739,\n",
       "   'Combination': 'TF-IDF + RandomForest'},\n",
       "  {'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'XGBoost',\n",
       "   'F1_Test': 0.7221297836938436,\n",
       "   'Accuracy_Test': 0.7806959947472094,\n",
       "   'F1_CV_Mean': 0.7002604639506317,\n",
       "   'F1_CV_Std': 0.008590499205535725,\n",
       "   'Combination': 'TF-IDF + XGBoost'},\n",
       "  {'Vectorizer': 'TF-IDF',\n",
       "   'Model': 'LightGBM',\n",
       "   'F1_Test': 0.7330612244897959,\n",
       "   'Accuracy_Test': 0.7852921864740644,\n",
       "   'F1_CV_Mean': 0.6987440000633877,\n",
       "   'F1_CV_Std': 0.008279563091651485,\n",
       "   'Combination': 'TF-IDF + LightGBM'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'Logistic',\n",
       "   'F1_Test': 0.6096,\n",
       "   'Accuracy_Test': 0.6795797767564018,\n",
       "   'F1_CV_Mean': 0.6003273151034345,\n",
       "   'F1_CV_Std': 0.003843342905352083,\n",
       "   'Combination': 'Word2Vec + Logistic'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'RandomForest',\n",
       "   'F1_Test': 0.648559670781893,\n",
       "   'Accuracy_Test': 0.7196323046618516,\n",
       "   'F1_CV_Mean': 0.6320058422038578,\n",
       "   'F1_CV_Std': 0.0051772984771402564,\n",
       "   'Combination': 'Word2Vec + RandomForest'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'XGBoost',\n",
       "   'F1_Test': 0.6832298136645962,\n",
       "   'Accuracy_Test': 0.7321076822061721,\n",
       "   'F1_CV_Mean': 0.6572720989104005,\n",
       "   'F1_CV_Std': 0.0029529212302806126,\n",
       "   'Combination': 'Word2Vec + XGBoost'},\n",
       "  {'Vectorizer': 'Word2Vec',\n",
       "   'Model': 'LightGBM',\n",
       "   'F1_Test': 0.6617886178861788,\n",
       "   'Accuracy_Test': 0.726854891661195,\n",
       "   'F1_CV_Mean': 0.6547331206341411,\n",
       "   'F1_CV_Std': 0.00518662576196552,\n",
       "   'Combination': 'Word2Vec + LightGBM'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'Logistic',\n",
       "   'F1_Test': 0.5767284991568297,\n",
       "   'Accuracy_Test': 0.670387393302692,\n",
       "   'F1_CV_Mean': 0.5633227413649674,\n",
       "   'F1_CV_Std': 0.008871980551601993,\n",
       "   'Combination': 'FastText + Logistic'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'RandomForest',\n",
       "   'F1_Test': 0.651277823577906,\n",
       "   'Accuracy_Test': 0.7222586999343401,\n",
       "   'F1_CV_Mean': 0.6298935099960262,\n",
       "   'F1_CV_Std': 0.004510108129895286,\n",
       "   'Combination': 'FastText + RandomForest'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'XGBoost',\n",
       "   'F1_Test': 0.6759474091260634,\n",
       "   'Accuracy_Test': 0.7248850952068286,\n",
       "   'F1_CV_Mean': 0.657223054378926,\n",
       "   'F1_CV_Std': 0.002762483888838669,\n",
       "   'Combination': 'FastText + XGBoost'},\n",
       "  {'Vectorizer': 'FastText',\n",
       "   'Model': 'LightGBM',\n",
       "   'F1_Test': 0.6887096774193548,\n",
       "   'Accuracy_Test': 0.7465528562048588,\n",
       "   'F1_CV_Mean': 0.6605381381404728,\n",
       "   'F1_CV_Std': 0.004211659462919232,\n",
       "   'Combination': 'FastText + LightGBM'}])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quick_start(df, target_col='target'):\n",
    "    \"\"\"Démarrage rapide - teste tout automatiquement\"\"\"\n",
    "    \n",
    "    print(\"⚡ QUICK START - Test complet automatique\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Split des données\n",
    "    X = df[['text', 'location', 'keyword']]\n",
    "    y = df[target_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Preprocessing\n",
    "    X_train_processed, _, _ = preprocess_data(X_train)\n",
    "    X_test_processed, _, _ = preprocess_data(X_test)\n",
    "    \n",
    "    # Test toutes les combinaisons\n",
    "    results = test_all_combinations(X_train_processed, X_test_processed, y_train, y_test)\n",
    "    \n",
    "    # Analyse\n",
    "    df_results = analyze_results(results)\n",
    "    \n",
    "    return df_results, results \n",
    "\n",
    "quick_start(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0149b-accd-4144-b485-f8b5d0050b5d",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf730cb1",
   "metadata": {},
   "source": [
    "#### Prétraitement sur les données (Split données + TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f3ccf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Preprocessing complet...\n",
      "🔧 Preprocessing complet...\n",
      "  📝 TF-IDF...\n"
     ]
    }
   ],
   "source": [
    "# Étape 1 : split des colonnes utiles\n",
    "X = data[['text', 'location', 'keyword']]\n",
    "y = data['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Étape 2 : preprocessing (déjà défini dans ton code)\n",
    "X_train_processed, _, _ = preprocess_data(X_train_raw)\n",
    "X_test_processed, _, _ = preprocess_data(X_test_raw)\n",
    "\n",
    "# Étape 3 : vectorisation TF-IDF (déjà définie aussi)\n",
    "X_train_vec, X_test_vec, tfidf = get_tfidf_features(X_train_processed, X_test_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df35727",
   "metadata": {},
   "source": [
    "#### Optimisation : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "290b332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params LR: {'C': 1.5, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best F1 CV LR: 0.7390213038494786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 1.5, 2],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    param_grid,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_lr.fit(X_train_vec, y_train)\n",
    "print(\"Best params LR:\", grid_lr.best_params_)\n",
    "print(\"Best F1 CV LR:\", grid_lr.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076888e8",
   "metadata": {},
   "source": [
    "#### Optimisation : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params XGB: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 800}\n",
      "Best F1 CV XGB: 0.715595017305107\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [700, 800, 600],\n",
    "    'max_depth': [7, 5, 6],\n",
    "    'learning_rate': [0.5, 0.3, 0.2]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    param_grid_xgb,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_xgb.fit(X_train_vec, y_train)\n",
    "print(\"Best params XGB:\", grid_xgb.best_params_)\n",
    "print(\"Best F1 CV XGB:\", grid_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6258c1b",
   "metadata": {},
   "source": [
    "#### Optimisation : LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5eed4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params LGBM: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}\n",
      "Best F1 CV LGBM: 0.6870690104510798\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "param_grid_lgb = {\n",
    "    'n_estimators': [600, 400, 200],\n",
    "    'max_depth': [-1, -3, 1],\n",
    "    'learning_rate': [0.05, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_lgb = GridSearchCV(\n",
    "    lgb.LGBMClassifier(random_state=42),\n",
    "    param_grid_lgb,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_lgb.fit(X_train_vec, y_train)\n",
    "print(\"Best params LGBM:\", grid_lgb.best_params_)\n",
    "print(\"Best F1 CV LGBM:\", grid_lgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e7c45",
   "metadata": {},
   "source": [
    "#### Évaluation sur test set du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5743b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 test: 0.7583081570996979\n",
      "Accuracy test: 0.7898883782009193\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.81       869\n",
      "           1       0.75      0.77      0.76       654\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.79      0.79      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "best_model = grid_lr.best_estimator_\n",
    "y_pred = best_model.predict(X_test_vec)\n",
    "\n",
    "print(\"F1 test:\", f1_score(y_test, y_pred))\n",
    "print(\"Accuracy test:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94c440b-b6ca-429c-8d77-168787cc495f",
   "metadata": {},
   "source": [
    "### Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756326b-bbf2-4f3a-9f40-b9cad9b1126b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:50:22.393907Z",
     "iopub.status.busy": "2024-06-29T09:50:22.392902Z",
     "iopub.status.idle": "2024-06-29T09:50:22.809396Z",
     "shell.execute_reply": "2024-06-29T09:50:22.808570Z",
     "shell.execute_reply.started": "2024-06-29T09:50:22.393907Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an experiment if not exists\n",
    "exp_name = \"house-price\"\n",
    "experiment = mlflow.get_experiment_by_name(exp_name)\n",
    "if not experiment:\n",
    "    experiment_id = mlflow.create_experiment(exp_name)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "    \n",
    "logger.info(f\"Experience id: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3febfc98-87c9-455f-8f33-f4f1f0293780",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a783eb4c-6485-495f-9069-d8ac2548316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save vectoriser\n",
    "import joblib\n",
    "\n",
    "#joblib.dump(tfidf, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713431b-acbf-4af0-a743-34f1f7397187",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "# Sauvegarde\n",
    "joblib.dump(best_model, 'best_model.pkl')\n",
    "\n",
    "# Chargement plus tard\n",
    "loaded_model = joblib.load('best_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559083c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Preprocessing complet...\n",
      "  📝 TF-IDF...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['le_keyword.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Étape 1 : tout le dataset\n",
    "import joblib\n",
    "\n",
    "X_all = data[['text', 'location', 'keyword']]\n",
    "y_all = data['target']\n",
    "\n",
    "# Étape 2 : preprocess complet\n",
    "X_all_processed, le_location, le_keyword = preprocess_data(X_all)\n",
    "\n",
    "# Étape 3 : vectorisation\n",
    "X_all_vec, _, tfidf = get_tfidf_features(X_all_processed, X_all_processed)  # X_test est ignoré ici\n",
    "\n",
    "# Étape 4 : modèle avec meilleurs hyperparams (ex: logistic regression avec C=1.0)\n",
    "\n",
    "final_model = LogisticRegression(C=1.5, class_weight='balanced', max_iter=1000, random_state=42, penalty = 'l2', solver= 'lbfgs')\n",
    "\n",
    "final_model.fit(X_all_vec, y_all)\n",
    "\n",
    "# Étape 5 : sauvegarde\n",
    "joblib.dump(final_model, 'model.pkl')\n",
    "joblib.dump(tfidf, 'tfidf.pkl')\n",
    "joblib.dump(le_location, 'le_location.pkl')\n",
    "joblib.dump(le_keyword, 'le_keyword.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8be508b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "data['clean_text'] = data['text'].apply(clean_text)\n",
    "X_train_vec = tfidf.fit_transform(data['clean_text'])\n",
    "joblib.dump(tfidf, 'tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea6214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfer = joblib.load('tfidf.pkl')\n",
    "tfidfer.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6eb95476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf.transform()\n",
    "type(data[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfa6d35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7613x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 99740 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.transform(data[\"clean_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
